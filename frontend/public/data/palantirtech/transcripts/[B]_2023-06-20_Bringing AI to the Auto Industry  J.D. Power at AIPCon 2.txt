================================================================================
METADATA
================================================================================
Title: Bringing AI to the Auto Industry | J.D. Power at AIPCon
URL: https://www.youtube.com/watch?v=WrK0raIVAV8
Published: 2023-06-20
View Count: 10,652
Duration: 946 seconds
Score: 55.0/100
Rank: B (Basic - 基础背景资料)
Transcript: Available
Category: 
Source: whisper

================================================================================
TRANSCRIPT
================================================================================

I'm going to give you a little bit of a show until of the things that we do with AIP and LMS before that letter of introduction. I think you know JD Power because we give awards to cars, we do benchmarking, we talk to a bunch of millions of customers of mostly OEMs, and then we give awards. So you probably see commercials on maybe the Super Bowl, so that's a dance review of the year or the best experience in a luxury car, so that's us. But in reality that's a very small part of our business. Most of our businesses around data analytics. So what we've done the last few years is to you know build and aggregate data sets that in a way define the inner workings of the auto industry. Which is a $1 trillion industry, which is a big, big industry. So we understand what cars can be built, what cars are actually built, where are they in scent, what cars are sitting in parking lots and the dealers, when are they sold, the price of the soul at, when incentives are applied, demographics or who buys those cars, the problems the cars have when they are repaired, warranties, etc., etc. So we have a complete view of the car industry. And we spent you know many years doing things with these data sets. We do some machine learning models, very domain specific models, traditional models. And we have decided, we decided about a year ago that we're going to get into AI with more conviction. And particularly this year we've been driving to become an AI first company. So we're happy to partner with Palantir. I think the last light about iterating with a part of your trust is very relevant to us because we've been learning a lot over the last five, six months. We actually been working together for about five, six months. So why now? This works. Okay, great. So what we've seen in the last few months is that even my mother asked me about charge of BT, correct? So there's a big step in innovation in AI, correct? Things that becoming in a way commoditized. If you go to sites like Hagenface, there are hundreds of thousands of open source models there to be leveraged. Many LLMs, some regression models, classification models. Some models are being downloaded tens of millions of times. So now the secret sauce of AI is not on the models that you can build, but how do you apply them to the data that you have? And as you will see, how you deploy them into applications that can actually move the needle. And again, we have consolidated this data set. We have bought many companies over the last four years that because we wanted those data sets to come into default, correct? So we bought companies that have inventory data, for example. Six months ago, we had, we bought a company that had data around EVs. So now we have all this data and we say awesome, with all this data and then with AI, with LLMs, why can we do? And as we look at the industry, a trillion dollar industry, it becomes a very exciting and open field opportunity. Our job with Palantir is not to necessarily focus on efficiencies internally, but actually build solutions that move the needle in the auto industry. Those are our main clients, correct? So it's a trillion dollar industry. Any problem that we solve is a big problem to be solved. So you can think about problems like when you buy a car, correct? And you try to figure out what is the car that you want that you need. You can find that what is the right price? That's for a consumer, a big problem. Dealers have to figure out what car should I order? How should I price them, correct? And OEMs have a lot of challenges and opportunities ahead, right? What car should I design? Which ones were actually built? Where should I send them in the US? What should be the right price? The incentives? And our car is breaking. What are the problems in repairs? In repairs, for example, in warranty, there's about a $7 billion spend today in the auto industry just to manage warranties. Can we move the needle in a $7 billion online, for example? So what we're trying to figure out, we're trying to experiment with Palantir, is how can we use AIP? And again, when I say we're trying, we started four or five weeks ago when the launch, correct? So it's impressive how much we have done. I'll show you a little bit where we are. But what type of problems can AI help us solve, particularly elements? So let's start with the promise statement that I started at the beginning, correct? I want to buy a car. And I want to, an AI to help me buy a car. So the first thing you can think about is something like this, correct? I want to buy my first plug-in hybrid. I currently have a small SUV, and I think it's the right car for me and my family. What models can you recommend to me? So you can think also, I love AI, I'm going to go to Chad Gpt and ask him this question, correct? But unfortunately, if you go to Chad Gpt and you ask them, hey, when will you last train? It will tell you that it was trained in September 2021. So how can a machine, an AI that was trained so long ago, help me buy a car now? And if you look at the industry, there are a lot of things that have happened in those 600 days from the time that Chad Gpt was trained, correct? 600 new models were launched in the industry. 1.5 different car configurations were made. 1.2 million cars in inventory are in each day in the dealers, but 1.1 are sold on the average. And that's got actually getting now higher than we're getting away from COVID. And there's about a $2.4 billion a month spent in incentive. So how can an AI that was trained 600 days ago help you understand what is the right choice for you now, correct? So we're trying to figure out that. What we're doing now with LLM, what we don't for the last few weeks, is trying to determine how can we take the best that an LLM can offer and map it with the data that we have that is real-time high quality data. So I'm going to show you, I'm sorry, so we asked Chad Gpt for the question that I'll show you about the plug in hybrid. And they gave me some models that were actually 2021 models obviously because it was trained that way. And he said, additionally, checking with local dealerships or visiting a manufacturer website will provide you with the most up-to-date information on the latest plug and hybrid models of the world. Great, because that's the only thing you can say, correct? Can we do better with that data? So I'm going to show you a demo. Let's not start it yet. And a couple of things before the start. OK, one, this is not a consumer-facing application. There's an application that we're building for us to prepare and build that consumer-facing application. That's going to be in our site and it's going to be in the site of some of the OEMs, correct? So what we're learning now is the interface, but most importantly, how do we tell the LLM how to quote unquote behave? How do we put all these things together, correct? And I'll give you some of the lessons that we have learned over the last few weeks. And then second, what is important to us is to understand that the dialogue that a person has with an LLM can go in many, many different ways and can go very deep or very shallow, correct? I'm going to show you here just a couple of questions that a user might have, but then we can talk about what's happening on the background and how that conversation can be extended to many other use cases. OK, so let's start the demo. Cool. So we start a session. We call this session Family Car. And the user types, basically, the question that I talked to you about in the first slide, correct, which is, I want to buy a hybrid. Can you help me out, correct? So this goes to an LLM. The LLM takes, I don't understand, what the question is about. And then the LLM says, OK, I'm going to look at JD Powers data and I'm going to find the best answer for this question, correct? So the LLM takes that and gives that to the application and it says, OK, these are the cars that I found on the JD Power Data that I think you will appreciate, although it would be good for you. But then there's an additional question. Hey, I like this feature that keeps me in the same lane. This is customer language, not technical language. And actually, I don't want to spend that much money. So the LLM takes that, tries to understand the message, goes back to the JD Power Data and say, OK, here you go. Here are three potential models that you can take a look at. And by the way, here are the trims, which are specific trims for those models. The customer can select a couple of things that they like and then we tell the LLM, awesome. Here are the two things that I want you to compare LLM and you also say, here you go. So those things that are written there, if you look at it, are basically the reasons why the LLM recommends one or the other. This text is written by the LLM, correct? And you see things like, OK, I got you, it's lane-keeping assist. It's great for your family. On the other side, there's a bunch of features that you might like, but it's pretty expensive. You told me you were not, you need to watch your budget, et cetera, et cetera. And then we tell LLM, please give me a list of the features that you think are relevant on these two cars, correct? Let's find a car. LLM, help us find a car. So LLM goes to inventories and shows you where the cars are. And then they just select the one that you're going to go. This one is a bunch of miles away, but apparently potentially is the best car you have available. And then there you go, correct? You find your car, et cetera. Again, short conversation, but just keep in mind that where the LLM is driving all this, and I'm going to give you a quick look at what does it mean. So I told you this is an internal application. So as you train the LLM, you might tell them things. Like for example, great, but when you show our car, also show the model year. Or you could say also show the payment, the monthly payment, because people don't understand MSRP very well, show the monthly payment. So, okay, so what's happening in the background? Very quickly. I don't know if you guys were in founder or not, but basically here we have the yellow, which is the interface with the customer, with the chat happens. The green things are what makes us possible the interaction between the LLM on all the parts of the applications, the ontologies in the middle. This is where the data is. The data that we think is relevant for this use case, and then the applications on the other side, correct? The beauty of having foundry and AIP is that you can build this really, really quickly, really quickly. So we build this in a matter of days and iterate in a couple weeks. Now the cool thing is then obviously we're connecting all the systems, but what about the LLM? How do we tell VLM how to quote unquote behave? So you look through a prompt engineering, and that also AIP allows you to build that into the system. So this is actually what we tell the LLM to behave like, to do. So you are an AI assistant that provides guidance on someone looking to buy a vehicle. This is an actual instruction. You are friendly and helpful. This is an action instruction. It's going to crazy, no? Et cetera, et cetera. So that's the first one. The second one is only in the information of vehicles from Jason, which is the jetty power, basically, data, correct? But make inferences about what the customer may want based on your query. This is actually what we typed as a prompt to build the application. So in the last one, we're telling open a charge of BTE. We're telling them, when somebody wants to compare vehicles, don't go crazy and go back to 2021 and find some stuff, don't do that. Just focus on the jetty power data, correct? So there are more things that are happening here. We are telling them, for example, data on our Vs. So if you can get, we will tell the LLM. If you can get a zip code from the, ask for a zip code, and you can check how much snow falls in that zip code. If there's a lot of snow, think about all-wheel drive and the clearance of the vehicle. If you have a zip code, you can know that the price of electricity and the price of gas. And you can give me a total cost of ownership comparison between plug and hybrid and non. Also miles, for example, you drive it there, et cetera, et cetera. So it's very compelling. The beauty of the system, again, is that on the top, you see some of the stuff that we're doing in volunteer. We have several applications, some of the applications are out in our clients already. We basically have data flowing all the way to an ontology and then applications feeding that ontology. One of those applications, for example, is an application around repair analytics to understand warranty costs and optimize warranty costs. So what we're doing now is an application used in LLM that will take verbatim from repair. Every time our car is repair, the technician in the dealer writes, this is what the customer told me. And this is what I found and how I repaired it. So you have hundreds of thousands of those, millions of those. So if you're a system engineer trying to focus on how to minimize warranty, you want to understand root costs. Why is this failing? What speeds? What temperature? What does it behave? What does it behave? So we're building an ontology application that is powered by a lot of them that will allow us to do this. And this is really real. I mean, we'd present this to a client in two weeks. And if we get it right, once we're convinced that it's right, we'll deploy as an application to our clients. Few things that are really critical for us. Like the slide of the previous talked about is speed. We need to be able to do this fast. We need to be able to not only take things to market pretty fast, but learn really fast. And the iteration cycles here are not measuring month or weeks or days. This is about every hour where we're learning things about how do we train in low-lamps and how do we understand how do they behave and how to leverage their insights, et cetera. Extensibility, we need to connect every single application, every single model that we have in our platform to an LLM driven experience. And the last two are completely fundamental for us. We have our data is our IP, is the value of our company. The data that our customers put in our systems is absolutely critical. So this has to be bulletproof. It has to be very secured in privacy governance are really must for us. Finally, the road ahead, the first one we have done this for a few years. So we bring data analytics and we do domain-specific models. Now we're leveraging LLM's to integrate into GD Power data and also bring on the models to provide new experiences. We've been working on that and we're actually releasing applications in a couple of weeks. And ultimately, what we think in the challenge is how do we build intelligence that actually understands the auto industry and can tackle fundamental questions. And that is combining LLM's with domain-specific models. And we are even arguing should we train our own LLM to be able to be more nuanced about the discussions that we have in the auto industry. So, right now, I want to thank the PANAMSTER team, great partners, Taylor, Susanne, Jan, various, all those guys that have done amazing work. A lot of respect and appreciation for the work. And looking forward to the next few months. Thanks a lot.