================================================================================
METADATA
================================================================================
Title: AIP Product Update | Palantir CTO Shyam Sankar at AIPCon
URL: https://www.youtube.com/watch?v=oW9svx4vT5g
Published: 2023-09-18
View Count: 13,987
Duration: 1200 seconds
Score: 55.0/100
Rank: B (Basic - 基础背景资料)
Transcript: Available
Category: 
Source: whisper

================================================================================
TRANSCRIPT
================================================================================

So, June of this year, you know, from this stage, I launched AIP. So, just three, four months ago here, which AIP is our open, extensible AI platform to help you build AI-enabled applications by bringing LLMs to your private data, on your private networks, that power these experiences, both safely but also efficaciously. But really, the observation that we've had over this period of time is that the battleground of AI, it's the factory floor. It's the front line. And that's because AI transformation is an empirical journey. You have to experience it. You can't think your way through it. You have to go do it. And so today, we have tons of product to show, but we're going to have our customers actually show all of the new things that we've done together. We're going to have them take you on their experiential journeys here. Customers like HCA, who have proactively looked into the future of their actual schedules to identify patient bottlenecks and staffing shortages and solve them upstream. Or Eaton, where they've gone from an alert inbox that tells them things that are going wrong to a solution inbox that tells them things that they can be doing about things that have not yet gone wrong but might. The important thing about all of these customers is that they have pushed to prod. I tell every parent here and internally, like, I don't want to see proofs of concepts. I want to see proof. And that's largely because these LLMs are different. They are something new. They break our existing mental models. And I thought that's what I'd really focus on sharing with you. What are the ways in which we think about these things and how are they kind of different than what's come before? And we could start there by asking what is an LLM doing or perhaps more importantly, what is it not doing? Well, it's not doing algorithmic reasoning, which is a form of computation that's so precise that there's no ambiguity in this execution. To the extent it doesn't do what you said that we call it a bug. That's not supposed to happen. On the other extreme, it's not doing human thought. A inherently creative medium that is wrought with ambiguity. That's part of the feature of human thought. It occupies this middle ground in between these two things here. And it can be very tricky as you're implementing these things to hold that tension in a precise way because it is fluent in natural language, but it doesn't actually understand what it's saying. So it's not human thought. But while it is instructable in ordinary human prose, it's horrible at algorithmic reasoning. So it's this third type of thing. And I think one of the characteristics of this third type of thing is that LLMs are statistics, not calculus. It is this kind of stochastic genie, if you will. There's a lot of randomness in it. The best metaphor for this, and perhaps one of the things to think about as you're implementing these use cases, is what it was like for us when we first started trying to predict the weather. When we first started trying to predict the weather, I think in the rough mid-1800s, we thought it was going to be like predicting the next eclipse. Like once we figure out the math behind this thing, I'm going to tell you precisely what the weather is going to be in this location 100 years from now at this time. And obviously intuitively, we all understand that's not how weather works. Astronomy is calculus. It's governed by the truth of the underlying mathematical equation. I can tell you that in the year 2186, I can tell you exactly where you need to be on Earth to see the longest eclipse in the last 10,000 years. That's the nature of calculus. On the other hand, with weather, it's more like it might rain today. The equations are dominated by the propagation of error and some underlying fundamental stochasticity, randomness in it. And we see this through another lens too. Like the fixation with pushing to prod comes by thinking about this by analogy. When you introduce even one stochastic variable into what was otherwise a deterministic system, your software, you have now made the entire system stochastic. And that requires us to rethink some fundamental things that are in the tool chain, how we use human reason about the problem, and how we incorporate it to achieve outcomes. This is the difference between a demo and a product. So we had a self-driving car demo in 2025. It drew 132 miles. It's not around the block. It's a meaningful distance here. And arguably, you could say maybe this year we'll have a product. Maybe. Maybe we'll take another year or two more. That's after 44 million self-driving miles. And some of that need to have all that experience speaks to this intuition that we have, that there's something stochastic about it. We need a lot of proof to understand how it's going to behave. So in some sense, it's kind of easy to build a demo. It's much harder to build the product. And that's what we're focused on. And when you really think about that, you start to uncover the fundamental primitives and infrastructure that you're going to need to harness the true underlying value of transformation that LLM's can bring to you here. And realizing that all of that value comes from the elegant integration of the algorithmic reasoning, the LLM, and human thought. And that's what you're going to see in the workflows that customers show off later today. But one of the core ways to leverage the power, the underlying power of the LLM is this idea of the KLLM kernel. Why would you use one LLM when you can use K? We can kind of think about, if you squint and just go with this analogy for a second, like each of these LLMs is like a slightly deranged mad scientist, mad genius. When we have this sort of situation as humans, we build committees. Why would you ask one mad genius the question when you can ask a committee of mad geniuses the question and then synthesize the answer that you're going to get back and understand the kind of rich context where they agree, where they disagree, and what you want to do about that. It also forces you to realize the fundamental truth here that there isn't an answer in a context like this. There are answers. And it's probably most important for the underlying application developers to realize that. Because if you're in this simple paradigm, pretending that this is a deterministic, asking you a question, getting back a single answer that could be hallucination, that could be kind of, it's not the answer. It's an answer. And then you're surfacing that to the user, you're going to find yourself stuck somewhere in that journey between 2005 and 2023. If instead you're using a kernel, like the KLLM kernel, and you're getting back multiple answers, you're synthesizing those answers, you're able to flow that through and drive the entire user experience towards operational outcomes. That's how you synthesize, analyze, and design the software. That's really going to matter here. So if we just walked through this approach briefly so that we understand what we're looking at, you would take your prompt and you would feed it to KLLMs. In this case, I just have four to keep it simple and reasoned through it. You get back the responses from each of these LLMs. You can use that to summarize the response of each LLM. And you send the entirety of each of the responses to a synthesis stage to understand, rate, compare the answers that you've gotten, and come forth with a best possible answer, the dissenting view of the answers, which models got what right, which models got what wrong, and then surface that to both the developer to drive the user experience, and the end user as a consequence of doing that. Now the synthesis can be deterministic. If you're getting back JSON or code, you could treat these as abstract syntax trees. You can do formal diffs of what you're getting back. It's a deterministic sense of what's the same and different. But also, the synthesis can be an LLM step itself, where you have a synthesis prompt that's helping you in the context of the workflow you're trying to do, create the right synthesis that matters for the work that you're doing in that stage. I'm going to illustrate this in a second. But I think what you will see is that the KLLM approach is the only way to manage model stability, operational risk, and harness the true nature of the LLM to win. So when we talk about model stability, if you remember a few months ago, there were reports that chat GPD or GPD4 suddenly got worse or better or different, certain things. If you asked open AI, they would say, no, nothing changed. I always told you these models were still cast. So yeah, you're getting different answers. That's the nature of the model. I think that's a pretty strong argument for why you can't have one LLM making critical decisions. You need K. On the other hand, if you think, oh, actually something did change in the underlying model, then it's a pretty strong argument for why you need more than one LLM. You need K if you're making a critical decision. So this allows you to triangulate the answers you're getting back and have a scaffolding around the critical decisions you're trying to drive in your enterprise operationally. Then if we think about, there's a lot of interest in creating custom fine tune models. That's great. There's a world where there's a menagerie of models in the future. How are you going to deploy them? You think about how are you going to have to do the equivalent of 44 million self-driving miles of testing and evaluation to understand and characterize putting this model into something as critical as manufacturing batteries or critical national infrastructure? I don't think that's a valuable surface area of solution. Why don't you just make it the K plus one model? It's the K plus one LLM. It's being evaluated in parallel. You're able to triangulate how it's the same or different in how it performs against a broad surface area. And certainly, contextually against the use cases, you actually built this custom fine tune model to perform against. So it's the fastest way to get operational value from the models that you want to build in the menagerie here. And then of course, a big part of this approach is recognizing that you don't have many priors or you have no priors on these models. How much experience do you have with this committee of experts? Over time, just like with a human committee of experts, as you get to know the experts better, you will have a stronger opinion, in this case backed up by the telemetry within AIP of historical responses of which models are going to be best at what sort of use case. So are you going to ask every question to K models all the time? No. As you have stronger and stronger conviction of where to go for what, you'll tune that down. You can tune it down for correctness and accuracy. But all those levers will be within control of the enterprise here. So maybe to walk through an example here, the first, so this shows us using GPT-4, star chat data, and Lama to 13b. What models don't really matter per se, but just so you have some context here. So the underlying prompt is that I would like to get back a Python function that takes in this table of employees, with columns like first name, last name, year employment date and company. And it gives me back the employee at each company with the highest tenure. So the prompt is being sent off to all three models at once. You see the GPT-4 answer streaming back in, the other, and once you get the full answer, you'll get a summary of the answer. So it collapses down. You see the star chat summary, and Lama will be coming shortly. Once all of these answers come, they will be sent to the synthesis stage, then to compare and contrast them. And that's where the real magic of the KLLM kernel comes in. So here is the synthesis answer streaming back in here. And I'm getting the models assessment of what code should I actually consider. What Python code should I actually do that meets these requirements here? But much more interestingly, now we see the relative performance of the models. So how did Lama 13 do? How did the star chat do? And we see that in the upper right and the lower middle section here. So let's step through each of these in a second. Here are the summary answers. Now here's the output from the synthesis. What we can see is that interestingly, Lama did something where it dropped all employees who had a tenure of less than a year. That was not an explicit requirement of the prompt. It's kind of interesting though as a developer, maybe I want to consider that. It also shows you the nature of the still casticity that's underlying this. So it either meets my requirements. It considers an edge case I hadn't thought of or it doesn't. Star chat actually failed to account for ranking the tenure of the employees. And so this gives me a sense, again, of this is another dot in understanding how this model is going to form over the fullness of time here. But I, as the developer or the end user here, have a rich understanding of what I should be thinking about. And a much richer understanding than if I just asked one of these models the question. Even if I got quote unquote the right answer, I have a much more varied topology, a 3D understanding of what's going on. And then operationally, one of the interesting things is, what if I only have one LLM? Even then, this is still valuable. Think about it as K by one LLM's where we've put this in production with manufacturing customers where they're trying to leverage their internal enterprise knowledge repositories and create abilities for frontline workers to interact with it. You can actually take your question and send it to in parallel, the same LLM at scale, and understand how varied, how much concurrence is there in the responses that you're getting back, how much divergence is there. I have a silly example here where I'm just saying, look, pick basically I have three doors, which one should I pick? And this is a real example where I get back at a different answer every time I ask. You could kind of maybe expect that in the context of having the stochastic genius your answer. Something it tells you is this is probably not a good question to ask in LLM. This is an LLM hard problem. If you're never going to get convergence on the answers you're asking, maybe you're kind of designing this wrong. It's some way of thinking about that. But importantly, then, this surface is, how do I bring this to the workflow? Do I need more knowledge augmentation on the front end? How do I, what sort of augmentic tools can I build to bring to bear? How do I bring this into production? And this has been very valuable where it deeply cuts down hallucinations and the reliability of the underlying use cases. A lot of the work I just showed here focuses on chat. But if I had the unfortunate experience of having me sit at you and talk about these LLMs for a bit is you know one thing as I'm highly biased against chat. I think it is the most limiting interface and it kind of structurally means that we're not going to fully harness the value of LLMs because it controls our human creativity on what these things can do. The reason that's the case, it feels this number one mistake about thinking about what are you relying on the parametric knowledge of the model to do. The parametric knowledge is like the knowledge that's implicit in the model. What I think is special about these models is that they speak a language, a regular grammar. Yes, sure, English, but much more interestingly, they speak domain specific languages. They speak code. They speak JSON. If I could just say it colloquially, they speak code and clicks and they do that really, really well. On the other hand, if you're trying to somehow jam the specific knowledge of your domain into these models, I think you will find that that's a Sisyphean task. You will find that you never fully get there. They never get better than your true experts or you're always chasing your tail. It's not like a database where you can just jam facts in there. It's kind of going back to this calculus statistics thing. It's like the pressure and the balloon is kind of moving around. You're going to be facing trade-offs on the frontier. But here, they are phenomenal. If you thought about this as LLM should be defining the experience your humans have when they're using their enterprise tools, then you start to see different patterns emerge here. This video will illustrate we have this application in the upper right-hand corner here. That is our operational application. That application has some sort of internal representation, some sort of JSON that represents the app. That's what you're going to see rendered here. It looks like computer speak. I would like to add properties to this table that are every location property that's in the ontology. I can do that as a human. It's probably on the order of 50 to 100 clicks to go into the application to edit it to add all of these things. I can just say this. The LLM can speak those clicks, if you will, and change my underlying application state for me. This is, I think, a better way of thinking about the paradigms that we could be using to power these sorts of experiences. It's a concrete manifestation of how the experience the LLM creates changes the human interaction with software. It allows us to leverage tools. LLMs need software tools. They need that augmentation to do algorithmic reasoning. That is how you get the precision of calculus with the power of statistics. All of the value sits at that intersection of your deterministic code and the stochastic LLM. This is how you register your existing AI models, your Jupyter notebooks, your Lambda functions, your high performance compute clusters. How do I take all of that deterministic value I have and unleash it in this sort of paradigm here? In many ways, chat also limits us to thinking about LLMs as a single tool. It's like hitting in music. It's like hitting one note. You want to be playing a chord. That chord is the elegant integration of human thought, algorithmic reasoning, these tools that you have, and the LLMs. I want to just signpost this to really watch for this in the customer demonstrations later. Here's an example that we'll see from Eaton, where we can look at what are these tools and what do they look like? We start off when we see the prompt here. This is, of course, a prompt that's going to an LLM. But underneath the prompt, you can see it's been given a tool, the ability to query the ontology for plant material, I think. You see more prompts, and then the interplay, okay, well, I can query for plants and for plant material as well. As we kind of go on here, this is like chained logic. This is an LLM-backed function. I'm building logic that's running my enterprise. It's fluidly going between the LLM and a query tool. And now what we see, it's also going to an ontology function. You can see that that's the function that's calling the AI model. It's calling the Lambda function. It's calling the notebook. It's calling the logic in my enterprise. It's letting the LLM wield it in an operational context here with the guardrails, with the safety, in an elegant sort of way. So I think you'll see that in each of these demonstrations. How did the customers build these functions? And how are they going between the LLMs and the Agma tooling? And that's a big part of this. What will also come through in these demonstrations, the LLMs don't change what problems matter to your business. The problems that matter to your business are the same problems that matter to your business yesterday. They allow you to solve these problems substantially more quickly. And I think that's really exciting, because with AI as Alex even kind of alluded to, all the value really accretes to the incumbent. The incumbent users, the incumbent applications, the incumbent organizations. And that means you can just solve these things substantially more quickly. OK, so I think enough with our mental models, let's get to the front lines. Ted will be up next to tell us and take us there. Like what works? How do you approach it? How do you get started? Thank you guys.