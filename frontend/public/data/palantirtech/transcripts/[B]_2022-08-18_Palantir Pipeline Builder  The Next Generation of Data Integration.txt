================================================================================
METADATA
================================================================================
Title: Palantir Pipeline Builder | The Next Generation of Data Integration
URL: https://www.youtube.com/watch?v=6_cyrBAf_dQ
Published: 2022-08-18
View Count: 26,824
Duration: 520 seconds
Score: 63.0/100
Rank: B (Basic - 基础背景资料)
Transcript: Available
Category: 

================================================================================
TRANSCRIPT
================================================================================

Today, it's clear that being able to leverage the power of data is instrumental to our collective future across sectors. There's too much at stake in an ever-changing world to settle for fragmented or siloed data landscapes. Over the past year, we've been working on a next-generation approach to data integration. It's the biggest single step forward we've taken since the release of Foundry's original data integration suite. Pipeline Builder is our new data integration capability that delivers on democratizing data integration while maintaining robustness and security. In short, allowing users to build better pipelines. Let's start from the beginning and see how it's put into action. We're going to work through building a simple pipeline from scratch. We can add any data on our Foundry instance to the pipeline. This data can include tablet data of all sizes, streaming data such as sensor recordings, geospatial data and telemetry from IoT devices, or unstructured and semi-structured formats such as imagery data, XML files and PDFs. Here we'll be working with a supply chain disruption. We want to bring all our regional suppliers into one place to build a worldwide view which we can rely on. Without a couple of datasets, supply US West and supply Europe. These datasets contain rows of suppliers with columns describing various properties of each supplier. We want to clean and union these two datasets together and derive additional columns. We'll also upload some XML files which can be quickly passed into a structure table and joined into our supplier datasets. So we'll start by creating a union between supplier US West and supplier Europe to bring them into a single dataset. Upon creating the union, we're immediately given an error message that the schemas don't line up. We can fix this by adding a couple of transforms upstream. One transform, renaming supplier to supplier ID so that ID columns match up and another dropping some columns that show up on one side of the union and that we shouldn't need. This said the error message is immediately resolved. This immediate feedback saves us time-watering and means we don't need to run any data computation to find issues. Next we can create a pipeline output which will let us deploy the final outputs of our pipeline. Pipeline builders underlying architecture is designed to support all kinds of outputs, datasets, ontological objects, streams, time series, even export to external systems. The aim is to take you end to end from the raw data source through to the final destination of your data. Pipeline outputs are the target of the pipeline. If we no longer meet this target, the user is immediately warned and deployment is prevented so that the user can determine where it's an intention to change the output contract or an accidental logic break or problem with incoming data and these resolving. He will create a dataset pipeline output. This output will expect this exact set of 35 columns. Again, if the pipeline does not produce these 35 columns, we'll immediately flag the break and prevent deployment to stop breaks downstream. Pipeline builder will produce these outputs while abstracting all of the implementation of logic execution and pipeline deployment behind the scenes. We use our union as a starting point for the pipeline output and add a few columns, an address, string column, and active Boolean column to mark whether the supplier is currently active. Our transaction span integer column for the number of days between the oldest and most recent transactions and the total number of suppliers in the same city as the total suppliers column which is an integer. As we continue to iterate, we'll be guided to fill out this target. A core concept of pipeline builder is a separation of data computation and schema computation. This separation allows us to immediately evaluate the success of our integration. We don't need to wait to run the pipeline across potency enormous or complex input data. We can check our progress towards filling out pipeline outputs. If bad data shows up tomorrow, I can miss in column. We immediately know and prevent deployment entirely. Throughout building this pipeline, we'll be getting immediate feedback. Now we have our supplier data in a single dataset. Let's derive additional columns using the powerful set of built-in data integration functions. As we derive new columns, notice how the pipeline output immediately identifies these are now filled in. First, we can encrypt the supplier tax ID column using foundry's built-in capability for encryption and selective revelation. Next we can produce an address column by concatenating street, city, and zip code together. We can calculate the time from oldest to most recent transaction with a timestamp difference function. And there's some white space in the branch column. We can use the clean function to remove this. We can also produce a Boolean column for whether the supplier is active by mapping values from our string column to Booleans. A histogram of the column provides the list of values we need to map. True and yes to true. I'll voice fall back to false. And finally, we can calculate the number of active suppliers per city with a window function. Up to this point, we've been a single user working on this pipeline. Our pipeline builder allows for collaboration between users with different expertise. Technical users now have stronger typing, less time running builds, ranking code, managing libraries, and no longer have to manage the complexity of the code reaper. Business users now have a tool where they can write data integration, which will manage performance and protect them from making mistakes. They don't have to go through the laborious process of writing code. For pipeline managers, pipelines are highly structured. Their usage and outputs are well defined. They don't have to worry about persisted data sets, and they can see exactly what's changed between deployments. And all these users can work together. This collaboration is made possible with pipeline builders sophisticated version control. Users can branch pipeline logic to work on a sandbox. Once they're happy with changes, they can review their changes and propose merging into main or another sandbox. Check so run on the changes to flag any breaks to the pipeline outputs and check for merge conflicts. If they're a merge conflict due to other edits being merged to main, the user will be prompted to rebase their sandbox and handle these conflicts. Reviewers can then inspect the changes and validate them before merging them into the pipeline. Suppose this pipeline is up and running. We're making further changes we'll want to work on a sandbox and propose merging changes into main, rather than editing and pushing directly to production. So we'll create a sandbox named add state data. First, we can quickly fix our remaining errors by casting transaction span and total suppliers from long to integer. As we add these casts, the schema compute through the entire graph from pipeline inputs through to the pipeline output. And we're immediately given feedback that the integration is now successful. Finally, we can pull in our third data set with a join. This will let us replace addresses that are completely missing with a country code. After joining, we'll co-less the country codes into fill where their addresses are now. Now we can create a proposal to merge our changes into main. We'll show in a comparison between add state data and main, where we can see additions, modifications and deletions. And we can click into individual transforms to see exact logic changes that were made. This looks good, so I'll create the proposal. When this is merged in, we'll see our changes on main. Finally, we can click deploy to deliver all pipeline outputs in the pipeline. Pipeline builder will deploy all these outputs while abstracting all of the implementation of logic execution and output deployment behind the scenes. Today, we've shown a simple example, but to list a couple of exciting examples, whether it provides a massive step up. We've complex healthcare systems where disparate data sources are integrated to construct pipelines which can be templated. And then these templates can be redeployed to specific facilities where last mile work needs to happen in hours, not weeks or months. And in the aviation industry, we'll need to continuously integrate real-time streaming data sources to make rooting decisions and inform other critical operations. And across all industries, where the burden of integrating data has been on only data engineers or technical users. But now citizen pipeline builders can construct production quality data pipelines themselves. Thanks, Cameron. This was just a sneak peek at the user experience for Foundry's next-generation pipeline builder. We wanted to synthesize the fluidity of no code, instant feedback and declarative configuration with the iron-clad fundamentals of Foundry, such as backend extensibility, Git-style change management, and active automated metadata for data security, lineage, and health. There's so much more to dive into, including a more detailed look at how streaming workflows, multi-user collaboration, and the integration with the ontology and other target substrates work seamlessly at scale. Moreover, with Foundry's modular architecture, it's designed to power the full range of enterprise systems, data lakes, warehouses, ERPs, operational data stores, edge synchronizations, and much more. Thanks for watching and stay tuned for what's next.