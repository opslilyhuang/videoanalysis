================================================================================
METADATA
================================================================================
Title: Synchronizing Distributed Data | Palantir CEO Alex Karp and Peter Wilczynski
URL: https://www.youtube.com/watch?v=IX55r1WhUWM
Published: Unknown
View Count: 0
Duration: 0 seconds
Score: 37.0/100
Rank: B (Basic - 基础背景资料)
Transcript: Available
Category: 其他
Source: whisper

================================================================================
TRANSCRIPT
================================================================================

So, we have, as you know, we have an ongoing series, Parvathy, did one, and where we talked about general-polligerent culture. People often wonder how this motley crew of people ended up powering the most important institutions of the West, especially in the military, largely things we can't talk about, especially since we're outsiders, and actually in USG for years and years didn't even have a sales force. But we've built some of the most important back-end instruments of war. These instruments are powering some of the most interesting important war efforts in the world now and have led to disproportional success on the battlefield. Probably the most important thing we've built, you are in charge of nexus peering in that context. Before we even get to the attribute, why is it difficult to supply operational data on the war in battle? What is it about that would require you to build a special component like nexus peering? Why is it difficult? And if you turn on TV and there's like whatever that TV show is, 48 or 24, it's like, it's just the data is all there, they go in the battlefield, the data is all there, and it turns out for special operators, the data was never there. Yeah, I mean, so many reasons, there's so many reasons why it's hard. I think at a very basic level, I think about that in terms of sort of geography, organization, and network, right? So special operators work across many networks, across many geographies, across many organizations, and that gives you the sense. Why do I know those organizations, lay people are like, well, they just get all the data from all the organizations. Why don't they just get all the data in real time? Why not? Yeah. I mean, I think to the extent you want to collaborate, you have to have a common language. And so if an organization wants to model data for themselves, they're going to model it in a way that makes sense for them. We call that the technology. Each organization models the data differently. And so if you were a special operator climbing over a mountain, jumping after you jumped out of your helicopter, sneaking into a building to take out bad people, you de facto get garbally cook. Exactly, exactly. It's sort of like trying to translate Spanish to English, right? You know, if you don't speak both languages, there are things that you actually can't express in either side. And so there's only a sort of subset of all the data, all the data written in Spanish that you can really translate to English. It's not a great example, but the more different the languages are, the more different the organizations are, the less you can actually sort of have an individual end user collaborate with another user who's sort of speaking a foreign data language. And some other issues, like so you have security issues. What are those issues? Like how, you know, what is involved in, say you have, you know, an organization, leaving aside even the complexity of multiple countries, each organization has its own classification methodology. Why is that difficult to translate onto the battlefield and what are we doing to alleviate that difficulty? Yeah, so there's really, you know, two forms of security. You have physical security and sort of, you know, virtual security. So from a physical security perspective, data is just stored in different databases on different hardware. That hardware can't always talk to one another. And there's not even a network connectivity, you know, available between two physical servers. So when we first built Nexus pairing, we were building it for two customers who were, you know, the medium of exchange was a DVD once a week, right? There was no, you know, cable plugged in from customer A to customer B. And so we had to design the whole system sort of end to end to work on a DVD, right, with extremely high latency. And so most of the medalities of collaboration you would imagine, so chat, real time sort of typing and seeing that synchronously between two users, you're getting that once a week. And so you just have to design the whole system in a totally different way. And the latency is another, like, so we have classification model. So how do you take different classification models and different ways of viewing the data? So that's one problem is how do you view the data we call ontology or like our, our dyno, our ontology allows you to translate their ontology. Then you have classification models. So every organization requires you to work at different levels of classification and on different missions. And now you're getting to a different issue, which is the latency issue. And my view is the one that people understand the least. Yeah. So people intuitively understand the ontology issue, the classification issue, and understand maybe their software isn't able to do that. And but the latency issue is an issue I think that I've never met anyone that understands it. And on the battlefield software breaks, why is the latency so, so didn't such a difficult issue for software? So I think it's about not just the software, sort of the user has certain expectations of the system. When they log in, they sort of have a mental model that they're operating within. And the latency issue really breaks that mental model, right? When you log into Gmail, you don't have to think like where is my email. It's just sort of replicated in certain ways. But sort of it's obvious once you say it, we build analytic systems. Those systems require huge volumes of data. And so that data just can't make it to the edge. If you have a small device on the edge, a tactical node, you're not logging into a server, you're logging into a mesh of servers. And so what that translates into is there's both a latency issue of sort of how quickly can you move data between servers. But there's also this mental model the user has to have, which is that they're logging into a mesh. And the data is going to be sort of available, different volumes of data are going to be available, different parts of the mesh. And so one of the really hard things is sort of how do you get the right data to the right piece of hardware such that the user can actually access it who's on the tactical edge. And how do you do that? And what is the role of the vector clock? What is a vector? Yeah. So if you think about a sort of basic system, how Palantir was for the first couple years, we really thought about data versioning, revisioning in terms of logical clocks. What is data versioning? Why is it important? Right. Data versioning is basically just data changes over time. Palantir has grown, Palantir has more offices, over time the world changes. And you want to reflect that in sort of your digital twin in your data itself. The clock is really like a watch. It's basically saying, hey, at this time, what did the data look like at this time? What did the data look like? And so if you have one clock and I can put my watch on the table, we can both look at the watch and we can see what time was. We can sort of roll back the clock. But as you have two servers, those two servers have independent clocks. They don't know what time it is. And so the two clocks or three clocks or four clocks or a thousand clocks. Exactly. They know the relative time of one another, but they don't have any way to reference absolute time. Right. So you could think about it as I can tell you, you know, 250 on my watch is later than 150. But I don't know what your watch says. That 150. And to figure that out, I have to ask you and you have to tell me. And so there's this sort of bidirectional problem of figuring out what time was it for you to use. Without using the server, basically, collapses. Exactly. And so this is super precise work, right? Like when you mess this up, it's super corrupted. The system just sort of falls apart. And so you want to be able to very precisely revision sort of what happened when relative to- And it just for historical clarity, we already understood versioning. This versioning is a normal concept in software coding, outside software people don't understand it. But the versioning allows you to also then work with thousands of developers and write the code to the trunk while still being able to experiment on the branches. And then we took that idea of versioning and built it into our software. We had versioning, which versioning still won't work in this context because you would need something like a clock. The lay version of this would be like to wait to time stamp every little shred of data so that you could version against it. Exactly. And otherwise the system collapses, which- and then special operators don't come home. Yeah. Yeah, that's right. And I think that with a normal clock, there's sort of a very well-defined relationship between something that happened before and happened after. We call this a happens after relationship. With a vector clock, that relationship sort of falls down because your clock can move forward, my clock can move forward, and neither of us know how fast our clocks are moving. And so there's this special relationship with vector clocks, which is sort of much more ambiguous than with the traditional logical clock, which is basically, you know, things can happen at the same time. That's just impossible with a normal clock, right? Because the time always increments when something happens. But because things can happen at the same time, you actually build conflicts. And so, you know, one of the core, you know, really hard parts of Nexus appearing from a technology perspective, but then also from sort of a user perspective is communicating conflicts, sort of helping the user understand when there was a conflict, how that conflict arose, and then trying to sort of build the system such that it encodes as much intent about what the user meant to happen as possible. And why is that important on the battlefield? So, you know, I think the biggest reason is because, you know, if you don't have that, you're going to smash conflicting edits in ways that just, you know, override one another. And so if you lose that intent, if you lose sort of like, well, why did I change the data? Why did you change the data? We're going to make these arbitrary conflict resolution choices that sort of kind of result in either, you know, you win because you happen after me or I win because I came before you, but really like helping the user understand it. And we have a pretty multimodal understanding of both intent and conflict resolution. Being able to sort of express the policy that gets everyone on the same page, like, you know, ultimately all this is about how do we provide a consistent view to everyone, right? Well, maybe another way of putting is if you're in a small unit, you better have a unique but consistent understanding of the worldview before you go behind enemy lines and in a situation where getting caught largely means a slow death. Right. And so, you know, you have to have that an absolute consistent understanding of what it means, what does X mean in regards to Y? Right. What is the concept of an operational transform? What is a transform? And why is that also important? Yeah. So, you know, you could sort of think of two ways of communicating about the changing of data, right? I can send you snapshots, which is sort of the full corpus and I can send you the whole thing or I can send you diffs, right? I can send you the edits, the changes I'm making. Being able to send the changes, they're much smaller and they're much more intent driven. So if I give you a snapshot of my data, you can compare it against your data, but you can't know what I did to it. You can't know sort of how did I change it, right? It's sort of a classic example in a database is, you know, if the value is three and I tell you the value is four, you know, maybe I added one or maybe I added four and subtracted three, right? And so if we're trying to make edits to that field, knowing the exact edits that I'm making, sort of at a diff level, at an atomic level of changes, sort of a difference. Yeah, yeah, just like an atomic unit of change. You just have to make sure. Yeah, most people watching this video are not writing code, but yeah. And so OT is sort of a system that was designed and, you know, theoretically implemented in the 80s, but never really brought to practical fruition operational transforms, that is. And it's a way of basically encoding sort of the intent behind the editor with the mutation. So I can, you know, encode as much information as I want and then sort of transform it against the state. And so if you get my sort of operations, you can then transform them. Well, the lay audience often can believe data is like just a natural resource that flows out in the processing of the natural resource from something that is maybe coming from the ground to something that's pre-usable through transforms to then is actually usable through management of the transforms is just kind of very much under the hood. Yeah, and I think historically, you know, I think a lot of what we've been doing that I'm really proud of that I think is really, really special is a lot of these things were just back end things. You know, there are things that database people talked about that developers talked about, but actually building a system that brings it to the user and actually makes them user-facing things. You know, that's the art of sort of connecting the back end with the front end, which is I think a lot of what we've done. And then on your experience kind of running this like without obviously going into details that we can't share publicly, but like at a high level, like what is the impact of this in things you've seen? Yeah. I mean, I think the biggest impact is just dynamism. Like so many of these synchronization systems require you to know how you're going to collaborate before you turn them on. And you don't know how you're going to collaborate before you turn the systems on. And so being able to connect the systems after they're already on, after they're already running, after people are, you know, using them independently, collaborating independently, being able to build that bridge. Like to me, that's the thing that really, you know, you could build software in a lot of different ways. But I think the way we built Nexus Pairing was really quite general purpose. We didn't, you know, we had a couple of cases to work with, but we built a key that wasn't fit for a single lock. You know, we built a key that actually can go in a bunch of locks. And so you can actually collaborate in a super dynamic way, a way that, you know, today you want to collaborate tomorrow. You don't, you can turn it off. As an example, if you were, it might be particularly relevant in current or current conditions. If American as adversaries were collaborating in a wartime situation because of the things you mentioned, security, modalities, the vector clock problem, and then the ability to manage operational transport actually leads to a context word allies that may not want to share everything or organizations that may not want to share everything can collaborate in a way that our adversaries otherwise couldn't because they don't have these tools. Exactly. And a year ago, we may not have wanted to collaborate that way, right? Like I think the dynamism of being able to turn it on and then turn it off. And modulate and modulate exactly. So moving from, you know, a partitioned world to an unpartitioned world back to a partitioned world. I think the modulation that you bring up is like in a, in, in, in world conditions that are like where the macro waves are just changing the world, you basically need a back end that will allow you to modulate according to them in either direction. And yeah, I mean, I went by the way, just another thing I'm particularly proud of is we didn't, we start building this in like 2008, 2008. Yeah. Before I joined, you know, it was the first team I worked on. I worked on it for 10 years. Shout out to Bob McGrew. Bob McGrew, John Carino, Catherine Breinard, you know, a ton of great people. Yeah, yeah. Yep. Just a ton of great people. And I think, yeah, I just, it's, I think with really great technology, it doesn't know what it's going to do when you build it. You know, you build it in a way, you build it in a way that sort of generalizes to do things you don't even intend for it to do in the future. One of the things I'm very proud of is this 2008 and we must have, first of all, we, I remember working on the problem, we're talking to Bob about it. We had like six months before we figured out how we could solve the problem with basically bringing the vector clock. And then when do you think this basically was a product like I would say like two, three years later? Yeah. But then you've kind of played, yeah, probably 2012, 2013. That's the first version of it that really felt pretty, pretty rigorously ready to go. I think the big shift since then has been moving it from pretty asynchronous, really relatively high latency to really real time. That's where the operational transforms came in. And yeah, so moving from, you know, collaborating on units of minutes or hours down to some second while remaining that adaptability so that if you don't have those sort of communication channels, you can still use the technology. Yeah. But then, you know, a 2008 piece of unknown technology that is powering the world in 2022, that's very cool. It's really cool. And I think we've learned, you know, I think in a lot of ways from a technical perspective, there's sort of two different types of complexity that we're grappling with. And in 2008, it was really a systems level complexity. Like, can you make it work just at a very basic level? Since then, it's been almost like an evolutionary complexity. So a lot of these systems calcify, right? Because what you've basically done is you've sort of forced two systems to be able to communicate and so they can't evolve as quickly. I think with something like Apollo, so when I was working on this a few years ago and we released Apollo, it was just a huge step change in terms of sort of decalsifying the system, making it so that you could actually release every week, which is a whole different type of complexity. So the system is running, but you're evolving it on both sides across a multi-node mesh. Those pieces have to move synchronously. Okay. I think this is really good. Cool.