================================================================================
METADATA
================================================================================
Title: Palantir | Visual Navigation for Drones
URL: https://www.youtube.com/watch?v=Ha0KrXzI4SM
Published: 2024-10-14
View Count: 42,660
Duration: 349 seconds
Score: 60.0/100
Rank: B (Basic - 基础背景资料)
Transcript: Available
Category: 
Source: whisper

================================================================================
TRANSCRIPT
================================================================================

The state of the world today is that you just cannot rely on GPS anymore to do your flights. Using visual navigation to provide an alternate GPS source in areas where there may be conflict. This is Palantir Software running on commercial hardware, so we've partnered with some really great drone companies where we're able to provide this joint demonstration, and then we're also tying it back into the rest of the Palantir platform. Everything that you're going to see up on the screen is running on a little compute device like this. This is an Nvidia Jetson Orin. This is just an off-the-shelf consumer-grade USB camera in a 3D printed housing that we just stuck to the drone to use for our ND. So all this is very configurable, hot-swappable, depending on your mission needs. All this is then going to feed into the visual navigation algorithm, which will be running the positioning when you don't have GPS. The first component is the telemetry coming off the drone. This is stuff like the compass information, the accelerometers, the barometer that gives you the altitude of the drone. All this stuff, the drone produces naturally as it flies. The second component is what's known as optical flow or visual odometry. So this is a technique where you watch the motion of the video feed coming off the camera. So frame, frame, frame, moving left to right. You can see if the video is moving left to right. The drone is also moving left to right. By applying some mathematical transforms to that data, you can extract the speed, the velocity of the drone and use that to calculate the drone's position over time. By combining these two things together, you can say, okay, I knew the drone was here. Ten seconds later, it's going to be over here. For short navigation, that might be all you need. And indeed, this is where a lot of other companies doing visual sorts of navigation work stop there. It works over short distances, but as you go over time, a little inaccuracies build up and where you are within two meters to start. Now you're within 20 meters. Now you're 100 meters. Now you're completely lost. So you need something else to fix this. That's where the third component comes in. This is what we call our reference matching technology. So the basic idea is simple. You take images coming off the drone and you match them up with pre-rendered satellite imagery that you've collected. And by using computer vision, you can determine where those images align and from that extract where the position of the drone must be. Easy to say, hard to do in practice. And this is where the special magic of the component comes in. Because this is able to over time correct whatever drift you build up in the drone position from those other components. So you can fly miles and miles. And since you're always checking your position against the map, you don't get off course, you don't drift. You stay locked on target. So all these three data sources then come in into what's known as a callman filter, which is a mathematical construct for combining different sorts of sensor data and giving you the best result out. This will then tell us where we think the drone is. And from there, we can send messages to the drone controller to say, this is where you are. And the drone controller then gets to decide how to control the drone to navigate the pre-programmed mission. So in this specific scenario, the target is right outside of a known GPS jamming site. So the nice thing about having this software on board is we're able to fly directly through that zone, even though we're getting our GPS jammed and compromised, we're able to fly through it because we're going to switch on our GPS visual navigation software and actually use that as an alternate GPS source and not rely on this other source that's going to be unreliable because of the jamming going on. The green areas are showing known GPS jamming sites. These are obviously what we're very focused on today. The orange areas are showing known area defense zones. So this is going to be where there's surface to air missile sites. There's other sorts of air defense systems that are planted there. And then there's some other intelligence data that we have layered on here. What we're showing now is actually this drone and proposed flights through this battle space and through this area. It's proposing us these different flight options and we can kind of look through this and see what we think is best. Now we're showing some wind forecast and we're showing some weather data coming into this that we can also incorporate into this model and into our analysis here. Once we're comfortable with this and we're actually good with this flight path through this area, what we can do is and you'll start to see as we can package this up and then actually like push it out onto the drone. All right, we are now watching the onboard recording of the drone as it flies through the mission. So what you're seeing in the top left is the video feed coming directly from the drone. This is the feed we're using for visual navigation. That's being matched against the top right, which is our satellite data that we've loaded on the drone in advance. You can see all these green lines showing where the algorithm has identified tie points between these two sets of imagery. As it progresses, you're going to see the matches strong right now. It'll be phases where it gets a little weaker, it goes yellow, we lose it entirely, goes red and through all of this, the algorithm is going to use the different data sources available to it. So telemetry, optical flow, reference match to keep the drone on track as one or the other source becomes unreliable. A lot of consumer hardware is designed around the assumption of GPS, which gives a certain signal quality. When you're doing a live visual navigation flight profile, it needs to control a little bit differently. So you end up with a bit more wobble in the flight path, but it's still locked on target. It's still making its way to the destination. You can see, we've turned the corner. We're flying over a natural environment once again with a mix of trails, but we're about to leave this natural environment and hit a man-made parking lot style environment. We've now finished our mission here. You can see down at the bottom, we have the profile of how strong our reference lock was from time to time. We're now descending and in just a moment we're going to switch back to manual control to land the drone safely.