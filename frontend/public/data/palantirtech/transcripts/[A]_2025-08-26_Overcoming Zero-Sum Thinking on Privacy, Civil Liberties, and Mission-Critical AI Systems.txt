================================================================================
METADATA
================================================================================
Title: Overcoming Zero-Sum Thinking on Privacy, Civil Liberties, and Mission-Critical AI Systems
URL: https://www.youtube.com/watch?v=x-NEdIcgboo
Published: 2025-08-26
View Count: 4,523
Duration: 3903 seconds
Score: 76.0/100
Rank: A (Active - 高参考价值)
Transcript: Available
Category: 
Source: youtube

================================================================================
TRANSCRIPT
================================================================================

All right. Well, I think we're going to kick off here. Courtney Bowman, thanks for taking the time to talk with me today. Any time. Yeah. I think that what we thought we would do today is, you know, a bit of a telling of the saga of privacy and civil liberties since the inception of Palantir, how it's evolved as we've expanded throughout the different mission sets across both government and commercial and kind of going to some of the details and specifically like the technology that we've had to build to support these missions, not just the approach or the theory that that's important as well as we'll talk about. But maybe to kick off, we can start with just a brief introduction as to who you are. Yeah. So I'm Courtney. I am technically the global director of the privacy and civil liberties engineering team. I've been in Palantir since late 2010. I was one of the founding members of the PCL team. We call it the PCL team. And I came to Palantir from Google where I've been working on, add the auction, design, and theory. So kind of a entirely different, removed world from what I focus on at Palantir. But I came to Palantir because I was particularly interested in the mission set. And I found it very intriguing that the company was focused at the time on kind of building a formal practice around privacy engineering. So that was enough of enticement to draw me in. And so over the years, I spent a lot of time in different parts of the business. I started off early on in my role in the PCL team, focusing a lot on our state local government work and law enforcement agencies and learned a lot there. And I think we'll have a chance to talk about some of those insights. But over the years, I've seen the mission set of the PCL team expand and branch out into different parts of the evolving business. And that's been a super cool journey to be a part of. I think that's awesome. And I think the eclectic background in some ways matches a lot of what attracted me to Palantir at this point almost 13 years ago. So I was doing biomedical work. And I was like contemplating whether or not to go to grad school. And yet friends were coming out to Silicon Valley and working at some of the big places. But at the time, as you'll remember, a lot of the like the other side, if you were working at a big tech firm was to be going join a consumer app startup or do maybe the precursors to be to be sass. And it was just the sheer concentration of smart, interesting people at Palantir. But the time of working on a fairly narrow set of missions, but still very meaningful missions. So it was working defense intelligence. It was anti-human trafficking work. It was anti-fraud work. It was working global health and in resilience. And in some ways, it's still kind of a very simple formula where it's like very smart people, very interesting technical problems and very ambitious missions. And that's what drew me here and what I think still motivates me to this day as I work across the architecture and product teams. So yeah, I think like maybe you alluded to it, but is it worth going into a bit of kind of the founding of Palantir and some of that early theory that informed some of the first things we've built in the PCL space? Yeah. I think folks generally have some sense of the lore of the the origin story of Palantir that the concept came out of the founders who worked at PayPal, but they had some set of insights around the utility of purely machine-driven approaches to dealing with complex problems, especially dealing with anti-fraud detection and a payment system. But taking those insights and applying them to a post-9-11 environment where there's clear observations that there were intelligence failings and that the intelligence community needed to reform, but it needed to reform in a way that addressed core concerns of constituencies of the American public. And so that sort of foundational insight kind of congealed into some key commitments that drive the company forward. Foremost, the reason why I have a job in a gamefully employed 15 years hence is this sort of the sort of realization that the extent to which Palantir is going to be building powerful technologies that are provided to powerful institutions, we don't want that work to come at the expense of the protection of fundamental rights, especially privacy and civil liberties and interests. And part of the difficulty of that is that the traditional kind of mindset of approaching this issue set is that there's a kind of zero-sum universe in which either you're aiming to protect security interests of the nation or you're going to protect privacy interests of the citizens. And our observation and sort of our challenge to that governing ethos is that that's a false economy. And there are ways of through intelligent approaches to product design, through thoughtful approaches to understanding the techno social dimensions of technology development. There are ways of kind of breaking down that zero-sum set of assumptions and build towards multiple optimization parameters. And that's kind of been one of the driving impulses of our work broadly within Palantir and then the central commitment of the PCL team is to help make that real in practice. Yeah, I think a lot of what you said about trying to resolve those contradictions between more security while having more privacy was the most compelling part for a lot of people on the engineering side as to why I joined Palantir. How do you build a system architecture which can maximize security and therefore like the power of analysis? So it's not just low data volume, limited analysis tooling, but I need to do a combination of graph-oriented document processing, geospatial analysis, top-down needle into haystack analysis. And these all have to work across essentially different compute frameworks and allow for different users with different scopes to be able to access different information. And of course that all has to work together in mesh with very robust security architectures that allow you to do that analysis when you should and under the circumstances that are allowed. So it's not just simple role-based access controls, but it's having investigation, authorizations, and scopes. It's being able to understand if you and I both give pieces of information that enrich a certain object or entity we know where that came from. So it's this immutable logging framework. And dealing in those tensions I think was, I mean, even from the earliest Gotham days, I think the kind of motivating force. And I think about all the resolving of contradictions that came after in other phases of the product. And so that's been a theme thread, I think, the entire product history of Palantir. Yeah. And it's, you know, I think one of the the insights that I've developed over the years is that you can be concerned about protecting privacy in the development of a powerful information technology platform. But the extent to what you try to do as a bolt-on, as well as an afterthought, it's going to fail. Yeah. Because it has to be deeply integrated into the core infrastructure and the primitives of how everything else works. Otherwise, things are going to fall through the seams and the cracks. Absolutely. I'm wondering if you have a few examples or things that stand out from kind of this early era of use cases that work pretty well and ones that maybe didn't work as well, but we maybe we visited later on. Yeah. So we spent a lot of, as I mentioned, I spent a few of my, a big chunk of my initial period of Palantir focusing on one of the core areas of the business at the time, which was our state and local government work. So primarily law enforcement agencies at the state and local level in some fusion centers throughout the Western states. And this is already a complex sense of environment where there's pretty core privacy concerns and privacy interests that motivate the work, dealing with a complex environment where there's different classes of information that's brought together in different environments. And this is a space where we kind of had to figure out how to make it work with the security infrastructure and the access control model to ensure that data was limited for specific purposes. One thing that we really focused on in those early years is some particular provisions for managing specific classes of law enforcement information. So criminal intelligence has particular requirements for how it has to be acquired, how it has to be managed over time, and then more critically how it has to be deleted after a certain period. So for some, some sets of this data, it's subject to a five-year retention rule. And if there's not a continuing nexus to criminal suspicion or activity at five years, the data has to go out the door. So we spent a lot of time kind of modeling different frameworks for tracking that information, tying it to ongoing casework, and then at the end of the cycle, making a clear determination whether it was consistent with holding requirements or whether it had to be deleted. So that figured into several different components of how we think about the security and privacy architecture. It's the access control model, it's the selective revelation of data, it's the retention rules, and then on the back end it's sort of the tracking all those pieces from an audit perspective to make sure that you can have a accountable framework and say that yes, we handled this data appropriately. So we dealt with this with a number of different data sources, but specifically we built a kind of module around gang intelligence, which is a particularly sensitive subclass of data. And I'm pretty proud of the work that we did in the space. My retrospective sense of failure there was that we couldn't get more traction on this. We built a really powerful gang intelligence module that adhered very rigorously and did some advanced stuff in terms of dealing with some of the security and privacy and regulatory concerns, but we can never kind of get it off the ground from a broad adoption perspective. So that was both cool and a little bit disappointing. Another instance of work that we did is around another class of sense of information, automatic license plate reader data. So this is an area that's been subject to a lot of privacy advocacy in civil society concern. It's concern around sort of broad collection of publicly available information, where cars are moving and the ability to identify that and potentially create patterns of life analysis without criminal suspicion or limiting that information used for specific contexts. So we spent a lot of time and effort kind of building an approach to Alper data management that really focused on limiting the access and the use for specific applications, enforced retention timelines for holding the data for fixed periods, and then did a bunch of oversight capabilities to ensure that the system was used appropriately and only access for specific purposes that an end user would have to certify and that became part of the audit trail in the back end. And the kind of cool successful outcome of this is the provisions capabilities that we laid out in the development of this application of Alper capabilities. Basically became sort of the baseline for a California state bill. I think it was a California bill 34. Like all the provisions that we built into the system and publicly talked about were ultimately things that became embedded in state laws, the kind of the gold standard for dealing with the sense of class of law enforcement data. So that was a pretty cool indication of where we were able. I don't want to take credit that we directly informed all of the legislation, but I think we proved a model of how this can be done responsibly that other institutions could take on board. No, for sure. And I think as we'll talk about I think as we go through the other points in time, that kind of ability to work with lawmakers or at least inform the process is a recurring theme as well. When I think about this period, some of the interesting things that lasted, those things that we tried that didn't necessarily last, of course this was the genesis of the Pallantura ontology system, right? In its first incarnations. And so when we think about the applications and defense and intelligence, you know, it was being able to say you're not going to just deal in rows and columns of data, but you can actually have a semantic understanding of these people, these events, these locations, the routes you're taking, the information being gathered from the field versus what came maybe from source systems. And like it was, it's again, a very ergonomic way of accessing data that can allow analytical, operational, and technical users to all work together. And of course, there's cross applicability to that outside of these domains. And so we had some interesting expeditions, I would say, in anti-money laundering, in work that was anti-fraud with the kind of the Gotham ontology product, you know, I think colloquially kind of catch the bad guy type workflows, right? But fundamentally, the system was very designed for those defense and intelligence workflows, though there was a precursor to what would eventually get built later on that was more general. And I think that's also true for some of the workflows that actually ended up going into another product. So Gotham's early sibling was Metropolis, which was a finance oriented ontology. I think every penetrating smiles and things fondly in some ways about Metropolis. Again, very interesting in how I thought about times, series data, the domain specific ontology modeling in finance workflows, but was once again a precursor of what would come later. And so I think, you know, the kind of ethos, the forward deployed engineering of trying to see how it can work, what generalizes what doesn't was in full swing at this point in the early history. Yeah, I go off script a little bit, but like I remember one of the my earliest hack week projects with Andrew Gervin. Yes. So I was coming out of my former job, I've been working as a quantity of analysts on kind of auction stuff. It was doing a lot with numbers and modeling. And we, the project that we built out was an R helper for Gotham as a way of kind of wedging the pieces of standard mathematical statistical modeling into this ontologically based environment. And it kind of worked. We got a little bit to, we got a useful application that was useful for some versions of workflows, but I think it was a precursor of where we needed to develop the product down the road. I have maybe I'll end with my own Andrew Gervin shadow as well. My first day at Palantir, I sat down next to Andrew Gervin and he had in Gotham a custom helper he had built that showed the actual molecular 3D structure of some, I think, you know, biological precursors and he was really pushing the limits of what you could do, I think, with the ontology and the whole kind of architecture at the time. But I was amazed. And again, don't want to get ahead of ourselves. But seeing what that work was eventually able to become through successor architecture is was one of the most awesome things I think that I was able to kind of see firsthand. So maybe fast forwarding a bit now to, let's say the mid-2010s. And our expansion into the commercial sectors in earnest and our work across Europe. You know, this is when GDPR was really coming into being and this is when a lot of, as we'll talk about some of the other industry trends, we're kind of coalescing in kind of the cloud and data space. I'm curious you can speak to kind of this period, especially from the privacy and civil liberties perspective. Yeah, this is a really interesting period in modern information privacy. Because if you really want to unpack what was happening, you kind of have to go back to the the Snowden revelations in 2013. And this was a bit of an earth-shattering moment because it called attention to concerns around the intelligence community and access to data. Those concerns both identified specific issues with intelligence community, but also started to create fractures in the transatlantic agreements that allowed information to be shared across across the Atlantic by multinational agencies. And so that led to kind of breakdown with existing infrastructure that was in place that allowed the free flow of information. It led to a lot of negotiations between the European Commission and the Department of Commerce and other regulatory agencies in the US. And it really hypercharged Europe's attempt to come up with a kind of comprehensive consumer data protection regulation that spanned all the markets and that this extraterritorial application, which means that it applies outside of the continental Europe to any commercial entity and other types of entities that operate with respect to European residents and citizens data. So GDPR general data protection regulation started to go through multiple rounds of drafting in kind of 2016 timeframe and then was sort of solidified and came into force in 2018. And this was a kind of pivotal moment for us because we were following all these trends, but also on the back of the work and the development and insights that we had built up over years and years of working in highly sensitive environments in intelligence community, in defense, and law enforcement. We really had strong intuitions about how you need to structure information technology to manage a lot of these these concerns and risks. So in the US, we have a kind of traditional framework, the fair information practice principles, which include things like data minimization, selective revelation, use limitation, oversight and accountability principles. But all these things were things that we had embedded into the core infrastructure of Gotham and we kind of knew what it takes to make this work in a reliable trustworthy sense. And we sell these developments on the other side of the Atlantic. And this created a large opening for us to start to reach out to and develop relationships with commercial entities in Europe. And because of that, we were able to out of the gate kind of already offer core approaches and insights into how European companies and transnationals that were in other markets, including the US could could start to address these emerging set of concerns and provisions under GDPR. So there was a lot of kind of basic infrastructure that we had built early on that was kind of critical to how these systems could be made to work in this complex regulatory environments. And it was one of the reasons why we got so much traction in the European marketplace in the mid 2010s. No, I think that's really interesting. And it also dovetails a lot with kind of the more industry-wide trends we're seeing across the world. So at this point, I think about the 2016 to 2018 eras, a lot of commercial enterprises, especially we're saying we're going to start moving workloads to the cloud, at least the analytics or data ones. We now have a data strategy around trying to centralize data, building a data lake, building a data platform. And I think like the kind of motivation behind the motivation for many were, how do we basically unlock the value of all this information or data that's trapped in different parts of the enterprise and make that useful for operations, make that useful for strategy. And so this was the genesis of Foundry, which is our broad-based platform, which now underpins essentially all the workloads that we do worldwide. And of course Apollo underpins that. But you know, if I think about the principles from that first period with the original Gotham and Metropolis work that informed the design of Foundry, a lot of these same principles were once again imperative, right? So if you think about the need for granular security over data elements, so not just row level or column level access, but the intersection of these things, being able to understand the derivation and flow of data. So if you're deriving your information, how does security flow with that information, especially if you're now working across big multinational, cross-functional organizations, the importance of lineage, so always being able to dynamically understand where data inputs came from, who authored it, who ingested the data, which version of it was touched by which people, kind of the entire who done it, but at a very specific level of detail. And then all the things that you mentioned while on audit capabilities and being able to understand, especially as you think about these regulatory frameworks, how to post-hoc or in a dynamic way understand the usage of information, not just as the data resides in the platform, but as it's being used in applications or different analytical tools that are built. Yeah, let me double tap a couple of the things that you are calling out, because it's worth underscoring going back to this theme of how the primitives set up the infrastructure to be able to deal with complex regulatory obligations. Lineage and provenance are especially critical from a workflow perspective, but also from a data protection perspective. And go back to the discussion on GPR. One of the things that GDPR introduced or reaffirmed is a set of rights that individuals have and are able to assert with respect to how their data is held and managed by entities that are corporate entities and other public service entities. But this is fundamentally a really hard thing to do, because some examples of data subject rights or things like just access to the data that is transacted used process by a company. But more advanced rights like the right to be forgotten, the right to have my data if there's no longer legitimate purpose for handling it. These things are actually really hard to do in practice. If you're dealing with a complex institution that has sprawling data assets and infrastructure, lots of different workflows, branches in those workflows over time, and all sorts of endpoints that are really hard to track. And so having the kind of infrastructure to be able to do the lineage and provenance control is actually critical to being able to identify where an individual's data resides, where it's been transformed and processed over time. And to go into each of those components and select them and either return that as a access request or deleted if there's a deletion requirement. Absolutely. And if I think about some of the customers that were instrumental at this time to us entering the commercial market and growing, these were companies like BP, companies like Airbus, which had essentially footprints across every geography you can think of, oftentimes with very specific policies in place that were somewhat different across regions, especially even within Europe. And so these were not just workflows around saying we're going to put data in a place and hope that it's useful someday. These were highly operational workflows. It was how do we manage our oil wells or reservoirs, how do we ramp production on the A350. And what's interesting, and I think what's been core to our work is you want to enable organic cross-functional work where design might start with the use case but then assembly wants to chip in. Or you might have integrity do a use case and then you want production to be able to help. And how do you actually facilitate those kind of cross-functional workflows or these collaborative realms while adhering to all of the policies and dictates that are there for a reason. And so it was a really interesting tension or trying to resolve the contradictions again, trying to build these operational tools in the early days of foundry with these initial customers and also making sure that like we were meeting all the challenges crossing all the T's, dotting all the eyes when it came to the core infrastructure and privacy assurances. Yeah, and we saw lots of synergies in kind of building on the architecture across the full range of clients that we were expanding into both in the commercial realm but also in the international government realm. Right. So maybe one example, we work with a couple of European agencies that focus on customs and transportation. And so part of their mandate is to handle passengers that are moving in and out of the country and understand the flow of people and deal with highly sensitive records like passenger manifests. And their formal restrictions on how that data can be used and managed over time and how it can be applied for potential investigations if there's cause to maybe look into a specific passenger moving through. But because of the this infrastructure that we built and one of the kind of things that I was poking at earlier, the ability to be able to manage that information, identify where it was, how it's used and to have an architecture in place to be able to do reliable deletion. We were able to enable these institutions to carry out their responsibilities and limited use purpose-specific driven applications that ultimately once the data is done and no longer can be held can be reliably deleted to be consistent with with a regulatory framework. On the commercial side of the business at the same time, we were exploring a lot of work and doing a lot of work with highly regulated industries like financial services, insurance agencies. And we started to see precursors of the AI revolution that was to come. So increasingly, insurance agencies wanted to look at ways of understanding risk information and insurance agencies are kind of the longest standing, one of the longest standing institutions that do advanced statistical analysis to define risk pools. But increasingly, they were looking at ways of workflow improvements and understanding the risk profile of customers and doing claims processing. But already embedded in GDPR, I think it's Article 22, is a restriction on automated profiling, on purely automated driven workflows. And so we had to start to think about how do we enable kind of the or continue to enable the human and loop elements that allow these institutions to gain efficiency through the appropriate use of their data, but also do it in a way that's consistent with a regulatory obligations and their ethical obligations to protect the interests of the customers that they serve. Yeah, I think when I think back to this period, I just think of an explosive amount of product work to support all of these millions and seven objectives you talked about. So the birth of the sensitive data scanner, the ability to build a build system that could actually reconcile all these problems details. The checkpoints framework are the first versions of it to be able to understand when and how access was requested, who approved it, the entire chain of kind of events there, right? And I don't think about, I think one thing that's unique about Palantir always has felt unique is like these are all intertwined engineering endeavors alongside the core capability of sets around data management, application building, etc. These are not separate things off to the side and back to a point you made early on, you can't do these things as a bolt-on. Like they just won't work at scale or across the heterogeneity of data types if it's an afterthought. And so I think just the creativity you can unleash when you're face with these problems, I think is a testament to how you can end up solving them, I think. Yeah, and I think it's a testament to the approach that we've taken, which was very deliberate to focus on these issues. So all the examples that you mentioned, sensitive data scanner, checkpoints, cipher, all capabilities that were built out of the privacy and civil liberties engineering team through close collaboration with the architecture and product teams who are managing the broader constellation of foundry capabilities. But because of that sort of central awareness of these core issues that we have to tackle as a company, they're willing to invest in the primitives but have a specialized focus on why these are core concerns for us as a company. We were able to kind of build them in to the basic infrastructure and make them things that could be accessible across the broad suite of deployments. For sure. So now jumping ahead to COVID, I'm thinking about all the developments on the product side that happened during this period of time and all the new privacy and civil liberties engagements and ideas and things that came to the forefront during this period too. I'm curious if you can talk through a bit of your recollection from this period and maybe touch on the key points. Yeah, it's a bit of a dreamy period or I mean the nightmare is periods to reflect back on it. It seems a bit like a blur but it was I think it was a pivotal time for us as a company to gain access to we've always kind of prided ourselves on a willingness to charge into the really difficult issues and oftentimes gray area issues where there's lots of competing equities at stake and lots of concerns and this is a great example of public health interests pitted against concerns from the citizenry around the management of data and highly sensitive health records movement information. This was particularly fascinating from the perspective of somebody who's focused on on privacy concerns because one of the things that we observed outside of Palantir was this race towards kind of whizz-bang shiny solutions to dealing with this complex pandemic. Yes. And a specific example of this that folks may have either kind of tried to tune out of their memory or may still recall is there was a massive push in late 2020 early 2021 to build contact tracing tools and mobile applications that basically would help to identify potential interactions with people who are COVID positive. And the motivations it makes sense. This is a communicable disease you want to understand the patterns of interactions and be able to carry out public policy based on that understanding. So in some sense the movement towards using devices that people already carry with them might be a useful way of tracking the flow of the communicable disease. But we and there was a whole kind of set of questions around how do you build this? Is it there competing schools and philosophies and all the tech industry had sort of built up massive task force to track this? Was it centralized decentralized? Was it Bluetooth? Was it web-based? Every film provider, every social media app was paying you with some version of this? Yeah. Is there commercial services that are available that could be repurposed for these applications? Our intuition was to pump the brakes and take a step back and say, wait a second. Is this actually over-engineering a solution to a problem that can be addressed through more fundamental means? And specifically as we started to have earnest conversations with public health providers and understand the data landscape, the thing that we observe is that the real issues weren't around or at least the first order issues weren't around being able to trace movement of the potential carriers of the pathogen. It was really around there's a ton of data that's already available in the public health domain and public service providers, public health service providers have no idea how to reconcile with that information. So our approach was more to focus on information sources that were already out there that the kind of patchwork of agencies in the US, state agencies, in Europe, individual nation states and their sub agencies and medical service providers to be able to put that information in a kind of common operating picture environment that allowed public health officials to make informed decisions. So this COP concept, this common operating picture concept was actually built on pretty mundane data sources. It was more getting at what are the reported cases in a given jurisdiction. Do we know what the backlog of PPE is in a given hospital? Are there enough ventilators to deal with acute cases that are coming into the hospitals? These are kind of basic logistics questions that don't actually implicate too much or in some cases any privacy interests, but they are core to be able to doing this the kind of public health service that needed to take place in this time. And that was one of our key insights that you can actually do more with less in some instances. It's more about creating the framework to be able to do a responsible version of integrating data and then building insights out of that data. As opposed to let's go out and scoop up lots of really invasive data that potentially tracks people's movement and could be funneled to law enforcement agency to enforce lockdown or to do other nefarious things if that information is hacked. There's a lot more you can do with a basic information used responsibly in a comprehensive environment. And that I think drove a lot of our public health work during the COVID era. For sure. And when I think about that period of time through a product lens, I think about the cop and of course the work around these common operating pictures. And that forming the basis for some deeper work we did. So both with operation warp speed with the supply chain for vaccine production distribution. And then also with AGS Protect. And this was the PPE distribution value chain. So you had all of these medical centers, hospital systems. And there was a shortage of PPE. And you had the federal government trying to help adjudicate and help route supplies to the most critical facilities and locations across the country. And of course in all of those domains are lots of privacy implications. And they need to be very careful about what information is being shared with private entities under what circumstances. And then being able to actually do the operational work to actually conduct these workflows, right? A lot of logistics workflows, a lot of traditional supply chain workflows. But in the midst of simultaneous demand and supply shocks. So what's interesting, I think, that developed when I think about specific product evolutions or new features was a much more high fidelity ability to represent scenarios and simulations in the ontology system. So of course we could always capture edits and you could take actions in kind of a more atomic or even like kind of like one, two, three kind of way. But what do I do if I need to understand the implications of routing PPE to the Northeast versus our existing plan of doing a more uniform distribution? Let me play out the consequences by then layer on two or three additional assumptions. And again, this is a thing where if you want to do simulations on top of objects and actions that are shared by these cross functional teams without compromising on privacy or civil liberties, those things have to be big right into the core system primitives themselves. They can't be bolted on later. So that was one set of functionality that truly I think kind of came into being in this period of time. A lot of the ways you can build expressive applications in the platform. So our workshop application builder, some of the ways you can author more complex functions and business logic, all kind of had to tie together again in this nexus of do all this work as a situation is rapidly changing without compromising on our core principles. Yeah. So it was a very odd way of working in some ways because Palantir is so inductive is so hive mindy with product teams working together in every office. So not being able to co-locate and be together in the same way was challenging. But I feel like it unlocked a certain type of creativity in folks. And like you said, even in those situations, we still had forward deploy engineers going on site and being with customers in these agencies, in these hospital systems with these private companies as well. So it limited our ability to be together, but it didn't stop us from getting to the edge of the problem. Yeah. I think one of the other kind of this maybe, you know, hindsight is 2020 and having the benefit of where we are technologically today in the sort of AI boom. We started to see on the back of a lot of the applications that you were describing epidemiological research on advanced statistical modeling to understand pathogen movements, development of new pharmaceuticals, a kind of growing need to build robust infrastructure to manage modeling workflows. Yes. And this is a place where we started to work more in earnest on the kind of full suite of capabilities that should be in place to carry out responsible AI machine learning techniques. So mind you, this is before the large language model boom that started to happen late 2022, early 2023. We're talking about kind of classical machine learning techniques. And a lot of what we started to build at that time was the kind of full suite of tools that helped manage the ML, what's called ML ops kind of lifecycle. So that includes things like model objectives, defining what your objectives are and then using that as a way of doing version control and tracking so that you can go from validation training data to testing of a model in situ to rolling out in production, generating feedback and then feeding that back into the next iterations of the model. And then all of the while creating a kind of back end signature or set of information that helps track that information. So it's accountable to to oversight authorities or to regulators. But more critically deals with this really challenging issue in the machine learning space of reproducibility and in the health space of reproducibility. How do you actually go back and affirm that the model and the modeling techniques that you've pursued are consistent and produce consistent results. And so because of this work that we were able to build out in the space, we were able in the foundry ecosystem to build out a suite of tools that helped advance these types of applications. Absolutely. And around this time, there are also a lot of policy milestones occurring as well. I'm curious if you could talk a little bit about that. Yeah. And actually even before the kind of COVID period, we've had the benefit of having a lot of practical experiences and understanding how advanced technologies like AI and ML actually work in practice. And this is important because I think there's more than a crack of daylight between what you might read about as research and development AI capabilities and what it takes for this type of technology to work in complex environments where the variables are changing. There's constant evolution in the data environment. The use cases are evolving and you have to do with people in real settings. And that's where the intersection between the technology and public policy really matters because if you have public policy of laws that are developed based on misinformed or under informed perspective on what the technology can do, you either end up with policy that just doesn't attach to reality or is so ephemeral that it only is useful for a certain period and then you're stuck with this policy until you can manage to get the political class to work through the next evolution of the legislative frameworks. So this has been a kind of continuous motivation for us. How do we take the insights that we developed as a business that's embedded in the field that has a broad swath of customers and use that to inform policy makers and give them the best tools to be able to build the best policy? So even as early as 2018-2019, we were engaging with, for example, the European Commission, which at the time had put out an early white paper on AI regulation. This is well preceding the current EU AI Act as a sort of the precursor to that movement. We responded to that with some practical insights that we had developed in our own experiences. We had issued a publication to a Canadian government invitation to qualify, which basically was a request to outline how a Palantir's a company thinks about the broad landscape of evolving AI, including the ethical commitments that companies should take and the types of model life cycle considerations that should be built into responsible applications of this technology. We published a piece on this. We were responding to request for information from standards bodies like NIST, providing perspectives on bias and development of AI systems. All this is an opportunity for us to take the learnings that we build out in the field and give these back to the agencies and institutions that have to think very seriously about the next generation of oversight. Absolutely. One thing that I think about as dovetailing with this period of time, at least temporally, is the expansion of our defense work. We talked about how things started in Gotham, in core intelligence workflows. Now that, especially around this period, things started to evolve significantly. You're thinking about the entire set of interconnected workflows from intelligence to targeting development to strike and other considerations around kinetic or non-cannotic operations to battle damage assessments and everything in between and around those workflows. To make things more complex, you're often talking about multi-stakeholder engagement with potentially multi-nation collaborations happening. As this defense work really becomes much more full spectrum, I'm curious what you're thinking about from a privacy and civil liberties perspective. What were the key things that we had to start to take into account there? This is in the vein of volunteering. We run towards the hard problems. Specifically for a division of volunteer that is responsible for grappling with the really complex issues, we have to roll our sleeves and get into the nitty gritty of things that may be uncomfortable topics for some people to talk about, like kinetic warfare, which has consequential impact on people's lives. This is a motivation for us, the PCL team, to really start to build a practice and discipline around how do we engage with a customer set and with a product development on core considerations around integrating law of armed conflict or in Europe, we refer to the International Humanitarian Legal Framework. All these are sort of historical treaties or conventions that dictate how warfighters have to deal with the legal and humanitarian implications of what they do in the field. It has implications to how everyone from the operator to the command line and to the generals have to think about their personal obligations and responsibilities. We started to look at specific capabilities when the product and in the context of our decision support system work, which is essentially the Maven suite of tools is providing support for warfighters to carry out complex decisions, sometimes with lethal effect. How do we integrate into the different components of those workflows, the types of capabilities that address international humanitarian law considerations? So this includes things like identifying no strike list entities that need to be factored into targeting decisions and making that very apparent to the target here or the warfighter so that they avoid any undue or unnecessary risks to civilians or civilian infrastructure. That was just a kind of stepping stone into a broader range of areas where we saw a huge opportunity to expand the scope of the privacy, nominally privacy and civil liberties work into a broader space of ethical engineering, including the ethics of war fighting. Absolutely, and I think about a couple things you just mentioned. One is the Maven Smart System and what that required on the technology and architecture side. So we're talking about the real integration of kind of the full suite of imagery and full motion video and there's a lot of technical challenges when it comes to being able to do sense, refusion and integration across all those domains while also making sure you're hearing again like we talked about to all the dynamic lineage and privacy systems that are inside of the platform. And then there are questions about how do you integrate these new data modalities with what you talked about previously with modeling objectives. So you now have this, you're trying to do entity extraction or other types of modeling workflows on top of all these new data sources to inform operations. And these could be models coming from many different vendors, this could be models coming from many different in-house teams. But you want to have a common backbone of how you're doing governance and how you're tracing data to model release to feedback. And so it's this kind of composite system that has to work together while allowing a lot of diverse partnerships in pursuit of the mission. And it's even more complicated than that because you have all these interoperability challenges with different agencies, different institutions across different jurisdictions. And they're all responsible for their own protocols and managing sensitive information. So another capability that was sort of an extension of the existing access control model was C-Back classification-based access control that allows our platform to be able to manage the classifications and the corresponding restrictions on data and Intel and work products that come out of the system, not just within a given defense agency, but across partner agencies as they as they work on a common environment. And this is a particularly challenging problem to to address in environments where you don't have the time to be able to do the mapping in real time. You need to rely on the system to be able to understand what the restrictions are on a given information because there are real consequences if you overexposed data across jurisdictions or across different workflows. Definitely. And I think about the fluidity that often is part and parcel with doing these workflows in the field. So you mentioned kind of the diversity of new workflows after you built on top of the core platform as you're operating. So how do we make sure that we're not just dealing with existing no-strike lists, but we can actually modify them or add to them if there's new intelligence about potential civilian harm? Okay, so how do you enable actual users who might be semi-technical to be able to add or maybe non-technical to add in that information in a secure way? And as they do so, it is compliant with C-Back or markings that flow with information. If I then want to use that in a completely different tool set, how do I make sure those controls are applied to that user author data the same way it would have been from any of those sensor feeds coming in? And then if there are different or mixed permissions and we're sharing this with a partner nation, how does that all get reconciled? These are all the interesting plumbing problems that are so critical to being able to do anything effectively in the field. All right, so now let's fast forward to generative AI. And the advent of this current revolution. I think about this again through the product lens of being the culmination of so many investments that we've made and that we've talked about here in this conversation. So our investment in the ontology system, our investment in these granular controls across privacy and civil liberties and data protection, the way that we thought about multimodal compute, the way that we thought about being able to simulate and hand off decisions between different stakeholders. There was so much more to build, of course, but in many ways this was the moment where a lot of our technical investments in our platforms had to meet their moment. And I'm curious kind of what comes to mind for you from the operational AI perspective writ large, given we're still in it to a degree in this moment. Yeah, so our kind of general philosophical disposition towards AI has been in characterizing some circles as an AI skepticism, but I don't think that's quite right. I think it's something closer to an AI realism. And it's built on a set of ideas that we've been ruminating over for years and years. And in fact, goes back to the earliest days of a palantir where we were challenging the presumptions of being able to do these Neil and Haystack problems through machines alone and having to integrate and build on kind of human computer symbiosis to take the best of human intuitions and the best of what machines can do in processing data at scale and marry those things together in a way that's defensible and responsible. And for years before AI was the topic to juror before large language models in generative AI came into the fore. Anthony Bakkenai used to do this this this university lecture. I think that had various titles, but it was typically referred to as the limits of AI and basically trying to describe what AI actually is, what it's not, and how it can be useful and powerful. And one of the driving insights behind all of this was AI is a really impressive technology, divorce from reality. If you're just talking about laboratory settings or just in research institutions, you can do some pretty cool parlor tricks in close environments. But the real kind of tests, the real metal where you demonstrate where the AI works is in the operational context. And that's all the gritty challenges of confronting technology with reality. That's actually the hard engineering problem. And there's a confusion that comes out of this. And it's a confusion that I think is reflecting in both the understanding of what AI can do and also in the discourse around what responsible or ethical AI is. And so around the time that that balance here really kind of taken a shift and moved in direction of building out AIP and building a advanced practice on integrating generative AI capabilities, I had published a blog post about this sort of need to reframe the problems of efficacious AI and ethical AI. Because really when you get down to it and you get into the operational realities of what makes AI works, there's actually a pretty strong confluence of concerns and considerations and practices that apply to both ethical implementation of AI and AI that works. And this is sort of a grounding set of insights that I think has driven a lot of our more continued development work on AI lifecycle management, beyond classical AI into large language model applications. So that's been kind of a thing that I continue to believe very strongly. And I think it's been a driving impulse on how we within the PCL team and I think of the broader culture of balance here, it thinks about AI development in our products. Once again about resolving contradictions. And I think we couldn't have totally predicted, though I think some of the antecedent moments that we saw through the maven work and some of the DOD work informed not in the literal architectural sense, but kind of what to expect from a, how do you operationalize highly capable models and AI systems. And the word that we say a lot in people give us a lot of flag for sometimes is ontology, right? And what's interesting is the expressiveness in the ontology system that had to meet its moment here with the advent of large language models because you have these models that are highly capable, but highly divorced from your individual context. And so if they're trained on human language, how do you actually give them understanding and not just essentially lexical proficiency? And so that means in the ontology system, which started originally as maybe just data and some amount of action modeled in a more compartmentalized way for the initial Gotham workflows and metropolis workflows, which then evolved an expression as we talked about through commercial and COVID to have more elaborate ways of defining action and simulation to now encompassing logic in ways of you can actually embody or represent reasoning in the ontology system. Like that becomes this sort of nexus of understanding for both the human beings and the models. And I think that was sort of the click moment where like, oh, this could be that actual shared substrate between humans and AI, which actually does the forefold integration of different data, different logic, different action and security and data protection as well. And unless you basically have that core or you have that way that you're transducing human and AI action together, you're not going to find a way, this art conviction to actually operationalize these models in a way that is compliant with your norms and actually can be measured in a normative way against the value that you care about. And so that has been the journey with AIP and we're still very early in it. But I think about all of the things that we've had to evolve in the existing architecture in addition to all the new stuff we've built. So all of the optimizers or traditional models you might have brought in in prior era to the model ops suite now can become tools for AI. Right. So in that way, we're almost like a maximalist AI company, right? Because you're using both generative AI and traditional ML tools and techniques. All the data protection frameworks that were just for users or handoff points, checkpoints, approvals that existed among different teams that were interdisciplinary or required approval processes now apply to AI agents potentially handing off actions for review prior to being submitted. There's just all these interesting symmetries and ways that things have kind of come full circle with how we're building these systems now for both humans and for our agentic friends. And kind of the journey that we go on now, I think with a lot of customers is well, how do you incrementally deliver more autonomy every day in the workflows or not losing control and while not losing a sense of what matters and what differentiates your company? Yeah. And to you know, go back to the resolving contradictions point that this concept of AIP observability is really is really both encapsulating what it takes for AI to work in situ and also addressing these normative concerns about what's the AI doing? Can I trust it? Is it explainable? Is there enough transparency into the system? Can I do a half the level of accountability that I can go back and make assertions to regulators or oversight authorities? Those are kinds of critical capabilities that have to be built into the workflows to be able to build the institutional trust that make these systems reliable. One of the other things that we spent a lot of effort kind of building out and is on the back of our prior work on classical ML testing evaluation frameworks is the sort of the testing evaluation validation and maintenance components because beyond the problem of where do you make the handoff between AI agents or other capabilities and other tools in the toolkit? You have to have a pretty nimble framework to evaluate in the specific context of application whether the AI tool is doing the thing that you wanted to do. So we've built the infrastructure and the different types of patterns and models that can be used to do this discrete testing to understand the specific risk profile of a given AI application to find metrics to assert whether you're doing the thing that you need to do and then have a reproducible framework so that as the model develops over time or as the applications change over time or the data environment changes over time you can go back and continually evaluate. You have to recognize consistently that there's a brittleness that's built into these systems and this maintenance component addresses the long tail of application which again gets into this point of you may have flashy applications that work up front but the institutional trusts around their use in application these normative considerations, these societal considerations are an enduring thing and if there's erosion in the application of the system and it's not working as well as it did at the time of product rollout six months later you have a real problem you have a erosion of trust problems so all these features are kind of core components of what AIP needs to have in place in order to to work. And I think this is again of I think back to our early days and what mirrors to now it's the ambition of that problem that I think attracts a lot of people and is very motivating to our product teams and to people who are looking at Palantir and wanting to work with us and if I think about some of the use case examples that kind of dovetail off of what you're saying I think about you know a couple hospital systems that are trying to integrate agents into critical workflows like nurse scheduling or capacity management or patient care and of course you're not going to flip a bit and just have an agent do stuff there nobody would think that's acceptable nobody would find that useful but can you have a system a shared harness where the master nurse schedulers who are experts at this can have AI start to be incrementally useful for maybe one part of their triage workflow and then maybe start to propose possible schedules or changes that could make their lives easier so not doing as much manual work and they can focus on amplifying their human expertise right we talk about giving human being superpowers through this software and I think about you know recruiting workflows where it's important to try to automate what you can but it's really important to make sure that bias and unintended factors are also controlled as you're doing it's that observability point is critical to even getting to the starting line with those workflows and I think in each of these cases it looks much more like turning the wrench incrementally towards an epic outcome and I think much faster than people think than it is some sort of magic genie that like you said could give a whole new meaning to the words model drift if it starts to do things you don't expect and all along the way it's it's about kind of rebuilding and reinforcing the trust cycles so this is again why the testing frameworks are so important because if you can incrementally demonstrate that the additional capabilities that you're offering are reliable capabilities not just from the perspective of meeting core metrics or benchmarks but more critically from producing operational outcomes that people can see or useful and are consistent with their kind of analog expectations of how a workflow should be carried out that's how you get people more comfortable with the system with its with its reliability and thinking in a mode of how does how do they kind of advance their their own their own responsibilities and build out more capacity and I think maybe just one final note on that is that that what you just described is a very compelling technical challenge because we've talked about data lineage but now really it's decision lineage yeah it's how do I trace every piece of information that informed every piece of the ontology that was used by these models handed off to a user potentially used to inform a task and then maybe resulted in a piece of feedback and you know that's a very interconnected system which I run time could be doing things very differently between executions and so in some ways it it feels like a continuous journey in another ways it's just much more ambitious than anything we could have probably imagined in the early days yeah and as that ambition expands and we see larger adoption and we see institutions take on more and more advanced applications of of AI technologies while at the same time there's increasingly complex and rigorous governance and regulatory environments there's a need to provide the the the kind of framework for governance of all those applications this is why we built and have continued to build out capabilities around we call the AI use case manager so it's basically kind of a governance tool that helps track all these complex applications the multi stakeholder environment against the specific regulatory or oversight requirements and be able over time to identify the consistency of the AI applications and the changing models and different contexts in which they're applied against all of these these requirements so that's been a cool thing to demonstrate and has gone a lot of traction across both our government and commercial businesses all right so maybe just to wrap up Courtney I thought we would maybe close out with some things we're excited about with some forthcoming things that people should look out for yep I think maybe one thing I can share is on the developer tool chain side it's really exciting times so you know we went from not really having any developer conferences you're much of a developer presence to doing three developer conferences under a year and I think it's been really explosive inbound demand you know with our startup cohorts and with builders inside of companies building their AI enabled products and services on top of foundry AIP and so I think about the ontology SDK and everything that's forthcoming there pound your MCP so you can build agentically against our systems in a secure way and all the capabilities that we're building into the core platforms from autopilot to all the new observability tooling that's really going to make it I think the equivalent of what hyper-scaler building was maybe in the mid-2010s for kind of normative builders for AI builders I think in this next generation so stay tuned for a bunch on that front but I'm curious what's on your mind yeah so lots of things that we're focused on within the PCL team but kind of picking up on some of the things that we've been talking about the space that everyone's interested in in AIP land I talked about the AI use case manager and there's just a huge open space of Greenfield development work that we're focused on in the months and potentially years ahead so near term we're super excited about again going back to the primitives how do we take the concept of ontology and build that into the governance framework of AI applications because there are consistent concepts that people have to think about their concepts that relate to the regulatory policy landscape and all that can be translated into applications of AI technology in the platform and the governance of those AI components so that's super cool and then sort of building on that we can with the ontology create more governance tools for builders within the platform so how do you take consistent concepts or templatized model oversight paradigms and then translate that into tools that builders in the platform can use to make sure that they're consistent with or their institutions are consistent with their regulatory obligations and then even building more on top of that is what are the potential applications of generative AI for AI governance within the AI use case manager are there ways that we can draw upon agent components to be able to help facilitate some of the governance requirements and the key insight here the thing that we're constantly being mindful of is we don't want to overstep in terms of in terms of assuming too much capability but there are instances where there's repeated patterns and learnings that can be translated into agent applications that can help facilitate institutions and oversight authorities abilities to manage their AI use cases responsibly so those areas of development are super cool and we're excited to build them out so maybe is fair to say our latest contradiction we're tackling is more AI enabled building or more building in general ways to build with better governance yeah that's right awesome well I can't think of a better way to end up than that so thank you Courtney for taking the time to chat today no it's been super fun thank you actually