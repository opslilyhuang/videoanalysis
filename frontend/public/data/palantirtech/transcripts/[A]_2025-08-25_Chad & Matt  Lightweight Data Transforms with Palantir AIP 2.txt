================================================================================
METADATA
================================================================================
Title: Chad & Matt | Lightweight Data Transforms with Palantir AIP
URL: https://www.youtube.com/watch?v=MITSJDI08R4
Published: 2025-08-25
View Count: 5,943
Duration: 1823 seconds
Score: 76.0/100
Rank: A (Active - 高参考价值)
Transcript: Available
Category: 
Source: youtube

================================================================================
TRANSCRIPT
================================================================================

Hi, I'm Chad Walquist. I'm an architect at Palantir. Today I've got Matthew Behr, who's a Ford deployed engineer or FDE on our Compute engines team. Thanks for joining me. Great. Thanks, Jack. Great to be here. So Compute engines, what is that? Yeah, so we build a lot of infrastructure that transforms data across the platform within Foundry. So if you think about anything you're doing that involves tabular data transformations, we're owning that. So whether it's a normal Spark transform or a transform in pipeline builder or lightweight transforms, which we're going to chat about today, we're sort of that backing engine for all of it. Yeah, and maybe just everyone can understand the framing. We talk about the multimodal data plan, which is really a huge set of features we're deploying around accelerating data processing to get to AI and autonomy faster. So you know, we have things like our data bricks integration, we have compute push down, there's virtual tables, all these different pieces and components. Today with the lightweight transforms, I think this is really about how do we accelerate the data transformations on the small and medium size? How do we actually reduce cost, increased speed, really accelerating people's journey towards the AI and autonomy pieces, right? Exactly. And so we've had a few trends sort of moving us in this direction for a while. So if you look back in history, we and you know, much of the data engineering community was intensely focused on Spark. The reason being it was sort of the most economic way to transform your data. So you'd have distributed compute a bunch of small nodes working together to together, you know, do a large transform and underlie all of your data pipelines. But that really has changed over the past couple years for a few developments. So the first is single node compute is a lot more efficient now. So as as compute and memory costs have come down, we've seen these form factors that are ideal, changed where you know, you want to look at a single node rather than something like distributed Spark. On the flip side, we have the AI revolution where folks are doing much more interesting things with their data than they were previously with just a standard Spark pipeline. So what a lot of people are looking for is something highly flexible and generic where they can run whatever compute engine, whatever AI workflow on really flexible data scales to encompass all these new developments. And so what we've been building with lightweight transforms is the answer to that it's for folks who don't want to have the overhead of a large distributed compute cluster with Spark. Folks who want to enable AI focused development and pipeline deployment wrapped all together in one experience. Yeah, I think that's important. Call it especially as I talk to customers like there are very few customers that are truly at web scale and most enterprises are working on billions or tens of billions or maybe hundreds of billions of larger scale where it's like small and medium sized data that now can fit into these larger form factors. And so it's really kind of the two things coming together and accelerating those different kind of customer profiles while still having the capability to go to Spark and other large scale distributed computing. But now I can pick and choose the compute that's right for my problem. I think that's a pretty cool framework to think through is now it's very interoperable. I'm not stuck in one place. Yeah, exactly. And I think it's very easy to remain spark focus given the history. So previously even with moderate data scales, you'd need to use Spark. And now we're in this whole new regime where, you know, much of the pipeline that folks do is in the smaller medium scale. And so it's not worth the spark overhead. Now that we have these, you know, new compute engines. So folks folks are looking a lot at these newer compute runtimes like Polar's pandas data fusion, which, you know, stand on the shoulders of the giant of Spark, but have really introduced a whole new paradigm for compute. And the framework there is single node blazingly fast, rust or native library based query engines. And so these are, you know, introducing these tremendous savings and speedups. As folks look to either write new pipelines in these compute engines or migrate their existing legacy spark pipelines to one of these newer methods. Yeah, that's very cool. Should we take a look at it? Yeah, let's do it. So let's walk through an example lightweight transform. We're going to take an existing spark transform, migrate it to lightweight and see if we can pull some resource savings out of there. So here we have a notional data set just for store sales and transactions. You see we have things like a transaction ID and a transaction timestamp. And it's pretty substantial. So we have almost a billion rows here. And it's almost 14 gigabytes compressed. So it's going to uncompressed to something even much larger than that. And what we're interested in today is, you know, looking through these transactions and see if seeing if any of them appear to be fraudulent. And so we have this existing spark pipeline set up that does just that. So we're going to load in all of this threats, these transactions. And we're going to look at various features that are indicators if it may be fraudulent. So we're going to look at, you know, how many transactions the customer has been doing recently, whether they have behavioral anomalies, we're going to look at network risk and some of the qualities of that transaction. And then we're also going to look at geographies to see if it's correlated with anything there. The interesting thing here from a pipeline standpoint is that we're processing this incrementally, which means as a new batch comes into the data set, we're only going to process that and not, you know, look at the whole previous data set. But from the data engineering perspective, we still need to look at the entire history of the transactions. Why? Well, we're going to take our new transactions in this chunk. And we're going to use the entire transaction history as a bit more data to see if these new transactions look fraudulent compared to what we've seen in the past. So you can see we do that with, you know, a relatively simple syntax. We just have this transforms object and we pull a spark data frame out and then we're off. What we're going to look at is see is to see if we can translate this to a lightweight transform, namely using pullers. So what is pullers? It's again, one of those, it's a new entrant on the field, which is a, be blazingly fast single node compute engine. So it's really come, come onto the show over the past couple of years and folks haven't been adopting it quite quickly. But I think the important point to call here is not just pullers, it could be anything that someone's choosing, right? So like we're choosing pullers today, but really you can bring any compute engine inside the platform and author that and actually run. I think that's pretty cool here as you go through just to think about this could be any engineer familiar with or specific to a use case. Yeah, exactly. And so we wanted to set up a very simple declarative API there where you can really just pull these into any set of common data frame objects that you care about. So you can see we're pulling it into a spark data frame, but there's nothing stopping us from doing pullers, pandas, ducty, or any of the top N Creanjans just natively out of the box. Cool. So first we're going to check out a branch on this repository and you know, see if we can make this work. So we're just going to call it lightweight. Looks like I've type. And we're going to head into the continue plug. So one thing I want to highlight here is that we've really focused our dev X on familiar open source technologies. So this is just VS code using the open source continue plug in as an AI authoring assistant. We want this to feel as familiar as possible when you're authoring in the platform. You know, so it's just like if it's on your laptop. And so here we have clawed 3.7 sonnet and we're just going to ask it to translate to lightweight. Yeah, I think it's pretty neat that we have the can to you know, basically bring VS code into the platform. So you have it in the secure enclave of Palantir have everything integrated with it that still have these tools as pretty neat. Exactly. And on that note about tools, I just want to highlight that what's actually going on under the hood is we're using an agent inside the the transform that you're offering. As a fully blown tool enabled assistant. So what we're doing under the hood is we've actually built an mcp server for your transform work and do things like discovering the transforms you've written. Editing files, you know, inspecting the data that you've imported so that your your agent has the maximal context that it can get to help you author that transform. And so we think off it's found this transform and it's going to do its work. This is going to take a second. So while it works, I just want to chat through to previous runs of this build one in dark, one in lightweight to sort of highlight those differences. So if you look here, here's the lightweight execution. We see it takes two minutes. And so, you know, what does that mean? We're going to look at the previous spark execution and see how long it took in the resource usage. So take a look at spark. We see it's at 220, which is roughly the same time. But if we look at some of our observability tooling, we're going to see that it took a much larger amount of resources to run. So if we head into our spark details. We see this because our spark pipeline ran with 16 executors, each with 16 gigabytes of memory, even though our pipeline only took two minutes to run, it's actually 21 full minutes of compute with hundreds of gigabytes. So this works. It's reasonably fast. But we can eke out much larger savings by moving to pullers in a single node environment. And so let's check out what we can see for the lightweight transform. Well, it's going to look similar. We have telemetry. And, you know, because we aren't using spark, we're going to supply more generic container metrics and observability for whatever you're running within this computer runtime. So we see it takes two minutes. And if we hop on over the metric tabs, we can look at CPT. And we can see that we can see that we have a small supply of 12 gigabytes, which we can see through the CPU and memory usage. So although we ran in a much, in a similar amount of time, we only use about 29 gigabytes of memory throughout this transform, which is, you know, almost 10 times smaller than what that spark pipeline was doing with executors. So we can see that we've seen executors with two cores per executor. So about a 4X reduction in CPU costs there. So we can see just by translating this small pipeline away from spark, we're incurring savings and a bit of a speed of. But yeah, that sort of begs the immediate question of, hey, I have this transform. It's a big lift to move off of pi spark. You know, is it worth it to save these costs? And so I just want to highlight here that, whereas that would have been a hugely costly operation a couple of years ago, with LLM assistance, you know, it really just works with, you know, with a wave of the hand. So we can see I think our LLM is done translating our pipeline. I'm going to click accept. And it's going to start applying its edits to the transform that we had in front of us. So you see it's going through. It's importing pullers there. And I'm just going to let this run as it makes its diff. So just highlighting a couple things. It is applying the lightweight decorator, which we could see. We'll let it run, but we'll take a look. Really all it's doing here is applying a decorator to your Python function that enables lightweight mode. You can figure the amount of resources, so memory and CPU you want to top of it. And then all we need to do is write this logic and pullers instead of spark. I think it's pretty neat though too, just to think about from like a translation standpoint of, you know, if you're the senior developer and you're like, hey, I need to go. This would be something you might hand off to a junior developer to go like handle this task, task, the fact that in the few minutes we're talking, it's gone and done this figured it out and actually making these changes. This is pretty powerful from man, I have hundreds of pipelines and I want to get the value of speed and cost reduction will now with the continue plug in MCP server behind all these different twins. It's literally just a simple question and go. Yeah, exactly. So previously we saw that data engineers who are authoring pipelines would take a while to upskill on certain libraries. So they'd be learning, pi spark or pullers syntax and they'd become the person who wrote those pipelines. With continue and new AI authoring tools, it sort of moves everyone a level up. And so what that means is you're really thinking about data engineering concepts. So do I want to filter here? Am I dealing with a big join rather than being bogged down in the syntax of the particular library that you're working with? And so we can see here it looks like it's done. As I mentioned previously, we've applied that lightweight decorator with the resource requests that we're looking for. And instead of reading in a spark data frame, we're just pulling in pullers data frames and these come in just a cell. So if we scroll through, it looks very similar to the previous pipeline, just with pullers calls and we can kick off a built down. So that's very cool. And the fact that you can do all this in a branch, you test it out, you know, having the foundry branching across everything is super powerful for stuff like this to and do a diff even on the output. Yeah, to make sure that the translation happened correctly. Exactly. So because of that, especially with the branching concept on data sets, what this allows you to do is check out a branch in the repository, try out a new change and then see what the denser data is as you're alluding to. This is a super huge feature for things that look like migrations. So it's, you know, you aren't just rolling production right over from spark to pullers. You can, you know, run two branches side by side for as long as you'd like, assure that the data is the same, and then move over at your leisure. I think you, you, you, you curse, you kind of went over quickly on the important point. I don't think people in the data engineering world normally think about you are branching the data as well in foundry. So you can actually do a diff on the actual data, not the schema, not the just the pipeline in the code, but the actual data. Yeah, into that note, Chad, especially with these newer, matureing libraries, we see new features bug fixes, documentation being released, you know, day by day, week by week. And so in the same way that you'd want to branch out for, you know, a full spark to lightweight translation, we have this same concept for simple things like version upgrades. So if you want to upgrade your pullers version or whatever you're using, it's super easy. We have a managed Python environment within this repository. It would be as simple as checking out a branch, upgrading pullers there, making sure everything looks okay and then merging straight into master. Yeah, that's pretty, a pretty slick process here. Yeah, awesome. So yeah, I just wanted to chat a bit more about what it means to be a lightweight transform sense. You know, we're doing some very generic computer, we're running a pipeline with pullers, but that's really only just scratching the surface of what we've enabled here in lightweight. So folks are doing things that are whole a whole lot more interesting than simple tabular transformations. And so we also wanted to encode in the flexibility into this product to do things that reflect more modern pipeline usage. So we have full full blown media integration where you can download images and unstructured data, transform them right alongside your tabular data, and then output it to wherever you'd like. I think some data science teams will probably love love that type of feature, right? Yeah, exactly. So we're previously you'd need all these different pipelines, you know, with all web of things flowing into one. But in one place, you can download images, say documents run OCR with, you know, whatever, tooling or LLM you'd like to use, output that into, you know, maybe some agent that you have written in a function that can transform it away like. And then really just output tabular data. So this is you know, encompassing pullers pandas or push down into data bricks or snowflakes should do like, but really opens just a whole new world of opportunities to do whatever you'd like in a Python runtime and then integrate it fully with the Foundry Data Study system. Yeah. And I think what you're describing I talk about is like I need to make more data computable, which be like the power of this is now I on long so much more context and knowledge across my business because I can compute across. Different forms and types, the multimodal component that's what we call the multimodal data plane, right? And so that is so powerful, but it's also super important because that context is generally very important for really the end goal of all of this is to get to enterprise autonomy. You know, AI agents automation, these was which means I need to compute these things that were stuck in PDFs and emails and phone calls and videos. Yeah. And so I do this tooling on locks. Everyone to move at the speed that the different tooling is because I have I can compute in the right way for the type of data. And it doesn't just have to be homogeneous. I can have heterogeneous compute environments to I think that's the other thing here is like, it doesn't just have to be pullers or it can be it can be spark pullers, ducty be in different parts of the pipeline, right? And that's I think super powerful. Yeah, exactly. And so there's a whole spectrum of places in foundry where you can have, you know, unapuninated to very opinionated compute. So if you think about something like compute modules, you know, that's all on one end of you bring everything you'd like to run and it's literally a docker container running within the platform. We've applied just enough guard rails here so that you have an easy declarative API to get access to data of whatever form or compute of whatever form within a Python environment, but then we pass the baton to you. So you're coming here with the Python environment, but you can really do whatever you like. And so lightweight transforms are one of the key places where we've seen compute push down to other providers like Databricks or snowflake or working with things like virtual tables. Because you can just specify whatever inputs you'd like into the lightweight transform here. So you see we just have a normal founder data set here, but these could be, you know, external connections to another S three bucket or a snowflake table or anything like that. And you can work with all of these different sources side by side in one place. And so, you know, the question you may ask then is, well, why in foundry? And the reason there is for free, you get all of these full fledged data set features out of the box. You're going to get security primitives. You're going to get the branching that you were talking about previously, Chad. You're going to get lineage. You can see where all of your data was coming from and where it's going. And so these are all these, you know, incredibly useful data engineering tools that you wouldn't get just running Python on your laptop or somewhere else. So, the best thing there is we can just infer that and give it to you for free just based off of what your inputs are and what your outputs are. Yeah, and I think in the enterprise space, you know, if you're working in the enterprise space amongst a team of people, you need to be in this environment where people can collaborate and work. I think the developer experience you're talking about is powerful of like we have a wide range of capabilities from if I want to be highly opinionated or not opinionated and giving you the ability to kind of choose where you are. But work in an environment that has that security, I mean, especially even when we talk about vibe coding like, you know, now you're already starting to see some of the stories gone wrong of that of having the security, you know, by default having the lineage and the branching across everything. Yeah. That really is what makes the difference in the developer experience and the friction to really at the end of the day get it to production. Exactly. Yeah. And, you know, LMS have been quite magical in terms of authoring tooling and translation tooling. But they do have the dark underside where things do go wrong. So LMS have deleted production databases. They've done all sorts of things that go awry when folks aren't using them in the right environment with the right context. And so a place where you want to look at AI assisted pipeline authoring is something that looks like transforms. So you're going to have security primitives where LMS aren't able to access data that they aren't given explicit permission to. You're going to have a standard get style change process where they aren't just coming away and editing pipelines with nobody knowing they're going to, you know, you're going to raise the PR and have it be reviewed and merged into a repository just like how a human author would do it. And so really in order to get all these benefits of an LM in a place like writing data pipelines, you need all these features. They're not optional. They're actually critical to make sure that your data is staying safe and correct when you're bringing LMS into the equation. Yeah. And I think really what you're getting at in my mind is this is system level thinking I'm not a bag of tools. I'm stitching together. You know, when I was a prior customer stitching all the tools together was great until you you had to own it and then update everything connected security and different security primitives, different scaling mechanisms. The system thinking here from a developer experience means I can just move at the speed that I'm thinking because all the tools are integrated and work together. And I think that level of speed to be able to execute is what people see. They're like, how does Palantir move so quickly? Yeah. Well, it's all these little things that we make work together in the system in that system level thinking is really where I see us accelerating every developer. Yeah, exactly. And one thing to your point about speed is that one of the biggest lags we've seen are the folks who have the legacy pipeline, where you know, the person who wrote it isn't there anymore and no one remembers the details. It's incredibly difficult to either migrate off those pipelines or upgrade them to bring them up to date. That whole paradigm has changed here as well as you think about managing systems instead of managing individual pipelines. With AI tooling, migrations, upgrades are becoming much easier. And with this transforms architecture where you're able to preview data before you commit it to production, it's the perfect ecosystem where you can experiment with upgrades, experiment to see, you know, maybe that's what I'm saying. It's just to see, you know, maybe that pullers version change, introduce, you know, some different behavior that you want to roll back where you discover these things before shipping it to prod. It's huge. And so when you have this sort of environment where you can play around safely and experiment and verify the outputs of everything that you're doing, it is also a huge accelerant to shipping these things that's production and keeping them up to date, which is going to accelerate folks who are working in your repo in the future. Yeah, that's awesome. The other piece I kind of want to hit on that you talked about was the LMs and you on the same backplane so an LM can, you know, work on a branch and it can put a PR for you to review. Like those same primitives that you as a developer are there for the LM to work with as well. Yeah, so, you know, we've seen a lot in the open source mainly this year with coding agents really coming into play for local development. And so one common feedback or iteration loop we see with LMs with local development is they'll do something like making a change and then running the compiler. And so if they've done something wrong, they're going to see a compiler error. And in many cases, the LMs going to be smart enough to go in, fix the error that it made and verify that that was fixed. And that works great for code. But we want to have the exact same sort of iteration loop here with data to sense, you know, often a transformer pipeline will work correctly, you know, output data, but maybe it's not what you wanted. And so in the same way that a human is going to do this, we want to enable our LMs and we do to write part of a pipeline and then preview the results. So you can imagine an LM, you know, looking at the schema or the row counter statistics of a data set that it's actively building and iterating on. And that allows these LMs to go much further on their own in an agentic capacity than they would from just, you know, one shot writing a pipeline and hoping it works. And so the nice thing there is, you know, it really is the exact same way a human is going to do it. They're going to be enabled by tools within a closed-off secure environment to build these pipelines and iteratively rolled them out to prod. And so as these LMs get better and better over time, we expect that they'll just be more and more adept at using the human level tools already available within the repository, further on good. And so humans can be, you know, further and further hands off accelerated and again working at the systems level. Yeah. I think this all goes under the ethos of the AIFDE is how do we delete FDE minutes from these tasks and having an LM as your sidekick to be able to do this. This is pretty cool. Yeah, exactly. And so it really extends through the whole life cycle of a pipeline. So if you imagine the non-AIFDE world of the past, you know, you'd write a pipeline and maybe it would be right, but then you have to go into a pipeline optimization. You're digging deep into, say, Spark logs. You have to learn what all of that means. You're tuning the number of executors and resources available to your pipeline. And it could be quite labor intensive, even for an existing correct pipeline, if just the data grows or something like that. And so we want to enable the next step here, which is automating that a bit more. And so one of the key places there with lightweight is making sure we're using modern resource efficient libraries that an LM knows how to work with. So you can imagine in the future, something like visiting an existing pipeline that maybe has run out of memory, memory you've gotten an OOM error. And having an LM just go in, you know, bump the resource request or something like that. Or maybe, you know, getting LinkedIn saying, hey, this operation you're doing is a bit inefficient for whatever transform you're trying to do. And just really accelerating those the human instincts and insights into pipeline authoring with this agent that of course knows, you know, every doc of every compute engine has seen millions of pipelines that maybe, you know, no human has ever been able to read in terms of that scale. And then just bringing all that context to a domain expert who may be writing a single pipeline, the single environment. So I'm super excited to see where this ecosystem evolves to in the future. Yeah, very cool. I just want to talk through, you know, what the output of this pipeline is. So we can see we have a very similar set of transactions with their IDs and dates. But we've gone through and have made an inference about the fraud risk of a number of them. So you see we have this new column here called fraud risk level. And fortunately, these all look low, which is good to see. But really, it's just been as quick as that. We've fully updated a billion row data set with one call to an LM and I think one commit. So as we're looking at this output data set, we see it's actually grown since we've added more columns to it. It's now almost 23 gigabytes compressed. And so this sort of gets the question of what is too big for a single node transformer? And when do you need something like Spark? And one thing that I've been super excited by to see out in the open is a bunch of these new libraries develop what they're calling streaming mode. So some of the leaders there are ducty B or pullers. And basically what that allows transforms to do is operate on data sets that are drastically larger than your memory. And the way they do that is by not loading the whole data set in at the same time. So you can just work on it chunk by chunk. Now, not all transforms can be streamed because sometimes you do need the whole data set in memory. But if you think about something simple like counting the number of rows or getting a simple aggregation like an average over a column, you don't need to load in gigabytes or hundreds of gigabytes or terabytes even all at the same time. And so that allows you to work with very small resource efficient single mode setups that are just scanning through your data sets over a period of time. And so what that allows us to do is lean much more heavily in the lightweight setup than just falling back to Spark because the data set is large. So given the new parameters that we're working with in the single node mode, I would say almost every transform that I see in my day today could be run with lightweight with a single node. Now you may need a lot of memory in the node. You might want to add a lot of compute to get it to run within a set period of time. But it really is the exception that you need a large distributed Spark set up these days that you're talking hundreds of terabytes or incredibly expensive transforms on data where you do need to hold it all in memory. And so that is just a key benefit of these new libraries that you know, unlike Spark are not Java based. They're super efficient with memory management on the system. And they're going to allow you just eke out a little more performance for a given set of cores and memory than you would in, you know, in the Spark land. And so that natural performance cost it's a big cost reduction. Exactly. And so we've even seen folks leveraging streaming style compute for all sorts of creative ways. So you can think about maybe ingress. So one thing that you can do out of a transform is connect to an external system. And so folks may want to just read straight from some system and immediately start transforming it in Python without any of the extra steps. So if you think about something like a streamed library in deck TV, you could you know read from an external s3 bucket, write some sequel against it, write some pullers against it, and then just be streaming it in from your source and streaming it straight back out right into a foundry data set. And that would be, you know, your transform right there. So we're super excited to see where that goes. And as new engines open up this feature, it's just incredibly cool to see what you can do with a very small compute profile. Yeah. And I think that's the big transformation is, you know, if you look step back and look at like what is the goal? It's, you know, everyone's like, I need a data like I need all of my data is one huge thing with distributed compute. Yeah. You know, actually the goal is, how do I, how do I get this data to be used in driving decisions and actions and honestly make it available for AI workflows, more humans or collaborating. But it's, that is really the end goal not to put my data in one place that it's in this big distributed thing. And I think I'm seeing more and more of that in in the customers I talk to is getting the idea here is now I can do things like this at a low cost threshold that gives me access to data. I never could process before. And now I've made more data computables that comes back to my same theme here is I'm making more data computable across my enterprise to drive better context for those higher order workflows. Really, that's the end goal. Exactly. And so when you have these like lighter for factors with an overall, you know, smaller footprint and shorter run times, it's just going to bring you that extra step and keeping your data up to date, keeping it correct, keeping your pipelines modern to really drive those those decisions there. I appreciate you taking the time out of your busy schedule and building all this cool stuff to come talk about it. Yeah, it's been great. Thank you very much.