================================================================================
METADATA
================================================================================
Title: Craig Savidge, Lockheed Martin | Software for Government 2023
URL: https://www.youtube.com/watch?v=_nvTXrIkbws
Published: 2023-10-02
View Count: 3,176
Duration: 902 seconds
Score: 55.0/100
Rank: B (Basic - 基础背景资料)
Transcript: Available
Category: 
Source: whisper

================================================================================
TRANSCRIPT
================================================================================

Good afternoon, as Ali mentioned, my name is Craig Savage and I'm here with the privilege to represent a multi company team including ASRC Federal, In-depth, Amazon Web Services and of course, Palantir and a project we did to put together some of the pieces we have been working on to enable delivering software faster and with fewer defects to our customer. So the heart of this is our work that we do in Morstown on the EGIS Combat System. We have been the developer for the EGIS Weapon System, key piece of the EGIS Combat System for many years and the key part here is that you just combat system if you're not familiar with it is the premier combat system for the Surface Navy. It links sensors and command and control and shooters together to provide multi-domain defense including air, surface and ballistic threats among others. The key piece is the integration of the software. We have almost 54 years legacy of the software and this capability. That means we also have some very monolithic old software and what we have been doing is working through the process of re-architecting the software and working through some of our challenges which include the delivery timelines and our ability to build and outpace the threat. We talked to the morning speakers talked about the need to move data and the ability to pace the threat and the ability to make updates out to users. So what we're going to show today is what we nicknamed our day in the life demo and we're going to show Julie promised you earlier we were going to get down on the weeds and we weren't kidding. We're going to build and deploy some software in about the next 10 minutes. So what we have is a cloud-based instance that runs our CICB pipeline, our continuous integration delivery pipeline which is going to build, test and deploy software. So if we can, as we switch over to the demo view, I'm going to introduce my colleague Eric Cutter. Eric works with me up in Morstown and is one of the lead designers and developers of the component or generic track provider component that we're using for today's demonstration. So Eric, if you can bring up our first tax set display, what we're looking at is data from Eric's laptop. Everything that he's showing is on the I-05 cloud where we're running the entire demonstration. So we have right now is a system up and running. We have some tracks in the system and we have gotten an urgent request from the user that these yellow tracks need to be displayed as blue. Now, we're going to get that call from the fleet, probably not. But the process, in an unclass demonstration environment that I'm going to walk you through is the same for whether that would be an AI algorithm update, an ML update, a cyber fix. The process is analogous. The version that we're using today is we're changing the aircraft color. So what we would do is then make the software update and push it through our system. So I'm going to pass it back over to Eric to kick off the commit and start the process. Good afternoon, thanks, Craig. So as you mentioned, our software team got the requirement update to change the color of the planes from yellow to blue. So we modified the necessary code. We did all of our internal local testing. We held all of the necessary peer reviews. Now that this change has been peer reviewed and has been approved, we're going to go ahead and commit the change to our master software branch. So I'll go ahead and do that, which is the only manual action that you guys are going to see today. So the key here is Eric is kicked off a build in one of our small components. What we will see is Eric brings up is the display of good lab doing the build. And don't worry if you can't see all the details here. The key piece is we're using this software. We chose this one because it's part of the Lockheed Martin software factory. There's lots of build products out there. We're using this one. And what you're looking for is this process as it continues will monitor the build processes. It checks off the various steps of building the software, creating the container, doing what we call level one and two tests, which is static code checks and basic component checkouts to make sure that the code is built correctly, will then build the necessary documentation, home charts and things necessary to deploy the software. This usually takes a couple of minutes, depending on how good lab is running today. And now, obviously, if we were going to take one of our legacy large components that makes up the EGIS Weapon system, a build like this would take a few hours because we would have to rebuild an entire large component even if we're only making one simple change. What would normally take a couple hours? We'll take a couple minutes. We're walking through it already. As I mentioned, this is on the on an I-05 cloud. So that means that whether it's Eric kicking the build off in Morstown or one of our developers in Columbia, Maryland or another one in another part of Morstown, we're sharing all the same data with the designers and the testers. The cloud access really means that we're not passing anything around. Everything's available to the team. If at any point one of the steps would fail, we'd get a red X instead of a green check mark, the build would stop. The developers would be able to go back and figure out what went wrong. We do not want to be promoting bad builds. We don't want to be sending bad software down the pipeline and ultimately out to the users. So the key part here is that it's the software, the build, the documentation. Along with this, the developers, when they developed the component, identified the number of system level integration tests that needed to be done to have a successful regression and make sure we didn't break anything by putting the fix in or by upgrading the component. So what we'll see next is that we'll use that information which gets published as part of this build to then go off and kick off a system test pipeline which runs also in parallel up in the cloud to go out and check out the component and make sure it's running properly. As we can see, we have a set of green check marks here which means that the build completed and now we'll switch over to our test view. What we're seeing here is a view of visualization of our test system. In up and more time we build the OASIS simulator, OASIS is part of the Navy's CS3 or combat system simulation component. That's what the Navy uses to test their software at a system level. We've built a cloud enabled version of this that is able to run and what you're seeing on the screen, the circle is ownership, the square, the half squares are aircraft. The system recognized that a build had been pushed out of GitLab and it's kicked off a build test process. There are 33 different tests that go along with this component to verify functionality. You may see some of those tracks jump around if you're used to and need to display and you see the tracks jump around that's bad. It's good in this case because you're seeing the different tests starting and stopping. If we were to do this same thing in a legacy build structure, what we would have to do is take that build that was just made. We'd have to go get some test time which usually meant bumping somebody off of a terrestrial lab somewhere. We had a team out, spin up the whole system, deploy the whole system and run the tests, collect the data, analyze the data, publish the results. This test is actually running what we call headless. The display is only here so you have something to look at and I have something to talk about. The system itself is able to run it, collect the data, the expected results, compare all the results and then publish those. On a good day, we could get a team out and a system up and running in a couple hours, probably take a few hours to run the tests and collect the data. Most of the time that we do this, it takes between a minute and a half and two minutes to run to spin up the system, run the tests and collect the results. Part of that is because we've got the system up and running on the cloud, we've got the dynamic, allocation we're able to spin up the test environment that we need. The other piece is because our system test pipeline is queued up and ready and is able to run those pieces. So we'll know that the test is complete as the screen cleans itself up, you'll see the tracks drop out. The key piece here is being able to run system level tests and regression and make sure, again, that this is a good build that we're pushing out. The next piece that we'll look at is then how do you keep track of all of these things that are happening? So we built some dashboards to monitor the system and monitor progress. This is an example of our dashboard. The really key piece are the vertical bars in the middle. They track the status of the latest builds. Eric just noted that we updated, you saw the charts move over one. Basically states that as it refreshes, it better come up all green. As it refreshes, we'll see that the build completed the test checked out. And then that again would be where if there was any issue, if we got a red mark or a failed test, the developers would be able to share the dashboard data and go in and see what tests failed and where, what the problem was. The build would stop, we go back and fix it, recommit the fix. Again, this data is cloud based, so our developers in Maryland, our analysts in Morstown, our customers as they come online and get into this, can see it as well. So Eric, can you give me a status to the build at least complete? Does it test? Yeah, everything completed, the results updated, I'm not sure why it's not displaying over here. Okay, well then in the interest of moving on, assume there's a giant green set of, there we go. See, look at all that green lines, so as you can see, the last build completed, it says we had 100% pass rate. And we're ready to move on. So the next step is our system test pipeline then wraps that up, notes that the test had passed and makes that update. So the next piece is where we come into our integration with the Apollo tool that's made here a pound here. And what Apollo does is manage the deployment of the software to the assets. So we're just sitting there looking for those updates. Apollo recognizes that an update has been made to a component. It looks at who are the users of that component and how they set up to receive updates. In this case, the environment we had run them before was set up for automatic updates. So it goes out, it tears down the old component, installs the new one, spins it back up, and the system continues. You can configure the system for pull updates and effect like, hey, I'm busy right now, now's not the time. It's also built for disconnected systems. So if your system's not online for a while for some reason, and then comes back online, it'll go through an update to the latest version. Also features rollback. So if there is any issue with the build, which thanks to our rigorous test pipeline, we don't expect. But if there is any issues with the build, we're also able to rollback to last known good version. So this enables us to manage that. Again, if we're going through, this is one simple example of one component, one test. As we move into scale, and lots of components are getting updated by lots of developers and tests are being run in parallel, and versions are being pushed, this level of automation and control is critical to be able to manage the workflow and not get bogged down to any one piece. So Eric, if you can switch us back to the tactical display we had run originally. You can see that our yellow tracks are now blue. The important part here is we did not have to tear down the whole system. We did not have to reinstall software from bare metal. By comparison, the checkout validation of that data would take a couple hours minimum. The ability to bundle up a load with our legacy systems with the the old way of doing business. The ability to bundle up a load, wrap that load up and deliver it to a facility could take anywhere from a couple of hours if we were going to the combat system engineering development site or C-Sets, which is right down the road from our facility. We have great relationship with the Navy there and our ability to push software is pretty good to get to any of the other labs like wallops or hard-earned or some of the other labs would require at least a couple of days to make a formal delivery could take weeks or if we're doing it inside a contract cycle could take months. Not just to build and check out the software but also all of the bookkeeping and recordkeeping that goes with that delivery process and the culture that the speakers mentioned this morning. So what you saw today was basically a push of a build we started with a software build went through a system test and metrics collection process and then on to delivery through a cloud. Now as we move forward in the future we really believe this is a game changer for how we deliver software and deliver capability to the Navy. The key is that it's more and more sites become available to a structure like this and I'm not saying the only way to do clouds is with these three products but this is our version of how we're envisioning this moving forward. The two key things for us are getting more sites getting that connectivity so now instead of pushing a project around four companies that are sharing some investment and doing development we can get to the labs, we can get to the waterfront, we can ultimately get to the assets. Whether it's a digital twin or a tactical deployment so as we push the future state and get into an I.O. 6 classified environment we really see the ability of this both from a connectivity perspective and information sharing perspective and a and just a dynamic availability of assets perspective to be critical. So summary you've seen what could normally take anywhere from a few weeks or months to deliver run through a couple of minutes for an example really with a focus on managing this process meeting that time to delivery while making sure there's no bad builds and reducing automation increasing automation reducing error bringing the tool sets together sharing all that data all that data that was available whether it's build status or test status or whatever would be available to any stakeholder. And ultimately opening up to an environment where we can collaborate and share the data across the enterprise so we feel there's a lot of power in this as we move into the classified environment and are looking forward to working with our partners and our customers to do even more of this in the future. So I've gone about a minute over I apologize for that but thank you very much for the time and I think that's it for me. Thank you.