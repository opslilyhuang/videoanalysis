================================================================================
METADATA
================================================================================
Title: Chad & Arnav | Privacy & Security with Palantir AIP
URL: https://www.youtube.com/watch?v=pGBkkZFqLn4
Published: 2025-07-21
View Count: 4,485
Duration: 1097 seconds
Score: 76.0/100
Rank: A (Active - 高参考价值)
Transcript: Available
Category: 
Source: youtube

================================================================================
TRANSCRIPT
================================================================================

Hi, I'm Chad Walkwist. I'm an architect at Palantir. You've been hearing a lot about our privacy and civil-lobedy stance in the news and all around on the internet. So today we were going to take a second to actually dive into this a little bit deeper. So I brought a good friend of mine, Arnav, from our privacy and civil-lobedy's team. Thanks for joining me. Thanks, Chad, for having me. So you're a software engineer in the privacy and civil-lobedy's group. You're embedded in building products across all the different platforms. But what does that mean to build privacy and civil-lobedy's products? Yeah, it's a good question. And it can take many different forms. So you can imagine what are the tools for advanced data protection workflows? How are we supporting purpose justification or sensitive data discovery or a data obfuscation? That's one set of tools. In the kind of gendered AI space, we're thinking a lot about responsible AI. What does it mean to do AI safely and effectively? Everything from testing evaluation to fairness and bias, to thinking about hallucinations. A lot of these very topical concerns about AI safety and AI governance. And our work even goes into the defense space. We think a lot about international humanitarian law and the law of armed conflict, really bridging both legal and ethical disciplines with engineering and practice. Very cool. I guess the best way I think about security is like walking backwards from me as an end user and just kind of understanding the providence and lineage to understand, can I trust the data I'm looking at to make this decision? So maybe we can go through a demo today that actually shows us from me as an end user, all the way stepping back to where did this data come from? How is it protected? How do we secure it? How do we govern it to make sure and reclassify it when needed? What do developers have access to? All these questions and things that I think would be good just to walk us all the way back from end user to the sources? Let's do it. So I pulled up a demo here that will let us walk through some of the core access control of security and privacy primitives within the platform. And it's really centered around Onyx Inc. Our favorite, notional manufacturing company that we like a lot of our demos based on. And we're looking here at an internal directory application, not the fanciest application, not the flashiest application, but something that we can actually work backwards from to really understand how every single component got put together. This is a very simple workshop application because there's only one ontology object type. There's just one employee, very basic, but there's a lot going on behind the scenes for that employee. So I'm going to open up DataLineage, which is yet another application that we can use to actually peel back and understand what's happening behind the scenes, behind this workshop application. So on the far right, we see that workshop application, the Onyx internal directory. There's one input to it. It's one object type. That's our employee object type that we're using for Onyx's HR workflows. And behind that object type, we see that there's several datasets. And if we turn flows on, that's an easy way of understanding that this is a DataLineage. We start all the way on the left with the raw data, we're transforming it, cleaning it, producing these pink, clean data sets, and eventually integrating that into the ontology. So I think a couple of things to call out here, even with just DataLineage, that this is produced by default. There's no real way of deriving downstream data without maintaining provenance. It's something that just happens by virtue of working within the platform. You don't need to really think about it anytime you take upstream data and then create something downstream. We're going to maintain the DataLineage or the provenance behind that transformation. And this is a pretty simple pipeline to raw data sets, some intermediate transformations, and then into the ontology. But you can imagine the scaleless to thousands or hundreds of thousands of datasets and transformations backing a full blood ontology. Yeah, and I think that is important because it's like when you're looking at data, you're wanting to like, how do you get here and what happened to it, right? That the provenance and lineage in order to trust it. Exactly, no, that's totally right. So in this example, we see that this employee object only has a single upstream backing data set. And that's what I see here in DataLineage. That's my source of truth. That's how I can understand the data that I can see behind this object type. But we recognize that organizations might have lots of different data sources that they need to bring together to get a full picture of this employee object for an HR workflow. In this case, I have some employee data that backs the internal directory, but I might also have finance data and payroll data and more sensitive data. So we can use the platform to bring that data together. We can also do it without compromising on security or privacy. We can have both. We can bring the data together, unify it in one platform to build applications over it, build analytics over it, monitoring over it, but also maintain those access controls or even more granular access controls with the more advanced tooling in the platform. And to really show that here, I only have access to one of those upstream data sets. But this employee object type is the object type that is backed by several other different kinds of more sensitive data. And this is just what AIP is showing me. It's showing me the access controls at play where I can only see what I'm authorized to see. And I can't even see or can't even discover any of the other data that is actually also backing the same object type. Very cool. So this comes back to both from a user perspective of I'm going to consume these applications, but also from a developer perspective as I have to build data products that the developers have access a lot of times to very sensitive data. So we still have controls both over in design time and in runtime of the security access controls. Exactly. It's both for the developers and for the end users and really every role in between. And we can get into some of those more granular access controls by actually revealing what's behind this data. So I think the easiest way to do this is if you want to add me into the group that actually is securing all of the other more sensitive data and that's backing this employee object type. All right. I just added you. Let's see. Okay. So let me just refresh now that you've added me to the group. And we'll also talk about what that group was and why it was important. But we can see here once I refreshed that there are two other backing data sets that are now viewable to me in my role that are backing this object type. And let's go ahead and actually expand all of this out and we're going to go through it in turn. All right. I'll go everything and then we can turn turn our flows back on to kind of understand what's happening behind the scenes. So originally we only had this employee data and that's the only thing that was visible to me. But in addition, we have these two other new backing data sets with some sort of address data as well as payroll data that is backing this employee object. And the easiest way to understand the access controls is to actually use data lineage to understand the permissions as well. So the data lineage is really helpful for understanding not only the provenance of the data sources that are backing the object types, but also a lot of other qualities of the data that's necessary for data governance. This could be staleness whether builds are running the original upstream sources or in this case security. So here I'm looking at the access data access in data sets for myself, but I'm actually going to switch to another user that is not in the group. And in this case, we can see the differentiated access controls in play. So this kind of demo user that I'm using here had the same access controls that I had originally before you added me into the group. And we can see that because the blue boxes here are the data sets and transform data that I originally had access to, just as employee data. And we have all these red data sets which I could not previously see. And that's where the access controls are actually working and are actually at play. The first thing we want to talk about I think is markings. We can see that this payroll data set on the bottom left is the upstream most source of payroll information. This could be an upstream data source. It could be a one off ingest of data. This is where the payroll information came from. And we can see here it's secured by two different security markings and HR data marking and a payroll data marking. Markings are one of the most basic forms of access controls in the platform. They're a type of binary access control. Either you have access to the data that's marked or you don't have access to the data that's marked based on whether you're added to that marking or you've access to that marking. So when you just added me to the ONIX US HR group, you gave me access to the two HR markings that are securing some of the more sensitive data that is backing this ONIX employee object type. How is the marking different than just like access control group? It is it's a really good question. So unlike a kind of static access control group where you're going to individually access control every single data asset, this is actually leveraging your data provenance and the data lineage that is getting built over time to secure it by default. If upstream data has a marking, any downstream derived data will inherit that marking by default unless it is specifically declassified by an authorized user to unmark that data. Perhaps it's been aggregated, the sensitive columns have been dropped. Those are good reasons you might want to actually remove the marking, but this is an example of data protection of security by default. Yeah, and I think this is this is one of the things that I always loved was that I'm securing the data not just the object. That's good. And then I have an audit trail of the governance about who is reclassifying it. That's totally correct. And there's so many different ways that you can individually govern those kinds of security changes. You obviously have the retrospective audit, but from audit logs were from a tool like checkpoints. You also have pull requests that you can do security reviews. You can implement your four eyes principle to say that a developer needs to pull in someone else who has specific declassification authority to remove the markings. There's so many different ways to actually manage and secure your granular level. But if you don't want to think about it, you don't need to really think about it because it's going to be secure by default. In this case, bringing the marking down the pipeline to any derived data all the way in viatology. Yeah, I think that's a huge differentiator about the Palantir platform across all the tools. I think that's the other thing here is like this isn't just a bolt on tool. This is literally the heart of everything we do. Absolutely. Absolutely. The second thing that's really interesting to talk about are restricted views, which implement role-level access controls. So let's imagine at Onix, we have an organizational policy that only US HR personnel can see the salary or payroll information of US based employees. But Onix is an international company, their international employees, and necessarily international HR personnel will be the ones who can look at their salary data. So how do we implement this kind of split within the system? So we can use a restricted view to actually implement those kinds of role-level access controls, really splitting access to data in a data set at the role level based on some kind of condition. So if we open this restricted view here, we can see a quick preview of the data set without ever living data lineage. There's a lot of employee IDs, room salaries, and a payroll group. And this group is that same US HR group that you just added me to at the start of this demo to let me see a subset of the data. And I can scroll through here and you'll get a quick preview, but you can see these are all going to be the same group. There's another part of this data set that I don't have authorization to view that has all of the data for international employees that have an international payroll group here. And I've implemented a restricted view with a group condition, which means that I will only see the row if I'm a member of this group, and I will not see the row if I'm not a member of this group. And this kind of reflects any dynamic changes to the group as well. So if you add me into the group, I'll have access to the rows. If you take me out of the group, I will lose access to the rows. And that way you can also do time-based or temporary access to any of this data as well, even at a row level. So now you really get to a cellular level because now I can secure the properties, the objects, the properties, the rows, and I can really secure specific actual elements of a data set. That's totally correct, but there's a third security and data protection concept here that's worth going through because it's yet another layer on top of the more basic access control fundamentals. And that's Cypher. So Cypher is a tool in the Palanture platform for a data obfuscation or data minimization. And this really is motivated from a need that you might not be able to use access controls to minimize data completely because you might legitimately need it for some workflow, but you might not need it all the time. So with a marking or a restricted view, you're taking away data that you're not authorized to see. You're either removing a data set from your view or you're removing a subset of rows. But what if you do need that particular cell of information for a legitimate operation downstream? You just don't need it in bulk or you don't need it at the outset. And that's where Cypher is really helpful. With Cypher, you can obfuscate data with an encryption algorithm and then bring it back with decryption down the line. So if we look at the preview here, we can see that the addresses are kind of in this scrambled format that we call Cypher Text. The Cypher Text is based on an open source encryption algorithm so you can perform the encryption to obfuscate it and then later down the line when you do need the value for some specific workflow, you can get that single value back in a rate-limited audited purpose-limited kind of way. Yeah, okay. That's that's a lot. That's a lot. It's a lot. But let's see how it all works in practice. And then yeah, now we can see how each of these pieces is contributing to the total access control story. So let's head back to our internal directory application and I'm going to look for James Wilson, who is the head of our international sales org here at ONIX. And we can see here off the bat that there's this new HR information box on the screen. And this whole section of this workshop application is based on that more sensitive data that I didn't have access to at the outset of the demo because all of this data is marked. And we see a couple of different things going on here. So first the address is obfuscated, this is Cypher. So I can click on this Cypher Text property and it's important to note that the ontology is also Cypher Text to wear so it'll render it in this way that let's me click on it and then also go through the checkpoints workflow if there's a checkpoint configured. So in this case, there is a specific purpose justification prompt just for this kind of decryption of address data for this one ONIX workflow. I can go ahead and confirm that checkpoint will actually get saved to a log that is later reviewable in real time for kind of oversight of sensitive actions in the platform. And then here in my workshop application, I see the address. So when I do need that address field, it's still there. It's on access control away, but it is minimized for privacy by default. And then at the bottom here, we see payroll information. And you'll recall that I was added to the US HR data viewer's group. So I do have access to James Wilson's information because he's based here in the US. I can see his salary. But you'll recall that I'm not in the international group. The restrictive view that was in place is only giving the access to just the rows that correspond to US based employees and not the international ONIX based employees. So if we go to one of James's direct reports, perhaps we can go to Melissa who heads up APAC sales. Melissa's based in Singapore. Again, address is obfuscated. But importantly, the salary is restricted. And this is not just a front end display of the word restricted. I'm actually not authorized to see this specific property of this specific object because of the restrictive view in place that prevents me from reading any of the data about international employee salaries. So here we can see all three of those permittives of place. We can see the markings that are giving me access to this section, cypher for obfuscation by default. And a restrictive view enforcing effectively property level access controls here in the anthology. That's a pretty powerful set of auditability in trails and governance across just everything. I mean, this is embedded into the ontology, which then is everything is built upon. I can't like stress that enough is that means everything you build upon here now has the same type of controls across everything you're doing. All of those components come together into a true system where you can have privacy and civil liberties and you can do the effective work and be effective by not and not giving away or losing anything on both sides. You can have both. You can have both. I think that's really the thrust of the reason access controls are so core to the platform. It's that you can have your operational workflow. You can make your organization more effective without compromising on security and privacy through the use of technologies like markings, cypher, checkpoints, access controls. Okay, so the last thing this brings to mind is AI and agentic workflows. And the idea that this is embedded in the anthology now means my agents also have to abide by these same principles as humans in the same markings and same security and access controls. Maybe you can just give a little bit on like how that fits into this picture around the access control piece. It's a really good question. So it's true that any kind of agentic workflow, any generative AI-based workflow will also have to reckon with the same access control infrastructure. So marking, restrictive views, cypher, a lot of those controls are still there. It prevents what agents have access to, what operations they can take. And there's a whole host of other features, tool scopes and logic, for example, as yet another access control primitive that's specifically tailored for the way that generative AI is being used in organizations. Yeah, because I think that's the things like, okay, we put these accidents for over there for people, but actually those same things apply to generative AI agents. Absolutely. Things working across here. Yeah, and it's a position that we held for quite a long time that our early investments in data protection, in privacy and access controls are what's letting our software accelerate in this kind of genai age that we're in, that all of the generative AI workflows that organizations need to do sit on top of a foundation of data protection and data governance. So by investing early in data protection data governance, thinking about it as a first class concept and not something that you tack on at the end, you're actually better positioned to tackle all of these responsible AI challenges from bias to evaluation to some of the same security challenges that we've been facing for the last 20 years. Yeah, not only that, but I think that's also what makes it effective, right? And at the end of the day, you know, all of these things, I mean, I can actually deploy real workflows into human environments with, you know, like real production grade, backing and evaluation software, you know, all of this is there. And I think that's what's so powerful about it. It's it's not just about how do I minimize things? It's how do I minimize things and effectively use it in productions. We're trying to change this to be no, you can do both, but you need to have the tooling to do it. Exactly. Yeah, awesome. Well, thank you, Arnav, for taking the time. I hope, you know, we can start to continue to educate people on what this really looks like, answer the hard questions. How does this effectively actually change the way that we behave, the way we develop, the way we go to production?