================================================================================
METADATA
================================================================================
Title: Palantir AIP Evals | Feedback to Fix
URL: https://www.youtube.com/watch?v=hXSx9uG0R4k
Published: 2025-07-16
View Count: 2,473
Duration: 383 seconds
Score: 76.0/100
Rank: A (Active - 高参考价值)
Transcript: Available
Category: 
Source: youtube

================================================================================
TRANSCRIPT
================================================================================

You'll notice next to this final agent comment, we can give it a thumbs up thumbs down. Oh, rating. That's maybe the entry point into how we're going to build up eVals for this agent. First, we're prompting the user to very explicitly give some feedback. That's one feedback collection paradigm. If I click, in this case, obviously it's getting it wrong as we mentioned. So I'll click thumbs down. That's this course structuring of the feedback right, we're saying either good or bad. So I think going a layer further, if we look at this style, they just opened. We can see we have the opportunity to provide feedback labels. So this is again a different modality to Chad's point about how are we allowing users to express their like case in the outputs. So if I click on this, we see this drop down menu where I have a few different options. These are ontology defined labels that let me as an operational user kind of express maybe a little bit with a little bit more detail what does negative mean, right? So in this case, I might click did not consult principles. But you could imagine also maybe I didn't even, maybe that label didn't even exist, right? Maybe there wasn't anything sufficient. So in that case, we need this escape hatch where users can give unstructured feedback. Again, a different modality for a user to express their preferences and given evaluation of this output. And they might have said should have consulted principles and found OF, but one, two, right? And so we're giving this user a few different opportunities to really capture what should have went right, what went wrong in particular, and what is their sort of taste in this output? Like what do they care about? And in this case, obviously it's this notion of, it should have completed this one step that we defined or it got this output wrong in some way. So I'll go ahead and submit that. So now that I've kind of gone through that process, I can think about what do I want to do with this feedback? Like I've understood maybe what the user is talking about. I've categorized it, but like how does that make my AI system better at the end of the day? In this case, I want to talk about eVals in particular. So what I can do now is I can look for these maybe different edge cases that users are finding or things that are just good tests. And I can click on one of these piece of feedback and add as a test case, just with a click of a button. What that does is it, again, mutate the ontology, it flags it as a test case in the back end. And now if I open AIP eVals through this little popup, we can see those pieces of user feedback are being dynamically pulled into this evaluation suite. So cool thing about eVals is like, it's just a few clicks of a button to kind of get this evaluation suite to run. I've already mapped some of my inputs. I've picked the model that I want to use as well as the prompt I'm using. And so if I just run this evaluation suite, what eVals will do is it'll take this original function, all of my test cases, execute, you know, all of these different test cases, compute these metrics I've defined for accuracy, using this LLM as a judge. And then it gives me a variety of ways to slice and dice the results of my evaluation suite. But you can imagine, you know, some of our deployments have a ton of users. Ton of users might be flagging stuff. We might have a ton of test cases, building up over time, not just six, right? We might have dozens or hundreds, or in some cases we have like thousands of test cases for certain agents. So when you think about how do I actually take, they're not just like, you know, feedback as a test case, but how do I then kind of try to keep closing the loop and get back to like a fix? How do I go from feedback to fix as soon as possible? It's not necessarily going to scale to check every single failing test case in debug the agents execution log and then come up with like a prompt improvement or a change in my ontology context or something. So the cool thing about eVals is it also lets you write all of your evaluation suite results to a dataset in Foundry that again, you can build on top of. So yeah, under the hood, there's a pipeline kind of building into this view, but this is a different sort of view for this failing eVals case, where we have our failing eVals here on the left, showing you all of those results. But on top of that, I'm running another agent, based in AIP logic, where it's doing a failure analysis for me, it's checking that trace, it's looking at my metrics, and then it's providing maybe a suggestion for, what went wrong, how do we fix it? Here's what you should do. So you can see that here sort of in this middle column. And then ultimately the output of this is sort of a prompt proposal, right? And an idea for what can we do to make the agent not fail in this case next time? And so we are talking about that one case that we saw where it wasn't consulting the principles. We see obviously we're seeing that a failure analysis come up, right? So our failing eVals are actually measuring this piece of user feedback, which is good. But on top of that, it's saying, okay, let's come up with a few ideas that improve on this original prompt, right? And it's expressing what a fix to the prompt might look like in a few different ways. So one is like, oh, maybe we should like prioritize the principles first in how we're doing this inventory resolution. So you can kind of see that here. Here's like a hacky diff here, I threw together, where you can kind of see the changes of the prompt. But we also have some other ideas like, oh, just like let's emphasize the principle citation or like, let's make it a more structured checklist than an LLM has to follow. Yeah. Where it's going through all of these different solutions. So they're whole lines, yeah, this is pretty cool. It's like, okay, so we gave it feedback. We had the evaluations. These were failing evaluations that were created. Now I'm using another agent. So I actually analyze all the failures to come up with potential root causes and then also potential fixes for this all. So you have agents checking agents. Okay, this is very cool. Yeah, yeah, it's, yeah, agents all the way down. But again, like I guess one of the meta points here maybe is, you know, we're building these agents into these processes where it's combined with my human expertise to actually do this. Whether it's in our delivery workbench or we're trying to satisfy these customer orders or it's in this debugging video, right? We're presenting it in this interpretable interface where I can actually use my own taste. I can actually go back to eValism, look at the failures to gut check that with what the agent's outputting. I can take a look at these problems and figure out if they're actually improvements or not. And I can do that sort of gut check. But ultimately, like it's all of these different, you know, composable parts of the platform that come together in this very helpful way, ultimately, for this feedback loop.