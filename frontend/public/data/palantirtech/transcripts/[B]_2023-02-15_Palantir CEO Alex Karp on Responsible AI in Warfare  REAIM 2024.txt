================================================================================
METADATA
================================================================================
Title: Palantir CEO Alex Karp on Responsible AI in Warfare | REAIM 2023
URL: https://www.youtube.com/watch?v=z4jGmKUc6Aw
Published: 2023-02-15
View Count: 98,869
Duration: 1999 seconds
Score: 53.0/100
Rank: B (Basic - 基础背景资料)
Transcript: Available
Category: 
Source: whisper

================================================================================
TRANSCRIPT
================================================================================

I'm John Thornhill from the Financial Times, and I'm delighted first to be at this conference, and second to be interviewing Dr Alex Karp, the co-founder and chief executive of Palantir Technologies. Now, Alex, I think it's fair to say, is a somewhat unusual chief executive of a very unusual company. There are not many chief executives I can think of who have a philosophy, PhD, and who trained under Jürgen Herb, Abermas, and who have gone on to run a software company that was launched to apply data and AI to help track down terrorists after 9.11. One of his friends was quoted in the FT as saying that Alex is allergic to simplicity. And you're also quoted in that piece as saying, the country that wins the AI battles of the future will set the international order. So you're in the right place today to discuss all this. I wondered if we could start by you telling us what role AI is playing in the war in Ukraine, and to the extent you can, what role is Palantir playing? Well, first of all, very happy to be here. Thank you for that kind introduction forwarded to my relatives who we happy to know. I'm so accomplished. That quote just as a reference, I think, was from three years ago, something like that. And when I was saying this, when we got involved in what's now called AI in parts of the DOD on named parts, because Silicon Valley's view of technology was please send it to the adversaries and send the Carson Agents to America and the allies. If you're getting really rich, why should everyone have a good life, too? And we took a different path, which was. And so when they abandoned the US government's efforts to build AI in the war context, we had a lot of trepidation about getting involved, because this was like five, six years ago. And AI, as it was until a couple years ago, was really a bumper sticker for fraudsters, half fraudsters, and people selling snake oil. And when you're in software, you have to be very, very careful not to not be proximate to that, because it all looks like a PowerPoint. I could hand out a name company's where I know the software doesn't exist, and they would have a PowerPoint largely copied. They would figure out what we're doing and print it up and look better. The people pushing it to you will look nicer or better in the American context. They have wider teeth in this context. They have a better swim style or something that's very appealing. And so we got involved in the digitalization of warfare a number of years ago. At the same time, we were already still maintaining products that we had built largely for the West, but particularly useful in Europe, because you have an environment where data protection and anti-terror work has to go hand in hand. And that gave us an architecture for understanding how an enterprise would work with segmented data. So you just have a huge, typically everyone, now everyone's excited about AI, because it's A, not a joke. It's clearly transformative. And you're seeing the results of open AI and the consumer internet context. I would maintain that the history of technology from certainly in the last 100 years is things coming from the military going to consumer. And that's also what I think primarily actually happened in AI, where, and again, our role in Ukraine has been widely discussed. We won't say that much about it. But as a matter of theory, the old way of targeting where you don't use, where algorithms aren't used, is clearly a failure. And if you go into battle with old school technology, even if you're spending 65 billion a year and you're highly accomplished warfighters like Russia, and you have an adversary that you knows how to install and implement digitalized targeting in AI, you obviously are a massive disadvantage. But which the world heard. And that's why there's a lot of interest in AI. But what's very interesting is the way we discuss AI and public networks, consumer internet, doesn't actually translate to private networks. So if you're doing digitalization or AI on health care records or in the context of war, you have issues of, well, the work-fighting context you have, well, who makes a decision under what condition who takes responsibility, who does the after-and-action report. If the AI makes a decision, who's responsible for that? Before war, you have the ethics of war who starts the war under what conditions. During the war, what targets are taking out, what kind of targets can be taken out? Every country has an idea of safe harbor, IE targets. You can't take out targets that need special authorization, targets that need different kinds of action action reports. You have different kinds of segmentation inside data sources. So without going to any details in the Ukraine or other countries, you have very different data sources, they're structured in a different way. You have security issues. If you're working at war as everyone's room would know, you can't supply all your data sources, your access sources, your human and your signal to everyone on the war field, even by proxy through an algorithm, because that can be deciphered. So how do you run algorithms and run targeting packages where these things are paramount? So the adversary doesn't immediately know what you're doing, how you're doing it, and can replicate it. And then in the civilian context, this is a heavily regulated environment, for example, in Europe. You can't just run inter-gear-stictional AI on data sets and health care legal pharmaceuticals in any of the inter-gear-stictional AI is for lots of good reasons would not be allowed legally. Or in my view, ethically. So how do you make the AI or digitalization work in that context? That's something we've spent the last five years building. And what's super interesting now is it was a joke. And when I was giving you this quote, I largely just got ridiculed by my academic friends. But I think it's pretty obvious to people in this room, not as obvious, maybe to people outside this room, but the ability to dominate the war battlefield on our ethical terms, meaning where we can control who does what and under what circumstances will define who writes the law and how it's interpreted. Now, this may be as less controversial in Europe. It is particularly controversial, I think, in academic circles in which I lingered. But I view it as a banality and a fact. And certainly, the fact that AI is not an incrementally better technology, like maybe mortars and tanks, or one country has a slight advantage, it is completely logarithmatic and nonlinear. And so, yeah, that's a lot. But great. So just to drill down in that, if I, with my journalistic super powers, will summarize that as saying what you're saying is that when we talk about wars, we talk about mortars and tanks. But in fact, its data processing is now the new superpower that is enabling the Ukrainians to more than match the Russians. Well, yeah, you say, yeah. It used to be that something like an analytic package plus, yes. But, and what I'm saying on top of it is that the differential lift by advanced augmented or digitalized or AI is so great that powers that know how to implement this build upon it can defeat powers that have been spending $65 billion a year over decades. More recent quote of yours, the power of advanced algorithms is now so great that it equates to having tactical nuclear weapons against an adversary with only conventional weapons. Now, is that really true? Were you hearing from Professor Lawrence Freeman this morning that when the war started, everyone thought that the Russian cyberoffensive would overwhelm the Ukrainians, take out all of their civilian infrastructure that has largely failed. What they're doing is a very kinetic attack on the Ukrainians. Look, there's so much about this war that can't be discussed in public. I don't think that's exactly how this went down. But people in the room will know more than that than I. And so I don't think that's actually a fair assessment. And to word that true, then you're assuming that the capacity, first of all, there's offensive cyber and there's offensive targeting. I'm in the offensive targeting what we build, metaconstellation is in the offensive targeting business. It's a different business. But what I think is we are at the very, very early days of AI. But in the last six months, we went from a world where AI was and digitalized targeting and in general, like ability to take out targets. You could call it a digitalized kill chain. Whatever you want to call it, it was either something for a highly-area diet ethics discussion, which I find fascinating. Or something that someone was pitching you to buy where the results were going to be a stake dinner. And this has now shifted to your ability to identify the right technology and implement it will determine what happens on the battlefield. And I don't think there's any dispute about that. In fact, one of the major things we need to do in the West is realize this lesson is completely understood by China and Russia. And we still spend a very small amount of our budgets on this kind of technology. So obviously, very in favor of the general budget staying even growing inflation, going more in inflation. But if you look at budgets in the West, what are we spending that's differentiated from our adversaries? In business, you have a minority where you try to invest where people don't compete well against you. What we do very well in the West is figure out distributed systems that are powered by software that will allow you to fight. And this is a very specialized way of building things. This is why our software companies have done very well, quite frankly, most of them from the West coast of the US. But this is why battleship battle readiness that works this way is way. But what we need to learn is, wait a minute, what are we focused on? Who is making the decisions about acquiring advanced AI digitalization and what part of the budget goes? Have they ever implemented something that works? By the way, very basic things. You're going to get bombarded by various software products. Has the software ever been used in the battlefield? Should be the first question. Software is not a theoretical thing. I spent the first part of my life as an academic. There are a lot of theoretically relevant questions. The first question in a software context is, how have they used it? What are the results? You can't, these things take five, six years to build if you're very good at it. And you need access to the battlefield. You can't build these things in a lab. So one of the advantages, quite frankly, we've had, is we've been on the battlefield. Why have we had that advantage? It's one of these bug feature things, because everyone in the Silicon Valley was bereft of common sense. They were busy exporting things to adversaries and corroding our system with carcinogens in the context of the consumer internet. So we got access to the battlefield. So for the people in the room who are actually trying to buy this software, how do they know what software works? Given it's a little shrouded in secrecy, and no one will really do it. Oh, anybody in this room, I presume, who's buying software can get on the phone and talk to the people who are using the software. That's not their problem. The problem is, what happens after you find out when it works? Is anyone at the top who's making these decisions talking to the people who are proxies on the battlefield? Is anyone, what portion of your budget goes to it? Probably 99% of your budget is going to very valuable things, and not this kind of thing. So you need to have a wider swath. By the way, you also need a wider swath because you're going to do new things that aren't going to work. This theme of this conference is responsible AI. So I'd like to get on to ethics and values which you have mentioned. I guess to massively oversimplify in America, particularly after 9-11, there was a theory that the ends justify the means that the national security theory. Not our theory. I know. We'll come to that. In Europe, I think there's a clearly an argument that individual rights in some respects more important than national security, so which is the bigger problem? Big terror or big brother? We, Zach, can you reconcile the truth? It's always interesting to tell people they'd things that they will don't believe, probably justified we will never believe, but are true. A lot of the reason why my company succeeded was, in the beginning, post 9-11, everyone was like, oh, Bill, find terrorists, we don't care how. Most of our investors are like, find terrorists, we don't care how. And you make lots of mistakes when you do things. I made a lot of mistakes. One of the best decisions I ever made was reflecting upon Hagle. We can have a dialectic. We're going to find terrorists and protect data protection. And why is that important? And by the way, we hadn't even bothered selling our product to Europe, which is, you may not know it, but almost every European country. In some form or another, uses our CT product. Counter-terrorism product. And why did they use it? Because if you just find the terrorists and you abandon civil liberties, you lose half the society. In my view, correctly, do you want everyone knowing what you do in your personal life? I don't. And I think it's our God-given right to have parts of our personal lives that are protected. But if you don't find the terrorists, and this is where I disagree with a lot of my academic friends, you're going to have goose stepping in the streets. And I got yelled at all over the world for saying, our product, Palantir, has played an enormous role in Europe in stopping the far right. Not, here's all these conspiracy theories. Not because we're in any way involved in targeting the far right, but because if you protect data and stop terrorism, you get mainstream parties. If you don't protect terrorism, you get far right people. If you own, so you have to do both. Now, getting to the ethics, and what we found, by the way, is in doing both, you end up with something better. Because in fact, you only need access to the data that you have access to, and you can do an incredible amount on that data. Now, you get to the AI context, where it is a winner take all thing. And you have, again, the debate, what we should just throw out our ethics, of course, our adversaries have by Western standards. They do have ethics. They just have ethics that are very different than ours. And those ethics do not include segmenting data so that you can see how the decision making was done, before and after action reports, so you can decide what can be targeted, what deciding what drives the, what areas AI can be involved in, and what areas they can't be. And I would tell you the same thing that sounds academic, but has made my company very successful. When you actually reach for the higher standard, you end up with a much more powerful system, and you bring everybody in the society with you. Whereas if you just say we're going to throw out ethics, first of all, even if you wanted to, that is not implementable in any country in this room. It's not implementable, by the way, in America. It's like there's a slight method in America. It's like we're cowboys, we're like, yeah, we just run, and it is true, we're more ready for action than say some other countries. But you do have very strong codes of ethics about what you're allowed to do, what can be targeted, what are the apps, what happens if something went wrong, who's responsible? In every Western society, you have a chain of authority that goes from civilian to military. There has to be an ability to map that chain onto digitized targeting. It takes how much you do that. You need, and this is where you need an architecture, it takes five, six years to build, which allows you to segment the data, and allows you to expose parts. First of all, AI doesn't work in private networks as well as it does in pursuit for very simple technical reasons. You need to have a certain kind of tagging algorithm on the data. So when you're doing that tagging algorithm, you can also create an architecture, impose an architecture that can't be changed by the user on what kinds of data sets they can have, and those data sets can be made transparent as inputs into the algorithm. And you need to have that as a separate product that works with the algorithm. And every Western society is going to have to do it. And by the way, so that's the politically kind of progressive version. Let me give you the right wing battlefield position. I don't care about this. The reason you care about this, if you don't care about civil liberties, is because that's the only way to know that you are controlling your human and significant data, and it's not being exported to adversaries in your own organization. So if you want to say, as a purely matter theory, your Intel services want to do a special operation with your special services. And you want to make sure that every bit of that data is tagged and the algorithms that are feeding into it are not exposed. You will need the same architecture that civil libertarians want. Exactly the same. And one of the most interesting things about running this business for 20 years is civil libertarians don't understand that they want the same architecture that spies and special operators want for a very different reason. They want to come home alive. That's how you come home alive. OK, I want to open it up to the audience very soon. I have one final question that I want to put in before that, which is the role of private corporations in this debate. A lot of the discussion today has been the fact that we're now in effect outsourcing a lot of critical national security decisions to private companies, whether it's kind of starlink in Ukraine, whether it's to volunteer technologies in Ukraine and elsewhere, whether it's Microsoft and what they were doing to protect Ukraine against cyber attacks and so on. So how can we ensure that private corporations are acting in the public good and what governance mechanisms are there in this area? Well, there is a strong consensus among people in government that governments should decide and we should provide infrastructure. There's just a question of does the company actually believe that or not? So obviously, almost no company in Silicon Valley actually believes that. That's why there's all these lawsuits between the European Union and Silicon Valley. But we very much deeply believe that Western institutions to decide and we should implement. So the implementation infrastructure comes down to who controls what happens here. I believe that should be 100% the Democratic elected officials and their representatives in the military and other organizations. That is a controversial view in Silicon Valley. And that has to be verified. Does the company believe that or not? I 100% believe that our products are built to make sure that the mandate of power is controlled by Democratic, legitimately democratic, elected officials and their representatives in military and intel. There is another more difficult question, which is should we build the software ourselves? I would just say, of course, there are lots of things you should build. And what you see in Ukraine are lots and lots of technical people building on products, including ours. And that's something that I also support. The idea you're going to build the whole infrastructure yourself that I think is unlikely because the cultures that build these products are where cultures of madmen and women and others. And managing that crazy group of people is going to be very, very hard in a government context. But there are also big divisions within those companies themselves, aren't there? Google famously did not participate in Project Maven because its employees were against. Palantir yourself. I mean, your co-founder Peter Teal is a big funder of Donald Trump when he was president. You yourself are the government ass in public. Yeah. I think you said, I respect nothing about the do. Yeah, and then I was vulgar. But how you talk about companies as though there are single entities. But in fact, they... Yeah, but you're doing something which is very fair. You're imposing our political beliefs on the decision making. And that is a case. But at Palantir, we have a line and we tell everybody, if you do not feel comfortable supporting legitimate efforts of America and its allies, in the context of war, don't join Palantir. And so, you know, that, by the way, we are discussing recruiting. There are a lot of people who don't want to join Palantir because of that. I respect them. But then this is the wrong company for you. And we have... We also won't work... You know, we've never worked in China. We've never sent our products to Russia. Other people are like, are you working sending your products to Russia? No, well, it's just a different beast. And we do have huge political divisions around other issues inside Palantir. We are a free discourse kind of place. Lots of people... Most people at Palantir disagree with me on most issues. But... And that's completely fine. You just... But if you want to be a Palantir, you should know. By the way, you don't have to work on the military defense. Our US commercial is the fastest growing part of Palantir. It's growing like a weed or an oak, however. But it's... But you can... But you can't then come and say, I didn't know. And I do this, by the way, for example. And I, you know, German speaker, I tell... It's like, yeah, I respect the post-war German passivism. And... And then I've told this to railing people. I... We have people who want to join Palantir, like I just want to kill terrorists. Well, go somewhere else. You know, it's like... That's not... We... I really do believe in this dialectic. And the people we recruit and retain internalize that so that you are both protecting data and finding terrorists. But if you're not okay with either as a matter of ideology, you just have the wrong company. And there are lots of other companies. You're talking about the software. Is it desirable or even possible to regulate lethal autonomous weapon systems? It... Are these things should be regulated? And do you have two parts, the regulation and the enforcement of the regulation? Both are going to be tough. And that's why there's got to be an immediate and concerted effort to look at how that works. And that should be very much part of the funding. Like, you know, one of the things that worked very, very well in Europe, we always have to sign these contracts we can't talk in Europe about users or product, which is slightly annoying because I'm constantly getting yelled at by people who are protected in our products. But... But one of the things I really respect about European countries is you had to show that the product could predict data before you could get paid. And this exactly is what should happen in the autonomous, semi-autonomous governance targeting perspective. You should have to show the product to work in the counterterrorism context, sell your product in the counterterrorism product, context in Europe. You have to show the product works. And what does it mean that it works? That you can actually find sophisticated actors that are hidden while protecting data. Otherwise, you cannot get the contract. It's not a PowerPoint you have to show it working. We had one unnamed country where we powered internal external police in Europe. And by the time the tender was done, no one else competed because they didn't want to show the product working. But this is exactly what has to happen in this context. Okay. Questions. Please, can you keep your questions pithy and don't read out your endotural thesis? Who has a question? We have a bearded guy here in the front. The front. Thank you very much. My name is Young Adams Holder from the Dutch Ministry of Foreign Affairs. And how do you see the AI competition between the US and China play out in the next five years? There's different parts of the AI competition. People usually focus on AI in general terms and are putting two categories together. Internal and external defense. I don't think we are going to win, nor do I want to win on internal surveillance. China owns that. And I really hope we don't try to do anything similar to that. You have actually the closest thing you have to that is your device. We don't need more of that. We don't need to put that in the government monopoly of power. And so I assume that none of us want to go down that path. And I'm certainly not involved. I think that's the wrong path. Then you have targeting. And other areas that involve finding targets, all the things we've just discussed. I think it's proven that we have an advantage. But then our central disadvantage is what are we going to do with our advantage? Because right now everyone knows we have an advantage. And we are just still operating in a world where most of our efforts are focused on hardware. I'm not against hardware development. And we should have more hardware development. And some very large portion of budgets should go to hardware. But what needs to happen in the West now is focus like why aren't 10% of budgets on the things we're actually better on that are winning on the battlefield. That is we are so far away from that. I would say at maximally 1% I would be more realistic if you said software products that are useful on the battlefield, useful as defined by our adversary, not by a PowerPoint written by a consulting company. But what does China and Russia, what makes them, what gives them trepidation? It's like 1% of global defense budgets in the West or less. And if you define it as something they're afraid of and has been used, actually use not your cousins, nephews, local product, that it's under a tenth of percent. That has to change yesterday. And then people like in this room have to stay involved. We have a lot of customers and very different kinds of industries. Leaders have to sit on this. This is not the thing you can afford to outsource to the deputy you don't really like. Which is like one of the reasons we do so well in tech in America in the West Coast is we make tech a business problem. Too many institutions make it the problem of the unlikable person I'm not going to talk to. If that's going to be your approach to this, your country is going to have a huge problem. Learn, that's not what the Ukrainians do. It's like this has to be a central and personal focus of the most senior people all the way down to the bottom, backed by some. It doesn't have to be a huge part of your budget, but it can't be 1%. It's got to be like 5%, 6%. Because there's got to be some room for failure. And it's like if it's a tenth of a percent, you can only actually, it's not going to work. There has to be some ruthlessness. I'm very pro sovereignty. Our product is built to support your sovereignty. We have deployments all over Europe. People are waiting on top and below our product. I believe sovereignty is a crucial issue for every country in this room, especially the large countries in Europe. I support that. There also has to be a realistic assessment of what can I get now and what can I get tomorrow? By the way, companies like mine are very happy to teach you how to build this stuff, but you got to actually buy stuff that's off the shelf as well that exists now. All right. I can't see any other questions. So, actually, there's one right here in the front. Good afternoon. Thank you very much. My name is Hans Korsd. I have a question on the sovereignty part where you talked about and about this technology. It looks that this technology, the company who is in front and has the real experience, has a major advantage for everybody who wants to follow. And in Europe, we talk about strategic sovereignty. And we see that all the technology of this kind is coming from America. What should we do? This is a very important and good question. First of all, it gets serious. Up till now, the efforts in Europe have been like, obviously not I mentioned names, but I met someone I very much respect from one of the largest companies years ago. It was like, okay, yes, it comes from America. Most of us have deep roofed in Europe. Have you ever talked to any of the people there? Why is it so hard to come here and work? Why is it that discourse here between people who are, like, I'm pretty Europe affiliate? I left most of my life in Europe. I wrote my PhD in German. I very pro-French in my own way. I'll leave it at that. It's like in America, if I want a meeting with somebody, they buy in large one. I mean, I'm not saying everybody and I'm offensive to some people. But it's like pushing a closed door here. And also, if you want people entrepreneurs to come here, there has to be a special program. In Singapore, I could get off a plane and I have permanent residency. You got to have something like that in Europe. There has to be a serious effort to partner with people like my company and others where we have a cultural affiliation where we believe in the European values, but we have different skills with local things. And we have, on our part, have to build platforms that you can build on top of, be willing to show people how to do it. This is much more like a learned art. And that learned art exists mostly on the West Coast. By the way, it's not all of America. It's like certain parts. But there's like a performative seriousness now. I realize it's hard in Europe because one of the things I admire about Europe is it's radically egalitarian. Tech is not radically egalitarian. That's just a fact. And that's a very hard thing for Europe to digest. I liked that about Europe. I view myself as a classic European progressive. Meaning I still get to say what I think, I still have a personal life and I want poor people to have health care and teeth. Probably would be, you know, so that makes me like far left in America and I think probably right wing in Holland. But it's very hard culturally because this is not in egalitarian practice. But if you want your industries to be able to compete effectively for tomorrow and provide all the egalitarian benefits that I think most people in Europe cherish and that have been a light to the world, you got to get serious about this. And serious means like, what is it that who does this? Who can we learn from? How can we, who's in our local culture very good at this? I think France has done some really significant efforts on this. You see efforts that are successful in Sweden. There are little offshoots. But it's, you often have the impression when you talk to political leaders here. Yeah, but I could never say that. I could never do that in public because it would look unfair. Tech is unfair. But if you want a fair society, you're going to need some monochrome of a better software culture. And Europe is so far behind. And like, everyone knows this in private, but we have to somehow do better in public because like the whole European experiment, which is economic progressivism, is crucial. And something, by the way, that I very much personally support. But it will be, it will be completely called into question if everyone with the high value revenue is sitting in America. And it's not good, by the way, for our Western Alliance. Like having an alliance where America is providing all the high value revenue and Europe is providing the dollars. That's not an alliance that's going to be as strong as what I think we will need to do well against our adversaries. Get serious, Europe. I think we have to finish it there. But thank you very much Dr. Kup for a great talk. Thank you. Thank you.