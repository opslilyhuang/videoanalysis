================================================================================
METADATA
================================================================================
Title: Palantir x Databricks | Partnership Overview
URL: https://www.youtube.com/watch?v=wOyByRnOgIc
Published: 2026-01-09
View Count: 9,747
Duration: 1015 seconds
Score: 76.0/100
Rank: A (Active - 高参考价值)
Transcript: Available
Category: 
Source: youtube

================================================================================
TRANSCRIPT
================================================================================

Hi, I'm Chad Wolk, I'm an architect at Palantir. Hey, I'm Ben Abude, I'm an architect at DataBricks, and today we're going to talk about the DataBricks and Palantir partnership. So DataBricks and Palantir are better together. Was that mean? Really we think about this, how do I get to value quicker? So that means reducing the amount of times I have to move data around. How do I securely share data, allowing me to actually integrate with Unity catalog and Palantir, allowing me to choose the compute and storage runtimes across my platforms, and really have the seamliness integration across the two platforms so that our customers can get to value quicker. That's really the end goal behind this is remove the friction between the two platforms. And I'll add that this is really a customer-driven integration. So it should be no surprise that Palantir and DataBricks have many joint customers. Some time around November last year, a couple of our executives got together and were talking, hey, we have all these joint customers. They've been asking me how can Palantir and DataBricks work better together. Palantir and DataBricks both play a pivotal role within my ecosystem. And right now there's a bit of friction as I'm deploying the platform side by side. I want to do things like Shad said, I want to reduce data movement. I want to keep one copy of my data between the two platforms. I want to secure data sharing between the two platforms. I want more flexibility on choosing which tool I'm using from which platform when. And so as we had this request from many joint customers, we got together and said, hey, why don't we build a partnership? Let's build some first class product integrations to allow our customers to achieve faster time to value through improved interoperability between our platforms. So our partnership is centered around four pillars of product integration. We announced our partnership in March and we started working on the product integration a bit before then. And our partnership was centered around these four pillars of product integration, which are data federation, governance integration, compute push down our compute federation, and AI and workflow integration. Awesome. Yeah. So let's get some more. Let's get into some of the details of like what does this look like across the platforms. Maybe we can jump to the next slide here. So you know, the idea here of like how do we build this bi-directional integration? I think that's a key part of this partnership is people are like, oh, you can connect here and pull data here. Really, this is the synchronous bi-directional integration backward and forward. So I can have data and data bricks and register it and volunteer. I can create data and volunteer and register it and data bricks and move those back and forth seamlessly all with the unified catalog and governance around it and really having that interoperability story. I think that's the dream that people want is reducing them out of times. I'm having to move and copy and create pipelines. All of that is just slowing you down. And so this is really kind of a look across this, but the end goal is to enable the use cases that are adding value across our business and getting there quicker by removing that friction. Awesome. So what is the multimodal data playing? You know, if you look at Palantar's website, you'll see a lot of references to MMDP or multimodal data plan recently. And what this means is any storage, any compute, anywhere. And so within the context of our partnership, it is Palantar foundry at AMP plus the data bricks open data intelligence platform. And as Chad said, this is a bi-directional integration. So there's going to be some data that lives in powder. You know, your workflow data or maybe Palantar has a great connector for some data source you're connecting to that data is in foundry. But you want to access that data in data bricks. Well, as of recently, you can now register Palantar's ice progress catalog directly in data bricks. So now I can access that data from data bricks without the need to move the data. And the other direction is, you know, you're using data bricks, you're open data lake house, and maybe you've ingested a bunch of different data sources you've built out your medallion architecture. But I actually want to use that data within foundry for my workflows or for my entology applications. You can register that data within foundry via virtual table. So again, I can access that data without the need to actually physically move data across the wire. Yeah. And it's pretty powerful. And people start to understand like, what is the difference of like, why do I want to have this in data bricks and ontology here and all these things? And I think the key differentiator and where we see the value is, hey, if I've already got all my data together and pipelining, I'm doing this work in data bricks, now I want to build operational applications. And I want to model into the ontology, the data, not only the data, but also the logic and the actions to be really be able to drive these integrated workflows across your business. So now this brings the best of both worlds together between data bricks and Palantar and our customers can build these operational applications. And I have this also powered on the back end by data bricks, powering the compute and storage for your data across your enterprise. I think that's, it's a pretty important point about what is the difference between these layers and like, how do we utilize these in the best, in the best way possible to get those synergies? So we're seeing a lot of customers see a lot of success with this exact kind of methodology if I've built out these pipelines. The cool part and we'll show some of this later in a demo where I can actually author a pipeline in Palantir even and actually have the storage and compute running even in low code tools inside of the platform. So the ability to actually author and Palantir and select the runtime be data bricks is a pretty cool and innovative thing. Yeah, and this is, this is really awesome. You know, you might have Palantir found your experts operating within your organization and maybe your data is being federated from data bricks via that virtual table. Well, your Palantir experts can continue to author their pipelines and pipeline builder and code repositories, but they can actually run the compute where the data lives. As you're federating the data, let's also federate the compute. So again, it's going back to the concept of minimal data movement, you know, the ability to interoperate between the platforms and really build a bunch of product integration points just to make interoperability more seamless. Yeah, maybe you want to hit also on, we just talk about kind of the security piece of this as well and some of the integration with Unity Catal. You want to hit on that? Yeah, definitely. So our governance integration today is centered around the connection level. So what this means is, you know, from Palantir Foundry, as I'm setting up a connection data bricks, I'm going to, you know, choose my data bricks works based on what I want to connect to. I'm going to choose whatever catalog, whatever scheme is whatever tables. And I'm going to give that connection on the foundry side credentials to connect to data bricks. And so we recommend using service principle and workload identity federation. And what does it look like on the data brick side is all credit service principle on the data brick side. I will give that service principle access to, you know, whatever data assets I wish to federate to Palantir, then I'll attach that service principle to my data brick source within foundry. Now when foundry authenticates with data bricks, we'll authenticate as that service principle and use workload identity federation so we don't need any secrets for access to data bricks APIs. And then any data access from the foundry side will be permissioned according to that service principle. And the way the data federation actually works is Unity catalog can vent credentials to the underlying data assets. And so, you know, if you have your data assets, let's say living in S3 somewhere, foundry will reach out to Unity catalog using that service principle for authentication and say, hey, Unity catalog, there's some data asset, you know, at this catalog, this scheme, this table name that I'm aware of that I would like to federate that I would like to use within foundry. And Unity catalog will, of course, first check the permissions, say, hey, can I federate this data asset? Yes, I can. Great. Now it's going to vent credentials to that cloud storage location for direct read and write assets. So now foundry can directly read and write to that blob storage, to that S3 bucket where your delta or iceberg or parquet files actually live. Yeah, and that's a huge boost for performance by having the direct access to the where the data is instead of having multiple layers I need to go through now, having that unified in Unity to be able to vent those out and actually get direct access to the federated data. It's a great thing for performance across the board then. Absolutely. And I'll say, all of our joint customers using this integration, we have a bit over 100 customers using the integration today. The majority of them are using this virtual table or data federation capability. Yeah, that's pretty cool. And I think that brings up the other point too is like, you can actually back an ontology object with a table sitting in Databricks. And the fact that I can have a Databricks table back in an ontology object, I also can do write back. So a lot of times when we're creating these operational applications, part of this is I want to capture part of that feedback in the workflow. Instead of now me landing that in Foundry and I got to create another pipeline and move it to Databricks, I now can actually just write that back straight to Databricks. And if I have to do reporting or other workflows on top of there out of Databricks, Google it's right there. And so having that integration starts to remove all these layers as we're talking of now I'm not having to create a pipeline to bring data in to build an ontology object and another one to move it back to Databricks. I've eliminated those extra steps and that's just less things to break faster time to value, which is great for our customers. Absolutely. All right, so yeah, let's get into this. So jumping, jumping into the next slide, this is our product roadmap. And I'm not going to read through each of the bolts on this slide. But really what I want to point out here is over the past six months since we announced our partnership, I believe on March 13th, we have built a bunch of product integration work and a lot of this has been customer driven or customer led. So we had our sort of initial set of ideas, hey, what are the things that our joint customers care about? The data federation, governance integration, compute push down or compute federation and AI and workflow integration. And since then, we have been responding rapidly to the feedback from our customers. So as our joint customers have come to us and said, hey, this part of the integration works really well. Here's something I would like to see on the roadmap. We're actively prioritizing the asks from our customers of, hey, what's the next thing we should build? What does the rest of our product roadmap look like? Yeah, that's pretty cool. And this slide actually brings up another point I think we missed prior, but around the AI and workflows pieces kind of hit on the right back piece, but also models. So if your data science team is built to a model in Databricks, you can actually just register that in Foundry and use it as part of your workflows. And so having that, even if you want to have it hosted inside of Databricks and use it in Foundry, you can bring the model into Foundry and host it. You have that optionality across your architecture. So really making it as seamless as possible to get value out of both platforms together. All right. So should we take a look at what this looks like in a demo? Yeah, it's great. All right. So this is Palantir Foundry. This is our data connection setup. And so you can see we have a Databricks connection already set up. So when we started talking about this, we had batch syncs is how we would traditionally pull data from the environment so we'd connect the Databricks, pull them data into Palantir Foundry, virtual tables is the new stuff that we're talking about here. So this is where I'm registering data from Databricks in the Unity catalog straight into Palantir so we can use it. So hit create new virtual table. So this is actually going to read the schema information across Databricks. And so we'll get into this environment here. And we're going to search for DBX customer clone here for this demo. And so this is giving us a preview of the data from Databricks and I can hit add virtual table and I can hit create. Now, I've already done that for the demo here. As you can see, I have this table sitting here. So if we actually open that table up, we can look at it here. Again, it just looks like a Palantir Foundry data set. But there's a few key differentiators. So we have the sources Databricks. We also have some table details. So this is a Databricks source. It's a JDBC connection. So this is actually going through the warehouse to the Blobs store. So actually this is where Databricks is actually serving up the data to Palantir and Foundry here. It would say external access, for example, if we had a table that was just giving us the vended credentials from Unity catalog and giving us the location in S3. So there's different ways that we can access it and you can see those details in here. So if we look at this, I can actually go hit open and Databricks. So having an open and Databricks button right inside of Foundry is pretty cool. And here is my table. So this is that exact table in Foundry and I can see all of the Databricks environment around it. So we come back to Foundry. Now let's say I want to go do something with this. I can hit explore and create new pipeline. And we'll do dbx demo and let's pick the location that we want here is save. And when we hit external, so this would be a batch pipeline or run externally. So this is where I actually want to have the storage and compute sitting in Databricks. So this is where I'm authoring a pipeline inside of Foundry and going to have it run in my Databricks environment. And so let's pick this is our source and voila. So now there is the table. You can see this is saying external. And so this is our customer clone. So let's just do some basic transform to it inside of Foundry here. We'll do a filter rows and let's pick which column that we're going to want to filter on. Let's see. I think there's a state in here somewhere. All right. And we're going to pick is equal to CA for California. And we'll hit apply. And so if we come back and close here. So this is the cool thing about building here now is like I can even see the previews. So this is actually going back to Databricks using the transform, getting the results that will actually give me previews along my pipeline as I built. So that's happening right now. We can also then, so there is our preview and it looks like it was successfully only have California states. Awesome. So now let's say I want to do something else here. I want to actually create a new table. And so this is where I can actually select where I want to still live in Databricks. So this is now, let's see here. And we're going to do the default. This is our demo here. And it's going to be a managed delta. It could be managed iceberg or external. So you have that optionality. We're going to hit configure this. And so now I have a source in Databricks. I'm doing a transform and here's the result. All of this compute and storage is going to be pushed down to Databricks. So if we hit save and I'll hit deploy here and it looks like we got an error. Import those. All right. The reference. Cool. So let's try to deploy that again now. And here we go. So this is actually going to go execute. It's going to create a table named DBX demo chat and ban. And we can even then look at the deployment history as we normally would. So we can look at the build report inside of Poundary or Foundry like you're used to. But this is actually executing in Databricks. So this is a really awesome feature. I'm just kind of wrapping it all together. You know, Chad showed the Databricks connector. We had that table sitting in Databricks. We federated that data set to Foundry. So we picked that customer clone data set. We didn't ingest the data, which is virtualized or federated that data set to Foundry. Chad was unable to use the Foundry workflow is that everyone is used to pipeline build or code repos. In this case, it's pipeline builder to actually perform a transformation on the data. But that compute ran on Databricks serverless compute and the output data set was written directly back to Unity catalog. So again, there was no data movement here. We read the data from Databricks. We used Databricks compute to run compute where the data lived at the Databricks storage. And we wrote that output data set directly back to Unity catalog. So really, really cool feature. You can leverage all the tools you're used to in Poundary. Using whatever compute, whatever storage you want to. Yeah, it was pretty cool. I can see the logs as they're happening here in Platform. I can see the build just like I would. So it really is a first-class citizen inside of Foundry. And then let's go actually update our catalog here. Let's see if we can find our table. I'm assuming no one else has a table with Chad in there. There we go. And here's our table with just California on it. So there you can see from end and end. Now there could be multiple tables. And I could have joined multiple things together in my pipeline. But the idea here is I now have that capability to author here and push down that entire thing, utilizing Databricks for the compute and storage. That's pretty cool. All right, so Ben, I appreciate the time today. This is awesome. I hope people can understand and see the value proposition by making this so seamless that really is not just like a bolt-on or an after-dot. This isn't just a JDBC connection to another data source. This is a first-class integration of Palantir and Databricks. Yeah, absolutely. Thanks, Chad. It's always a pleasure.