================================================================================
METADATA
================================================================================
Title: AI-Powered Master Data Management | MacDermid Enthone & Palantir
URL: https://www.youtube.com/watch?v=m9IZeHzylcU
Published: 2024-06-04
View Count: 7,768
Duration: 2502 seconds
Score: 64.0/100
Rank: B (Basic - 基础背景资料)
Transcript: Available
Category: 
Source: whisper

================================================================================
TRANSCRIPT
================================================================================

All right, hi everyone. Thanks for joining. Let's go ahead and get started. Hello and welcome to our AI powered master data management webinar. We're excited to have you. I'm Audrey Sullivan and I work on the comp's team here at Palantir. Before we get started and introduce our esteemed presenters, a few housekeeping items. First, the session is being recorded and will be available for public viewing after the webinar. We ask that attendees not record the session. If you'd like more information, please reach out to us. We're always happy to walk you through the material in a one-on-one meeting. We'll kick things off with a few slides. There will be a live demo portion of this webinar and it's meant to be interactive. Please use the Q&A box at the bottom of your screen. We'll say some time at the end of the webinar to address as many of your questions as possible. Okay, I would like to introduce my colleague and master data management expert, Bruthic Vakkar to get us started. Bruthic, take it away. Awesome. Thanks Audrey. Hey everyone. Thank you for joining. I'm Ruthie Fikkar. I lead a lot of our engagements in the industrial space and particularly I'm very familiar with our master data management offering. We have a really exciting agenda for you today and I'm joined by my colleague Lucas from Palantir and Timothy from the Denver Dent Zone. As far as agenda today, we'll start with a quick sneak peek into the journey that McDermott and Thon took to deploy AIP for their master data management operations. And so Timothy will walk us through that and then we'll actually, like Audrey mentioned, get into the software itself and show you a demo of how AIP can help accelerate both the cleaning of master data. But then it can also be injected into the actual operational processes to prevent messy data and master data management problems from arising in the future. And then we'll end with a Q&A. So please save your questions for the end. We'll try to get to as many of them as we can. And with that, I'll hand it off to Timothy. Thanks, Bruthic. So the title of my presentation came from something I've blurted out once and it's kind of stuck with me ever since. But in the way of a brief introduction, I am not in IT. I am not a computer scientist. I am a chemist. And so my interest in this tool stemmed completely from my wanting to have my chemist work on chemistry rather than dealing with data. And fortunately, Palantir foundry is proving to be a good platform for McDermott and Thon industrial solutions to do just that. I'll give you a little bit of background. Next slide, please. So I think it's not an unfamiliar situation for many people to find themselves in a company which is grown sometimes through acquisition. And it's actually easier to buy companies than it is to integrate them. And my company, McDermott and Thon came together in the last five years over a series of acquisitions, which have significantly increased this in size. We're about three times as big as we were seven years ago. And after three major and one minor acquisition, we were in a position where our multiple ERPs and master data systems were really causing significant difficulties because the chemists who develop our specialty chemical products. We're spending a huge amount of time resolving discrepancies between the different systems and comparing things that were in one system against another rather than developing specialty chemicals. And this was causing me some angst. And the next slide kind of shows how it felt on some days where we were, as they say, devoting a significant amount of time to what I would call non value added data management capabilities. So fortunately, next slide, please, we were, we had our I team proposed a solution to this, which is quite simple. We'll go to one master ERP system, which as this beautiful diagram shows is really nice and clean. However, those of you who have ever been within the blast radius of such a project recognize that it's a lot easier to draw on a PowerPoint slide than it is to make happen. I was particularly concerned about the difficulty we were going to have with all these data alignment challenges. And it was pretty clear to me that quite honestly my group was going to end up being on the critical path for this. Some of our initial estimates of how long it would take us to do the cleanup required to get to a unified ERP were more than five years using the resources that were available at the time for for doing this. So this clearly something was going to have to change in order for this one master ERP vision to be put into reality. Next slide, please. Fortunately, we at that point happened to run into Palantir and the Foundry tool and were introduced to a capability I had never seen. I say I'm not a data scientist, but I had never seen a system that was able to actually hook in to all the legacy data systems wasn't trivial to do so, but it happened in a relatively short period of time. And then when they built the ontology, that was kind of amazing to be quite honest because suddenly not only could we see how all the data was interrelated, but for me, it was a good way of actually visualizing and describing the extent of the problem because it was a tangled web we had woven of all this. Fortunately, Foundry was very helpful in helping us untangle that and working together with with Palantirians, we built an app and Foundry that allowed us to compare all the data across the systems we were able to create and consolidate all that data fairly efficiently into one data model and put people, you know, in charge of doing that so that they could get really good and really efficient at it. In addition, along the way, we were able to create a mapping table kind of a Rosetta stone that would allow all of the entries in the consolidated data model to describe the relationship to the old data models in the legacy data systems. So this prevented us from having actually go change the old systems until they were migrated into a new system and allowed anybody who had access to that mapping table to be able to make sense out of how their system related to other systems. So we now do have a unified master data management system that contains this mapping table and allows people to easily access this information so that they can get on with their lives quickly. Next slide, please. At this point, we estimate that we were probably somewhere north of 70% faster on a per material harmonization rate that we're able to do and we're a little below half of the 16,000 entries that we need to harmonize. Of course, given the Pareto principle, even at 45% of the total number of entries, we probably are actually handling more than 80% of the volume of materials that we're doing now and we continue to get faster all along. So we the system, as you can see from the screenshot here allows you to see exactly what system you're working on. So it's also helped us in the project management in that we can target a certain system if it's slated for consolidation or migration into another system. We can effectively target that system for prioritization of mapping and that's that's kind of what we've done. We're not just we don't just have a giant pile of data and we're gradually sifting through it at random. This is this is targeted. The R&D control tower is the name of that. So it makes it very easy for everybody involved in the in the project as well as managing the project to see what the status is. And it also has allowed us to monitor something we call the collapse factor, which you may not be able to see the small number on there, but the current collapse factor in our data is 5.2. That's kind of a rough measure of the data quality. What that means is that for every five current raw material codes that we have in the systems, they end up collapsing into a single one that actually describes all five. The graph in the lower left left corner of the screen shot actually shows the distribution of collapse factors. We have some raw materials that are truly unique and have only one entry. And then we have some that have 30 or 40 entries which end up actually describing the same thing, which is kind of remarkable. So very happy with this. This is in daily use by a team of people in the company who are crunching through this on a regular basis. And we have a high degree of confidence right now that raw materials are something that we are going to be able to complete on the schedule that it needs in order to be able to do what they want to do for a single ERP. Next slide, please. Now moving beyond that, the nice thing is is that the ontology in addition to being a fascinating sort of graphical interface. As you build it and it sort of sets the foundation, the ontology once you understand it is actually an asset in and of itself. And we're starting to build on top of it and compound the value of it. So we're looking at all the processes that are in here and foundry can not only do the, you know, create this ontology, but you can also build workflows into it and different visualization tools. So we've been doing that. Now we're looking into the AI P tool as a way of accelerating some of those workflows and automating particularly new vendor qualifications. So what we are doing is a test case like everybody else, I think, you know, we're learning about what AI is and what it can do for us in generating confidence in the ability of the tool. So what we're doing now is that we're able to have a local purchasing person can come in and submit a new vendor qualification. So this would be for a raw material that already exists, but they have found somebody knew they want to buy it from for some reason. And last, this would be an email mediated process that they would send in. There is a process defined, but they send an email to somebody essentially and say, hey, can I use this vendor. And then there would be a series of manual steps for somebody to decide where there was possible. With AIP and basically what I'm out into a single day, we were able to sit down and go from a schematic diagram of how we wanted this process to work and what decisions we were willing to turn over to an AI model into a functioning prototype that afternoon, which is currently in testing within the organization. And this is like really promising in terms of what we may be able to do in the future to continue to use the data that we have and the aligned data to further add value on top of the current processes. Next slide. So as I mentioned, we're doing raw material qualifications. We have actually seen some material savings on this. The other nice thing about this is a test case is that because it does unlike these efficiency improvements, which anybody has tried to generate a business case based on efficiency improvements recognizes that there's a certain amount of hand waving quality to them. This is nice because you can basically say, hey, we qualified a new vendor and we're now going to be able to save this many dollars because, you know, dollars, euros or yen or whatever on this single material because we know the volume and we know the difference in price. And we can calculate it. So we do actually have one nice instance where we have actually received a significant cost savings projected on a single raw material that was qualified through this. We're also looking at a bunch of other things that we're doing with Foundry in which I think AI is also going to be helpful with this includes vendor management and harmonization customer harmonization as well as formulations management of the actual recipes that we use to manufacture. And this SAP S4, which is the ERP that was chosen that we're going to head towards for the unified ERP is now in a pilot phase with planned installation in the fourth quarter of this year in Germany. And as I said, I think we are going to be able to provide the master data that's needed to support this thanks to the tool that's been developed in the Palantir Foundry. There's also I meant to mention it the regulatory department as a chemical company, obviously, we want to be compliant with regulations in all of the global places that we do business and that can get very complex as well. So the regulatory department has actually started to work with Palantir Foundry to develop processes to speed up that process, which, you know, that's kind of an ethical thing for us. We feel very strongly about making sure that we're compliant doing this and now we're able to do it. And the estimate is that that's actually happening three times faster than it did before. So we have instances of a good hard savings that we can point to as a result of the development of the system as well as substantial increases of efficiency in processes that we were doing by other means before. Next slide. And with that, Ruthik, I'll turn it back over to you to show everybody how this works. Awesome. Thanks, Timothy. So with that, we're actually going to dive into the demo itself now. Fantastic. All right. So in many ways, one can think of mass data issues as a form of debt and not just any kind of that, but one where every siloed system that's operating in isolation is contributing to this debt compounding at a rate faster than you're able to pay it down. With every system where it's just easier in the moment to add a new material, add a new customer or add a new vendor, you're contributing to this compounding issue and hurting operational agility. And as Timothy pointed out, that can cost you real money today, where you're not able to leverage this integrated data asset and procurement negotiations or to optimize the inventory decision making. And what we're going to walk you through as part of this demo is not just about how you can leverage volunteers, artificial intelligence platform or AIP for short to perform this cut down of this existing debt more quickly, more scalable and cheaper than ever before, but perhaps more importantly, how you can transform MDM into an operational asset that can help drive your organization continuously forward. The example that we're going to walk you through today is centered in the world of chemicals, but you can easily imagine this extending to any sort of other environment where you have inputs and you need to produce an output, right, this could also be in the automotive sector. This could be in the medical devices environment or really anywhere applicable. The thing here are chemical raw materials, we can see potassium hydroxide, caustic, potash and many more. And in this demo scenario, these have been ingested from different systems from SAP, from Oracle's JDE and different users enter them into these systems at different points in time. So there are different levels of data available for each of these. For some of them, I have an identifier or a cast number in this case. And then for very few of them, I've already gone ahead and started manually harmonizing them into these super codes or parent concepts, here called raw material groups. At the same time, though, I can see that for most of them, this work has not been done yet. And it's a very daunting task to manually try and accomplish that. And the first step in a pre-AIP world would have been to parse through all of the documentation that I get for each raw material and try to find the needles in that haystack that help me then harmonize it. Right. So how do I find the density, how do I find who sold me this, the form, the impurities. I would have to go through these documents and extract that myself. And that task becomes even more complicated if you consider not just the volume of documents, but also the fact that for one, you'll qualify employees would have to spend time doing that because this is going to be domain specific. And if you're operating globally, this is also going to be complicated by the fact that you may get these pieces of documentation in Spanish, German, as Timothy mentioned, Japanese, English, whatever the case may be for this particular raw material in question. How do we simplify this, how do we speed it up, how do we make it more scalable with AIP? What we've gone ahead and done here is we can leverage AIP in order to perform this task that I just described that's very tedious, very labor intensive with what we call an agent. An agent is kind of like an intern that is going to be able to perform these labor intensive tasks for you with some instruction centered on the data that your business is operating. So that ontology that integrated data as it representing all of these systems that Timothy just pointed out a little earlier to the right, you can kind of look under the hood of that agent or into that brain, where it's actually going ahead and pulling up these two documents. Right here that I just looked at manually and then instead of performing that search for the needle in the haystack, it's able to using what's technically called semantic search extract the relevant properties almost like you can imagine doing it with a magnet much faster than you'd ever be able to if you were to do this manually. Now what we're able to do after that is we're able to go ahead and say okay, we want to not just get a chat answer back right what we want to be able to do is we want to go ahead and say hey, we've extracted properties, but we want you to provide a recommendation to the operational users such that they're able to act on these or reject them and provide feedback to create data driven feedback loops where we're able to learn over time and provide better. And provide better and better recommendations with each iteration. This also ensures that instead of parsing through the documents, the operational users can actually focus on only dealing with instances where the agent may provide recommendations that may not be entirely correct and most of them will be able to over time with trust in the system be auto approved, which further increases the level of automation that I'm able to achieve. Once I've gone ahead and extracted these recommendations or these properties, you can see them filled in over here, I'm able to go ahead and generate raw material group recommendations right. And again, what I want to be able to do is I want to leverage a P in order to be able to do that. I don't want to do these pairwise comparisons manually myself. I actually want a key to provide me with recommendations as to hey, this material that I've just selected. So potassium hydroxide 5%, which of the existing parent concepts is this most likely or should this most likely be matched to. And then in addition to that, in order for the operator to have as easy of a time as possible, I want to be able to get a percentage some sort of confidence associated with that. And while I'm looking at the life into the under the hood chain of thought here, this may not be true when I'm actually going through multiple of these right. But I still want to summary of how this AI agent came to that conclusion. I don't want some random answer. I also want a little explainer that I can leverage in order to make my decisions here. So you can see happening to the right here is it retrieving this particular raw material extracting the properties that we just pulled out of that documentation using a IP. And then it's performing a comparison against the representative properties of the parent group that describe its constituent materials. And then ultimately it's going to output recommendations for me that I can then again act on as the operator with the domain expertise here in order to decide whether to move forward and harmonize this particular material into that parent group. Or if I want to reject that and again use that to create these data driven feedback loops that will help improve the quality of these recommendations over time. Now, what I've just walked you through is how you can pay down that existing debt going back to that initial analogy that I was using to motivate why this is important. But what you actually want to do is you want to integrate this MDM process, even though it's AI driven into your day to day operations to prevent that from occurring in the first place. And my colleague Ruthik is going to walk you through how to do that. Awesome, thanks Lucas. So like Lucas mentioned, this is only part of the equation, right? So cleaning up the existing systems, the messy data is important. But it isn't enough because you want to get the the highly skilled individuals like the chemist on Timothy's team, for example, off of the hamster wheel of constantly having to go in and clean data, albeit a much, much more accelerated pace than they would have without a IP. And so what we actually want to do is inject a IP into the processes themselves that are creating the messy data in the first place. So whether that's procurement, supply chain, manufacturing, finance, etc. We want to help drive a IP to contextualize data and new data, more particularly that they're bringing into the organization such that not only is it keeping the data clean, but that additional contextualization is actually driving better operational decision making for those end users. So I'm going to walk through an example where let's say I'm a purchaser in my procurement organization and I've just received an email from a supplier that I'm looking to purchase a new material from. Now as a purchaser, I'm most likely confined to a specific manufacturing site where I'm tasked with solving problems by sourcing new materials to either alleviate shortages or to get my costs down. So I don't have a global view of the world unless maybe I'm an experienced purchaser who's been working at the company for 30 plus years. And so I'm often making these decisions in a silo and that's what's leading to this messy data in the first place. So what we're going to do with a IP is actually leverage the power of the ontology to contextualize these decisions as I'm making them in an operational way such that I can avoid creating data debt for my chemist downstream. Let's say I've just received an email from this supplier and he sent me a safety data sheet for a new material that I'm looking to procure. So I'm going to go ahead and upload that to a IP and what it's going to do is it's going to parse out all of the text from the material document and put it into chunks and embed those chunks such that we can use those downstream embeddings are effectively like bar codes for your text that make it easier to pull out just the relevant pieces of text necessary for the material document. I also have the ability to actually enter metadata so description of the document what type of document it is and maybe even the price that this vendor has told me this is going to cost so I can start to contextualize this against other materials that I might potentially be buying already. And so here I can see the document that I've uploaded from a supplier called calchem it's a sodium hydroxide solution and we do not have any data. So we have a sodium hydroxide solution and we just saw in the previous step that we actually already have sodium hydroxide 60% solutions that we're already buying and using potentially let other facilities. And again I might not know this as someone in purchasing and so without a IP injected into my process and contextualizing this for me. So I don't know this I might just drop this document into whatever repository I have create a new material entry in my ERP system and then maybe email my chemists for approval on this material. But again this this silo process is what's leading to this messy data and this debt if you will in the first place and so I'm going to leverage a IP to ontologize this document for me and put it in the context of the rest of my organization. So right there again I have visibility into what my AIP agent is actually doing and so what it's going to do is it's going to go through and retrieve the relevant chunks of text from this document for extracting entities. So the same thing we saw previously things like density form, cast number, etc. And again this is specific to the chemicals domain here but this can easily be extended to parts and assemblies and any other sort of physical good or material that you have some sort of multimodal data on. So here I can see my agents going through finding the relevant text extracting the entities and then starting to correlate this with the rest of my ontology. So my vendors and my existing material such that I can actually start to figure out hey do I have a material group that potentially corresponds to this material that I've just uploaded. Do I have an existing vendor for this material that I'm looking to purchase and I can see on the left here a IP has gone ahead and found hey you already have sodium hydroxide 60% and you also already have a vendor entry for palkem in your procurement system or your ERP system. On the right here I can see this sort of contextualize ontology graph that Timothy was referencing for this specific document. So as a result of ontologizing my document I can now see that this document is close to or functionally equivalent with this other sodium hydroxide 60% that I'm buying from a different vendor. And I can also start to see how much do we spend on an annual basis with this vendor. And I can also now see okay for the vendor that I am looking to purchase this from what are the global sets of materials we buy from them and how much do we spend on an annual basis with them. So not only have I avoided creating additional debt for my master data management team or my chemist or whoever sort of responsible for cleaning up this data. But I also have introduced context into my decision making process such that I might now have the ability to use the annual spend or the global materials that were buying as leverage for negotiating a better price or actually deciding whether this material is even worth buying or not. And so what I can do is leverage AIP to not just contextualize the document in the context of the ontology for me but also drive recommendations on hey based on this newly contextualized ontology. Should I buy this material does it lead to cost savings does it solve any supply chain problems for me. And here I can see my procurement agent is recommending that I should buy this material because it's going to lead to cost savings based on the existing materials we buy that are functionally equivalent. We also have significantly higher annual spend with this vendor and therefore we might have the ability to negotiate larger discounts. And then from a supply chain perspective this material actually helps prevent potential shortages that are on the horizon. So I've now gone from just having this document that exists in a silo that I typically would have just kicked over the fence to my IT team to put into whatever document repository and created additional debt to now having a contextualized ontology and the ability to actually action on all of that context that I've now been surfaced so I can immediately from here go ahead and create a purchase requisition or maybe notify other sites that are using sodium hydroxide. And then they say hey we now have a cheaper source of this material that you might want to look at procuring instead of whatever other more expensive source you're looking at. So this is sort of how we can start to inject AIP into the actual operational processes that are siloed fractured and often lack enough context to not only make sure the data is clean and we're not creating more entropy in the organization. But also that we're making the best possible decisions based on the data at a global level in the organization. So that's sort of a sneak peek into how we deploy AIP both for cleaning up existing data debt but also making sure we're not generating new debt and creating more entropy. Now most importantly here if this is interesting or something you'd want to get your own hands on you actually have the ability to get access to this yourself. So you can play around with this demo version and notional data using that QR code there. And then most importantly if this is something that sounds like it would be interesting for your organization if you're someone who purchases things and has data. You can actually deploy this with us in a multi day bootcamp such that we can create an ontology for you and configure your AI agents that are relevant to your organization and your workflows. So with that being said we'll now turn it over to questions. So I'll just look through here. So the first question it looks like is how long would it take to get this setup and operational maybe Timothy if you want to comment on sort of how we got the harmonization setup for you. The harmonization process. I mean the harmonization process took. I mean it was a few weeks or short period of months because it depends on how you define it. But if you have to hook into all your legacy systems there's obviously some work need to be done there. Then you have to understand the workflow that you want to use and the people who are going to be in it doesn't require a whole lot of training. So it's a nice thing to be designed relatively quickly. So I think depending upon the complexity of the systems that you need to hook into. Yeah you're probably looking at weeks or short period of months even if you're starting from nothing. Amazing. Okay, it looks like there's another question here about the guard rails that are in place to protect against hallucinations. So with these large language models being stochastic in nature there's no way to completely eliminate hallucinations. But the way we actually manage them and mitigate their impact is by having a human in the process. And so if you noticed in every place in the demo where we're using a IP we actually have a human in the loop to verify the outputs of a IP and we're giving them a look into the actual process and chain of thought that it's following to get to its recommendation. And then most importantly we're giving it the ability to giving giving the human the ability to actually provide feedback such that a IP isn't just a step change but it's also set up to continuously improve over time. And we can continue to mitigate hallucinations by encoding that tribal knowledge that operators have in their heads. Cool, can you talk a little bit more about the types of downstream or other workflows that this can unlock. There's a there's a handful here that you can see so once you have your ontology in place, right, it's really easy to move downstream into managing your inventory into negotiating with suppliers into material switches for managing material shortages, etc. But I also sort of like let Timothy ask sort of answer the question here because I know we've been building a lot of things downstream with his team here. So yeah, I mean the interesting thing with the Foundry tool is that you can if you choose give it both read and write access to your systems. We've been very cautious about that, like you say for two with the legacy systems, but we have automated the writing of cleanse data into the master data management repository. So that saves a significant amount of time as well. And certainly with things like a vendor qualification workflow, we have not deployed yet, but we do have a list of automations that we would like to move on with right now. It's basically doing what I'd say is a very advanced form of triage of the request and it is there is still very much a qualified human being in the workflow to decide what to do with it. We haven't seen honestly in the examples we've done we haven't seen any hallucinations from that yet, but we plan to include a human in that for an extended period of time. It's going to take a while for me to feel like, you know, the I don't know what how many nines I need in that certainty figure to actually turn it over, but we will I am confident that once we decide to give it a decision capability, you can automate an awful lot of information movement. So I'm not sure there's there's anything that's impossible. Awesome, and then we have another question saying at the end of the project, did you still end up with one ERP was there a cut over where all the acquired companies switched over to the new ERP and foundry is just housing the legacy data or is foundry continually bringing the disparate ERPs into a single repository. Timothy, I'll let you take that one. Well, we're working progress. So we are starting with the pilot deployment of what will become, you know, an extended rollout of a unified ERP. So the question is being asked is actually it's a really smart question because you're thinking out about what it happens. And we've had the same question at this point, foundry is not acting as the master data repository it's asking it's acting, you know, kind of in a transactional way to clean it all up. But there are scenarios and we've kind of entertained them where you could use it more extensively as a repository for information. Currently, we're using it. They say to process information and it holds on to some of the information along the way, but ultimately we do have designated repositories for all this information that will be used to fuel that one ERP. So they say, we've checked back with me in nine months and I'll let you know sort of where we're at, but at the moment, I think we're headed in a very good way. Yeah, at least your your chemist are no longer the rate limiting step as far as. So that's good. That's awesome. Okay, we had another question saying, can you please share where the ontologies use and where where AI is used that is the scanning of the document being OCI is part of AI. So that's a great question. So I think the sort of like the AI portions of this are sort of infused throughout right. So AI is being used for doing everything like helping create the pipelines faster to actually generate the ontology and workshops faster. But in the specific demos you saw it's being used to guide recommended recommended actions for the user. So the agents that you saw the chain of thought that was all AI in terms of actually parsing out the documents were leveraging a combination of traditional machine learning. So OCR models with stochastic large language models to actually extract the entities like density form and the specific chemical properties that are relevant. So maybe to add to that. You are seeing AI in action in many different places of that demo even right. So if we're looking at the documents, either the one that was uploaded or the ones that came from existing document repositories. It really depends on the complexity of the document slash the type of data that is present within that document as to what tools we end up deploying in order to get you the most value, the most bank for your book right. If you have documents that contain for example engineering drawings, you will not get very far with plain OCR, but OCR is much more cost efficient for all of the other text sections present within the document. So some hybrid approach even in this extraction stage could likely be deployed there. But what's really the core differentiator throughout the demo is that every time you looked at that chain of thought and the agent was looking through your existing ontology. Every time it did that and that's kind of also the reason why hallucinations within a IP are going to be occurring at a far less frequent rate than they would be anywhere else is because this is rooted in your data exclusively and in your tools as to how you interact with that data, the types of problems you need to solve the types of questions you need to answer specific to your domain versus something that is just rooted in the internet or data that was scraped from somewhere right. That is going to lead to issues where it's going to elucinate or combine something that it saw in your SAP ECC system with something that it saw on some SAP forum website and that's really where the risk is at. And that's also why everything that you're doing is always going to be rooted in that ontology and every time you saw that little document or material or vendor pop up in that chain of thought, you actually able to interact with that and click into it. Understand where and from what system out of all of the ones that have integrated did it pick this up from and I can also evaluate this over time as I go right I'm not just able to do some sort of chat question I don't understand what's happening and I get an answer but I'm able to understand at scale and we're happy to showcase this in one depth to anyone that's interested. I have hundreds of agents what did they do what data sources that they interact with what decisions that they may and also ultimately compare that against quality metrics that are relevant to the project that I'm working on. Yeah, and it's a really good point which is like the large language models are only as good as the data and the extent of context that you're able to provide them. And so the value of the ontology there wasn't there's sort of like interplaying with each other where a IP is helping build the ontology by contextualizing your document with everything else. But that contextualize ontologies now helping the downstream agents actually provide recommendations and operational actions better, which again, you know if you're just relying on a human going into some sort of chat interface and just pasting in some context you're sort of rate limited by the context that the human themselves has. Right where here you're setting it up such that AI can augment the ontology and on the ontology can augment AI and you're sort of creating this virtuous feedback loop with the human integrated into it. Awesome. Fantastic. Okay, cool. That's all the questions we can we can end it there. Thank you all for for joining us. Thank you, Timothy for walking over through this sort of journey you've taken with pound here and all the exciting things on the horizon. Like I said, if you're interested in actually getting access and playing around with this yourself, you can sign up via the QR code and then that's also how you can schedule your bootcamp where we can deploy this on your own data. And we'll also send a follow-up email with these details as well. Thank you so much. Thank you, everyone.