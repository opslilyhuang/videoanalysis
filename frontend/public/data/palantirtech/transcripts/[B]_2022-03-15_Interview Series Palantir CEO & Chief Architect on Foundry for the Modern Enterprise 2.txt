================================================================================
METADATA
================================================================================
Title: Interview Series: Palantir CEO & Chief Architect on Foundry for the Modern Enterprise
URL: https://www.youtube.com/watch?v=sp_xg38lJpE
Published: 2022-03-15
View Count: 33,862
Duration: 966 seconds
Score: 63.0/100
Rank: B (Basic - 基础背景资料)
Transcript: Available
Category: 

================================================================================
TRANSCRIPT
================================================================================

So I think like the traditional kind of analytics or data architectures kind of come from a long lineage of saying like the primary problem is data consolidation right data is coming from different sources. So it's going to be a little bit more of a process that puts it together in a place where I can then access it for reporting or for BI. So explain just in detail what is relational database do. Why is it built for certain things and why we'll break down and going from read from read to write. And then we'll talk about how it breaks down into just jointed world. Yeah, so relational databases and specifically data warehouses and what we think of as MPP or massively parallel processing databases are designed for effectively serving data at low latency to hundreds or thousands of users. Where the primary access pattern is I'm trying to read this data. Right. So how data is locked how data is kept somewhat consistent is sort of within the paradigm of saying the primary use cases I'm trying to visualize this data or pull it into another place to then kind of look at it or work with it in a read only way. And then let's so can now let's just start with the very basic functionality of you have to write meaning right function means you would actually take insights and put them in the data that are not the ones in the database. Where does that break down in here we're going to bracket where does it break down in a multi jurisdictional perspective where does it break down from an access control meaning data protection. Where does it break down in low latency so in a classified that we're going to leave those all out for now but in a very very simple all of which we built products for which is why we're powering a lot of the Western efforts and Intel and defense but just very very simple moving read to write why does it break down. Yeah, so if you think about how data is intended again to these systems it's through kind of standard interfaces where there are existing connectors or existing interfaces from other systems so it's system to system writing right. The key difference is when you have users trying to write data typically that happens through you know updates or inserts that are coming from end users was very limited attribution as to what the user is doing or how they're actually doing that insert or update. There's also like very limited capability to kind of segment out priority as to what users rights or update should take preference over others like it's very much designed to kind of via you know bus of in and out for I'd say like pretty pretty course types of rights right you think about like user rights the same way you think about system rights. It introduces a lot of peril in terms of well how could I potentially overwrite it. What is the different way of asking the question would be if you didn't have a standardized system to do this new loud people to write to the core code base or data what would happen and why are they preventing people from doing this what what is the technical challenge. Yeah literally the technical challenge is data that's coming in from source systems into these databases are getting stored in directories or tables they'll then be modified by in the exact same modification schemes by end users. Delete data you can overwrite data you have nobody to understand why data was modified to a certain extent and so it's just a lot of orchestration and a lack of versioning a lack of lineage means that you can't really allow people to in a multiplayer what. Most people don't know what versioning is what is versioning so versioning is actually understanding over time how to data change and not just how did it change like today's data versus yesterday's data but who changed it under under and from the results of what code or what process changed it that's kind of the key bit that's difficult you can of course have a log or a change. And then you can of course have a log or a transaction log of data changes and other assumption people have is you could just make some kind of app and write it on top of your relational database and all of a sudden you would be able to do versioning. Why is that why why is that wrong yeah so the devil's in the details there right so if you have exactly one app or a set of apps where you've built in the processes into those apps are into some middle where we can actually orchestrate and say this app under these conditions will write into these tables and take preference over simultaneous potential rights from another person. Then this can all work at scale right but you have to basically put the burden of that sort of orchestration or reconciliation work somewhere in the system right where like you could technically theoretically extend the database to do some of that but becomes a pretty intractable problem if you think about dozens of applications hundreds of users all doing rights under different paradigms again here we're bracketing some of the very important issues that are in the national security context like how would you deal peering between people who are both allies but not just sharing all the data how would you do this under with math with high latency how would you do this from space to mud as we've said somewhat in our jargon how would you how would you reduce the aperture of the data so we're bracket those issues for another video so just all which we we probably should do but but then okay so now we we have a basic nascent understanding of why it's hard to write to a database that how do you solve this problem and how do you move from read to op read to the data how do you move from read to operational and then as a last part of this video what we want to get to is in the past we've been viewed as antagonistic to IT programs and there there this is this may in fact you know this is a process but it's my view that there's been a major shift there where IT programs across the commercial space have done an enormous job in building read extensions onto their database and now have begun to understand that they're going to have a to do right functionality into operations and in fact very much poised to buy our functionality in the same way they were posed they've already built functionality that we built 10 years ago into their enterprise so first how do you solve go from read to write what is that what is the solution that we built in the core of our architecture and then how do you go from right to operational so I think the key thing that we did is we built a layer that sits on top of the relational data stores we assume there's actually going to be many of them the no-sequel data source the IoT data source the geospatial data source called this ontology is ontological layer which actually provides a abstraction point we're actually able to not just sort of present all of the data in a semantic and kinetic view to end users but also say this is going to be the mechanism to which you accept rights with all of the relevant metadata required to actually reconcile those rights with the underlying data and so the ontology provides in a sense not just an operating language why is it hard to build an ontology like this I mean first of all I could be yeah why is it very so the term ontology is actually not new right the industry people think ontology is one of the taxonomy technology is a little older than the industry but hey case go back to that even in the industry right even in the industry it's like people sometimes because it's a data model file is a taxonomy and the way that we've actually thought about the ontology is it has to encompass both data but also actions and process as well so I think the key thing is is you want to understand not just what is the way which you need to present data in a governed semantic way to users so the factories the trucks the transactions the customers the interconnections but also what are the legal actions what are the processes that can be taken with those objects by different user groups so I can actually modify this work order I can actually flag this customer transaction and as those different actions cascade I'm capturing their consequences in the ontology but also then pushing them back to underlying systems and so it gives you sort of this is a bit of simultaneously model both objects and also processes together the maybe a little bit of background here when we entered the IC space what we discovered was in this is actually sounds like the same problem but it is actually very different in the IC space that you would have to be able to index against data sets speak for both legal and practical reason so you you would actually be working with indexing layer not the underlying data and that we we realized that the actual databases were not set up to abstract out of the databases where the abstraction is actually the reality but you have to disambiguate the abstraction because there are going to be thousands of abstractions inside an enterprise that have to work together and then we brought that to foundry okay so you assume you have the ontological layer and it allows you to work with abstractions why is that allow you to write right and then you've already begun to answer it why does it allow you to operate against the data or the data becomes operational independent of what you're doing so the key thing I think is that the ontology layer holds both the objects and the relations but also it holds the actions as well and the and by way of doing that it not just captures how actions and cascade across other parts of the ontology but also says what are the mechanisms through which we did capture those rights reconcile them make sure they actually are approved and then push them to other systems and so you actually have a buffer where you hear by the way you're you're adding in how an access control model works against the abstraction layer you should probably talk about that yeah I think that's critical to why the rights can work so the fact is like there is no right happening in the ontology layer either from a system coming underneath or from an user coming back that isn't going to the access control model so this is a combination of group based access controls which would be integrated with the inner existing enterprise systems classification based access controls and also purpose based access controls and by the way without going into my macro thoughts once again obviously if you were building something for a disjointed world where jurisdictional sovereignty was more and more important you were sharing data across jurisdictional sovereignty but only according to the rules where you're allowed to do it you would need a robust ontology and an access control model any case so that that's the reason we spend a lot of time working on this okay so let's let's move quickly to or let's move to okay so you have here we started we began in the early parts of our business understanding limitations of the current of the data storage way in which they are so restored and software we moved from read to read to write to operational you see this both in our government and in our commercial business and the normus impact it's having and of course how even more in the future why was this viewed as being antagonistic to IT by the way in government forever although our business they are so big that and that that's kind of changed and then there's kind of a caricature of parent here that best software in the world they do a lot of things we don't actually understand but they seem to be very very valuable and someday we will understand them but is there going to be an absorption of this so you've seen it in in among some of the largest companies in the world and obviously it's growing in American businesses like doubled from 50 to 100 to 200 and we see this enormous you know the this enormous absorption with low with a with a with a nascent sales horse which is quite I think anomalous 18 years into a business I don't know any other business that looks anything like this so just this acceleration which you don't expect from a business has been around which is obviously product driven but there is this thing well is the friction coefficient of the internal architecture of internal IT structure going to be said now we went through the same thing in government and all over the world and we've seen how the shifts from being antagonistic to being people's best friends but what is it in the in the in the in the in the IT context of the shifted in the last say year and how have we shifted also to accelerate that shift and how we're providing a pound here yeah I think the fundamental source of friction was the assumption that we're trying to compete with the data integration and model integration layers right so the idea that hey I've invested in my existing databases data lakes data warehouses the snowflakes the red shifts the GCPs of the world and therefore I can't use pound here I can't use foundry whereas from our perspective it's like all the value is in extending like you said past sort of data storage into operations and so I think what we've been able to clarify and I think we've tried to do a good like an increasingly better job of is telling the story of saying but we actually think this ontology driven approaches and how the actual software works which is highly modular as you can bring your existing data storage you can bring your existing analytics tools and it's a little bit so it's kind of ironic because of course it's not competitive because we built the these capabilities almost ten years ago so we we know where the road is going because you know we've already been supplying those kind of things but in case keep going I think what we had to clarify though is saying like okay even though we provide solutions at those layers of the stack you can swap in your own if you want and I think that's where a lot of our engineering effort has gone as well I'm saying like how can I make it seamless to come to the table with my existing data warehouses your version is that you know and it but by the way that's it's true version it's not exactly to use it it's not exactly the way I see it I think it's both true that we probably you know because we're primarily engineering organization and we're not particularly good at sales have done probably a pretty crappy job of explaining to people how we're not competitive and probably a great job of explaining to people how we're smarter which is you know whatever probably not exactly what you're supposed to do but it's also true in my view that there's a learning process that just has to take place you know a lot of the issues that that IT organizations are dealing with are exactly the issues that we had to discover you're going to deal with I mean if you just look at our progression first we built things that were like data warehouses you know to so so the people could actually get something done right so the nascent pg and then then we saw well of course that won't work with you know so like unless you have these other capabilities and then we wanted to build it into a robust product and then we saw if you wanted to work very quickly you'd have to be able to build a product that you could sew into the architecture and then we began we under began to understand what are the issues up the chain and so some of this is our fault and some of it's just it takes it takes five years of doing these things to actually understand the issues that are coming down the road yeah and I think that process has actually been underway with the whole cloud transformation too right so people have moved to the cloud and I think successfully ported over analytics workloads right so instead of kind of the data stores of your and the MPP databases of your they're now using data warehouses data lakes and also analytics tools to do the BI and dashboarding stuff but now the logical question is in the market today is like awesome I've done that stuff now how do I actually get this stuff into operations like what is the what is the what is also people are under a lot of pressure because of you know just general macro events and that accelerates the learning process yeah so I think there's like I think broadly it's I think about it is sort of like different to knowing you have a problem and being able to act upon it or change plans or kind of like implementations that might already be in place so like as an example in utilities like it's like you might know that you're having like net decreases in power or you have issues that need to be addressed through a public safety power shutoff process but then how do you actually modulate how you're actually running the assets or how do you decide which work around decommissioning of old assets as prioritized first there's an existing plan in place which is a read-only view it'll be able to update that with new information not just based in our hypotheses but what people are doing in the field as well so there has to be kind of an orchestration of field driven learnings that are actually building the knowledge asset and I think that's what's super exciting that we see across you know supply chain telco healthcare as well is the decisions being made by end users in different contexts is actually the most important data not to go away and again on supply chain why is it difficult people tend to think okay I don't mean they watch some movie where people are like oh why is this a difficult problem for them in the kind of classical world the ERP and the planning process were all pretty integrated right so if you think about where the forecast came from it was kind of codified in a way that assumed kind of the underlying kind of standard sources were contributing to the forecast and then you know if there's kind of disruptions happening both in the supply and demand side all the time you say well you need to develop a better forecast really what I need to do is develop a better forecast almost every day and so to do that requires different data sources requires basically understanding which models to use under which circumstances and getting constant feedback from people who are operators using those forecasts and so it changes from saying here's the forecast for the month or the week to saying like what's the process for continuously assessing what optimal looks like with changing objective functions sometimes across factories or facilities too so it's just like a very different granularity of problem and also speed of iteration