================================================================================
METADATA
================================================================================
Title: Palantir AI Inference Platform | Enabling AI at the Edge, with Palantir CEO Alex Karp
URL: https://www.youtube.com/watch?v=b2b8NCSoqZY
Published: 2021-12-01
View Count: 25,394
Duration: 272 seconds
Score: 63.0/100
Rank: B (Basic - 基础背景资料)
Transcript: Available
Category: 

================================================================================
TRANSCRIPT
================================================================================

Here we are in New Hampshire in the center of the universe and my barn where so many things have happened. We are talking about some of the stuff you guys are working on. Palantir has had many iterations where we build products and we have to standardize them because our clients rely on them, especially in the national security context. Each time we have a cascading thing where you have this maximally creative focus and then it has to be kind of standardized. You guys are kind of in the beginning of the cascading chaotic creative process. So I'm Robinick. Software engineer lead. Been a Palantir about 10 years. What is the technical challenge actually solving? Why is it important? Yeah so the big change is shifting compute to the edge. So instead of bringing your data back into a foundry or a Gotham and making your decisions in some ground environment or cloud environment. We are actually moving the logic and moving AI but not only AI but also just like transforms physics math. But moving that logic out to the edge to actually run on aircraft on boats, satellites, ground vehicles we have deployed this. Moving transforms logic to the edge. Why is that difficult? Yeah so it's difficult from a technical perspective because you don't have all the power of foundry or a Gotham there. So you have to figure out how to build an engine per se that can run in these very typically low power disconnected environments where you don't have good network access. You're usually running on somewhere between 10 and 50 watts is typically where we deploy you don't have this big rack of servers but you have a very small device that has to run all your code. So we've been deploying to these sorts of environments and basically building this logic up and building these algorithms up in foundry. And then once you have confidence there in that algorithm and that model on the way it's performing then you can push it out to the edge so it can operate completely disconnected in a low latency environment potentially feeding back into the system so you could have an autonomous platform. And there's a lot of benefits to that especially in the defense that we've seen. My way of explaining it to outsiders is you have two problems one that you have you know low energy source so batteries and not endless supply and two that you have to somehow have the semantic layer reduce the complexity to the problems you can solve. Is the second problem also real or is that my own just my imagination. Yeah it is yeah because the difficult thing is you have to you basically have to build the tools that you're going to need in this edge environment and you have to know those before you land there. So you have to know a lot about the sensors that are producing the data what that data looks like and a rough shape of what you want to do to that data so that when you get it in real time you already have what you need because in foundry you can do data integrations you can get data that you know looks like whatever and you've made never seen it before and you have the tools to be able to work with it and to manipulate it and do a format that you can make sense of. But at the edge you you kind of have to know that you have to have it structured a little bit more where you can know what that sensor is going to be outputting. So like maybe also bastardization lay version would be you know in foundry we have unlimited supply of power unlimited ability to integrate the data on the back end. Here we have limited data and to factor you're pre integrating the data you're allowed to integrate on the edge. Exactly. Here's the Pound Here's Future. Yeah thank you.