================================================================================
METADATA
================================================================================
Title: Chad & Nicolas | Lightweight Transforms at Merck KGaA, Darmstadt, Germany
URL: https://www.youtube.com/watch?v=PGNWo-UuXLs
Published: 2025-10-09
View Count: 2,573
Duration: 1307 seconds
Score: 58.0/100
Rank: B (Basic - 基础背景资料)
Transcript: Available

================================================================================
TRANSCRIPT
================================================================================

Hi, I'm Chad Walquist. I'm an architect 
at Palanteer. Today I have Nicholas from Merc. He happens to lead all the data platforms. 
Palanteer being one of them there. And so we're going to talk about something called lightweight 
transforms today, which is a pretty cool new thing around processing and transforming data with 
different compute engines. So, thanks for joining me. Hi Chad. Thank you. Thanks for the invitation. 
Yeah. So maybe just a little bit about what are you responsible for at Merc and like what are you 
doing? Yeah. So at Merc I'm uh leading the core data platforms uh one of them being Palenteer 
Foundry. I'm with the company since 2019. Um and I think our partnership goes back even beyond 
that. Um and um yeah, I'm really excited to share a little bit about what we did together uh in 
lightweight transforms. Yeah, and and lightweight transform has been around for a while, but we've 
really reintroduced it and and rearchitected a lot of the back end here in the last few months 
and honestly a lot from your feedback uh as a customer and your willingness to work with us to 
really iterate and make it what what it really should be and what it is now. Um and so maybe just 
as a precursor for everyone you know lightweight transforms is allows you to have different types 
of compute um inside of uh the platform. So traditionally you would have like spark or flink 
as the runtime as default inside of pounder today. Now you can have those but you also can have 
different um engines like duct db pullers data fusion uh or even bring your own. So one of the 
things that I know you're a big fan of duck DB and you've been doing a lot of the transformation. So 
maybe as we think through this like just to frame it it's how do I like take the existing pipelines 
I have or even new pipelines and you know what is the benefit by moving to one of these engines 
for you? Yeah, I think I think the the benefit is definitely like the we we see a clear um reduction 
in runtime of the of the transforms jobs which translates to cost savings and uh so not only the 
jobs are faster but but they also need less less uh resources. Yeah. And so you get increased speed 
and runtime the less resources to go run and like what like for you like what is attributed to that? 
Is it, you know, uh different node sizes? Is it is it just the efficiency of the compute? Like what's 
your perspective on why it's so much faster and like how much faster are you seeing? Yeah, I'm 
I'm see we we're seeing like across the board 10x improvements in terms of of the performance. 
Um and I think I attribute it to like DDB or Polar Stata Fusion, the the things you mentioned being 
just another generation of of computer engines. um leveraging the the hardware that is available 
on the hyperscalers uh instances with NVME SSDs uh high memory instances where actually for 90 or 
even for us I get want to say 99% of the workloads you don't need a distributed system uh you don't 
need spark so there's no overhead um distributing the task collecting the tasks and so on this it 
just runs on one machine that has a very fast disk very fast SSD So even if the data goes beyond 
the memory of the instance which can very be uh well the case when you when you think about PA 
files being uh decompressed in memory uh you end up with potentially hundred of of gigabytes 
of of of uh data. Uh but for the new engines since they are streaming based and they they 
juggle between um the CPU, the memory and the disk um this is not an issue at all. Yeah, that's 
awesome. So really like new rust based frameworks, high performance single node, faster disk 
spillover, like all these things come together to really change the paradigm of how we compute. 
And so okay, so you've got probably hundreds and hundreds or maybe thousands of pipelines inside of 
Palunteer probably in legacy, you know, the Spark kind of framework and transforms. And so you kind 
of I think we have like rough around about 100,000 builds a day on the platform more or less 100,000. 
Yes. Wow. Okay. So huge scale. That's that's uh awesome. Okay. So hundreds of thousands you can 
you can step in my role and say like how can we make that more efficient, right? And then and then 
uh obviously all of that is written in pispark. Well, not all of it but most of it is written in 
pi spark. Um, and that's where um really like the idea of lightweight came in. Uh, and yeah, I mean 
actually it's triggered like by by by by by the evolution in the field. you see those benchmark 
and you see hey why how can we how can we um uh benefit from this and that's why we I 
keep constantly um chasing the palent team right and saying hey um it's time it's time um 
and yeah essentially I think I think that's when this year or early this year this took off and 
um somebody said yeah let's let's do something um yeah Yeah. No, that's cool. I I love it when 
customers push us into new avenues, new areas based on what you're seeing, what your needs are. 
Like the the intersection of hundreds of thousands of builds a day, uh, and your cost reduction, 
like that's huge potential, uh, for you. So, I could see why you were pushing so hard on that. 
Um, and I'm glad you did because now like together we have created something that's pretty cool 
to be able to have any of these different uh, data and compute engines um, running inside the 
platform through lightweight. So as you're down, okay, so you have hundreds of thousands of builds, 
tons of PISpark code, what does it look like to transform into lightweight transforms for you? 
Yeah. So um I want to take a step back. I want to highlight like what really enabled the new 
lightweight for us, right? The new lightweight experience. And that was basically to to say 
the standard should be the S3 API because as every every framework the best support you get is 
for the S3 object store right and and making that also the interface in lightweight I think that 
was the game changer that unlocked all of this um and not having any proprietary APIs you 
know where where Palunteer or you know you become a bottleneck running behind and say oh we 
oh we support this or and then tomorrow there's a new API. Oh, we need to support there's a 
new engine, right? We need to support engine X engine Y. But like once uh Lightweight moved to 
the S3 API or to the what what you call S3 proxy, right? Um that is the fundamental unlocker here uh 
to to leverage any kind of engine. And in our case um we we are observing uh this space for some 
time. Um and and really the the current leader here when it comes to stability as well and 
performance is is stuck db um and have a perfect and really efficient S3 uh park reader and writer. 
And if you have that uh in a lightweight container um there's not much stopping you from getting 
quick builds. Yeah, that's very cool. Okay, so that fundamental architectural shift unlocked 
not only DuckPB but a lot of these other ones to have better performance, better interoperability. 
Exactly. People can bring their own engines now as well. Um, so but DuckDB specifically has 
been pretty fantastic for you from a stability standpoint as far as you know, no out of memory 
issues and things being handled really well from the stability that opensource. Let me let me 
go there a little bit more into detail because yeah you you asked for it as well. Um for um for 
our scale there they there are kind of like two um uh like it's kind of a distribution of of 
jobs, right? At the one end you have the ones that run multiple hours and uh actually take a 
lot of uh compute seconds and a lot of cost in the end. And then there's the long tail. you have 
hundreds that maybe run for a few minutes and but then maybe also run very often and for both 
of them we kind of needed a solution. Um so um yeah actually um what we created internally in 
in my team was a reusable library uh that uses a open source framework called SQL frame that is 
implementing a spy pispar compatible API. So you can actually take your Pispar code, your existing 
Pispar code and translate it or com compile it, transpile it, how however you want to call it, 
right, to SQL to to Suduct DBS SQL. And then we we hooked in the the inputs from Foundry and the 
outputs and um were able to uh first of all give that to power users that worked on the um high uh 
impact data sets that take multiple hours and so on to build um and are very cost uh highly cost 
uh costly on the platform. But at the same time we were able to give it uh and bring it into 
the processes that are highly standardized. Uh, think about your SAP data sets where you always 
need to do the same cleaning steps for string columns. It's in every data set and there are 
literally thousands on the platform because as a company with a with a legacy, right, you have 
we have a lot of SAP systems. Um, yep. And um, but anyway, all of them need the same cleaning 
steps and then you do it once, but then it's a basically a huge generator of transformations and 
and then you roll it out across the board. So, so those two things is where we where we initially 
uh and are now running uh lightweight with stuck DB in production. Wow. Okay. So, this is in 
production. It's across lots of different modalities that you're kind of testing out. Um 
so, like maybe maybe some examples. Do you have anything we could take a look at as far as some 
examples of what this actually looks like? Yeah, sure. Um we can we can take a look. Um maybe we 
we start with um like one of our transforms in the company uh and the controlling area um where 
basically yeah the complete um data supply chain of of the controlling area is brought together 
um and and those jobs they with and and this is I think with at least eight executors or 16 
executors I know it's a little bit small but they took like you you see um like six, seven 
hours here, right? Uh in run. Wow. Um and then after we migrated here um in in July or in June, 
um we are at um uh 26 22 uh u 10 minutes even. Um if we're lucky. Um so and and this is B this 
job is really not like not not a hard one, right? It's it's a it's union. It's a little bit union of 
different data sets, a few filters here and there, but I think the key difference is that DuckDv is 
able to vectorize a lot of instructions um and has highly efficient like union all capabilities um 
that we don't see in in Spark open source. Yeah. Um and then yeah, you can imagine that for me as a 
platform owner this is really nice. Okay. But also for our controllers or for the head of controlling 
or whoever in the end looks at the reports, the dashboards, the or at the data products, the 
AI, whatever is done with this, right? Uh it's a much uh shorter cycle from from the ingestion to 
uh to the actual finished uh end data product. Yeah. Wow. That's so you're talking 7 8 hours 
down to 20 minutes. um if I'm doing my math, that's more than 10 times on that one alone. But 
obviously that's not every use case. It's running with one node with four or eight CPUs instead of 
eight nodes. So it's like maybe it's an extreme example. Um but um definitely something that that 
you see here. But I think that goes goes to your point though around like just even the engine 
optimizations around unions. Like if I don't have a distributed system, be able to do unions on 
a single node is just much more efficient period. um rather than just you know like all of the 
gathering of stuff back together to do a union is a very expensive operation in a distributed 
system. So like those optimizations on top of the node size it's rust based it's you know all of 
these kind of things uh add up to pretty cool use case like that. So now that you're doing this at 
scale like what are you seeing are there things in palanteer you're still working with us on? Yeah, 
I mean first of all we are working together to get the duct DB integration to be first class, right? 
Um I think that's important for me. Uh but also I think I'm also talking to many other customers 
that want it. So um um that's the first thing. But then at the second uh the second thing is that 
I think there are still some um infrastructure um limits that could be improved uh in the platform. 
So this could even be faster. This could be even faster. That's my my hint. U comparing to like 
bare metal performance of these jobs. Um so that's quite exciting. Um probably like maybe 
that's like my personal theory. Nobody has hit those limits like with the Spark jobs and now um 
it's time to to push that barriers as well. No, that's awesome. This is what I love customers 
pushing us to continue to evolve and improve and so this this is awesome. I think like you said 
just like how do we benchmark against you know what's out there and finding these new new areas 
whether you're hitting a new benchmark because of like crazy high utilization because you're just 
hammering these nodes now with duct DB. Um I I'm very curious what we find there as well. So 
you've got some more examples here and it looks like this one is a build that's happening quite a 
bit more often versus other one was kind of once in a while once a week or so like this build all 
the time like this is interesting. It's like what type of transform is this? Yeah, I think and and 
and this one is um also interesting because it's not like a simple union or so. So it's like a 
really complex thousand lines of pispar code that is being translated to duct DB with SQL 
frame and um yeah you you can see clearly see the impact here going from I would say three one 
between one and 3 hours to uh to under under 30 minutes 30 40 minutes but this is run uh at least 
every day. Um, so the impact is also multiplied by the the amount of times this is run. Yeah. 
Yeah. Well, that's really cool. So these you're basically able to just take this library, convert 
the pi spark code into lightweight trans into the lightweight transforms and have duck DB be the 
backend engine here and then like you just get the improvement. Yeah, exactly. I mean this is 
an open source library, right? So um you you you need to know what you sign up for, right? Um but 
this is what I love from open sour source right um we're able to contribute we're able to open 
um and same for ductb right same for polas right um we're able to contribute we're able to provide 
um issue descriptions reproducible issues and then uh and then it's it's getting fixed or we fix it 
ourselves right um and and and that's essentially the beauty about open source yeah very cool and 
I see that you've got an M seg Egg tap. Well, I I know too much about SAP to know MSEG is is one of 
the bigger tables and fun ones that Okay. Wow. Uh that's a pretty big drop. Yeah, exactly. Yeah. So, 
I told you about like the long tail, right? The long tail of of of jobs that are all kind of the 
same cleaning jobs and and and you see like this seems to be um um running very often um according 
I I hope there's a valid business need for it. Um but that's not for me to decide because that's 
closer to the use cases to the data products, right? Not in the platform engineering team. 
But you see it's running quite often and after me we it's even not a it's not like has kept all 
the history in this chart. It's only showing the last 1,000 uh runs. Um but luckily for us in this 
in this session that we still see the spark runs um and you still see that they take like you know 
15 16 minutes maybe if we're lucky like 6 minutes and I I guess you clearly see the migration 
point right um and yes and this is now running with um with uh I mean I have to lie probably 
two CPUs and 15 gigs of memory. Wow. Okay. So that I mean just the the footprint and the speed 
here. I think that I mean that's the thing that's surprising me. It's cuz normally it's like okay 
I can speed a job up. I throw more resources. I take resource away. It slows it down. Seems like 
you've got less resources and a faster runtime. Yeah. I don't know. It sounds too good to be 
true. Um yeah but but it actually you can see it here. It it it actually uh does work. Yeah. 
Yeah. Very cool. Awesome. Well, this is this is really cool stuff. I appreciate you sharing this 
with me. Is there other things um you know in the future that you're excited about this transition 
or like are you going to continue kind of pushing us for performance other things and then you know 
migrating more and more jobs or what's your what's your bigger vision with this? Yeah. So yeah, 
so I'm I'm really excited about the like the future in in this space as also in the kind of 
lakeost space right because uh with iceberg um so I'm not only managing like foundry right also 
other data platforms and and with iceberg for the first time there is a protocol there's a standard 
uh even though it's definitely not perfect but um all of us can somehow uh agree on and with foundry 
having embraced iceberg and yeah I'm working with with with with your colleagues on testing it and 
and and uh and adopting it um but then also with the other vendors um and and that's really great 
because that allows us to pick the right engine for the right job um ultimately um maybe it's 
duct DV maybe it's some other cloud data warehouse um but in the end we can decide uh based on our 
prerequisites, based on our skill set, what is the right tool to pick uh to provide the most value 
for for for the business. Definitely. Yeah. Yeah. I'm excited about iceberg as well. Like I think 
there's a lot of cool stuff that's going to be coming there. Um but I think I mean your your 
description right there is the thesis behind the multimodal data plane is like any storage, 
any compute, anywhere. Yeah. Sometimes I think it's because of us. I don't know. Um because you 
know it's definitely not. I'm just joking Chad. It's definitely um like we have I we have we write 
like since since virtual tables are there right we write to big query we read from bigquery with 
virtual we have snowflake read right to snowflake AWS iceberg glue catalog all of that we have in 
production and it it wasn't possible two years ago not at all I mean just with maybe with a lot 
of hacks but uh but today it's supported and it really allows us to to build this ecosystem 
here at Merc this platform ecosystem that truly integrates them. Um and and that's what I'm 
excited about. Yeah. Um real enterprises have very comp especially enterprises of your size have very 
complex heterogeneous architectures that have been through acquisitions and time and all sorts of 
stuff. And it's like yeah in a perfect world you replatform everything and have some clean slate 
but that's never going to happen. And so like how do you embrace the chaos? And it's like okay let's 
actually give you the tools to to manage that. And so that's that's cool that you're really starting 
to see that and embrace that across uh everything you're doing. So I really appreciate you taking 
the time to to show me and and everybody the the cool stuff that you're doing. Like I I just also 
value your partnership here of like pushing us and seeing what is going on in the field, what's 
working. Like this is exciting to me. I can I think you can tell I'm excited because like I've 
been doing this data pipelining stuff for 15 20 years as well. And like this is truly cool to be 
able to see both cost reduction and speed up out of a simple like addictive like when you do those 
migrations. Um and it's also everyone benefits because it's not like um we don't have enough 
use cases, right? Uh we need the room. We need the room to build new stuff, right? And to build 
AI use cases and to leverage AIP and all of that. uh which which would be very hard if we 
we keep like going with with Spark engines uh all of the time. Yeah. Yeah. Awesome. Well, 
thanks again. I appreciate it. I really enjoyed the conversation, Nicholas. Thanks, Chad. Uh 
thanks for the invite again. Appreciate it.