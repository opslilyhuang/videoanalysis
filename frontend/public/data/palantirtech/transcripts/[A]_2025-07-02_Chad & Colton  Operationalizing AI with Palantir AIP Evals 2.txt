================================================================================
METADATA
================================================================================
Title: Chad & Colton | Operationalizing AI with Palantir AIP Evals
URL: https://www.youtube.com/watch?v=iMF6cnU3o3o
Published: 2025-07-02
View Count: 5,741
Duration: 1929 seconds
Score: 76.0/100
Rank: A (Active - 高参考价值)
Transcript: Available
Category: 
Source: youtube

================================================================================
TRANSCRIPT
================================================================================

Hi, I'm Chad Walkwells. I'm an architect at Palantir. Today, I've got Colton joining me from the Privacy and Civil Liberties Team or PCL team and you are a Ford Deployed Engineer, right? Yeah, I'm a Ford Deployed Engineer. These days, I helped lead a lot of our work on AIP evals, but I'm also a member of the Privacy and Civil Liberties team. Yeah, how long have you been in Palantir? It's coming up on two years now, which I'm going to fly and buy. Yeah. That's the same here. I'm about to hit the two-year mark. I think this week. Okay, cool. So, all right, evals, privacy, civil liberties, all these kinds of different kinds of things. Maybe we can step back and say, what is evals, both from a product and a conceptual standpoint? Yeah, for sure. To answer the question of what is evals, it's essentially our integrated toolkit integrated with all of AIP's tooling for building generative AI-based applications. That's things like AIP logic or even AIP agent studio where your regular old touch script functions or Python functions. Essentially, it lets you test and evaluate them in a way that's purpose-built for handling all of the challenges of dealing with is very powerful, but unwieldy, perhaps fallible, generative AI models. Yeah. So, basically, it's the tooling to help you think through how to evaluate stochastic functions that are non-deterministics. We've had ML opts suite of tools for a very long time on deterministic classical ML. But now, with generative AI and agents, they're like, how do I know this thing's doing right? When is it hallucinating? Right? And so, I think that's the focus here is that same concept of classical ML and evaluations into the generative AI space to be able to really productionize it. I think that's the other key part here. It's so important to get production to workflow. How do I know and trust these agents running through my business, right? Yeah, exactly. Yeah. And with, I think, AI agents in particular, it's an interesting blend of some of the sort of testing evaluation you might want to do in classical machine learning systems, as well as these like more software engineering unit test style practices where you kind of need this marriage of the two to effectively test and evaluate generative AI systems. So, that's kind of the genesis of AIP eValus as a product. It's supposed to fit that niche exactly. Yeah. And I think, at least the mental model I have, and maybe you can correct me here, but it's like an eVal is in the agentic kind of flow is what is the outcome of my business process that I want this to happen? And then through this tooling, I can determine if these agents are basically driving towards a business outcome? Yeah, exactly. Yeah. So, you can definitely connect through of an evaluation or like a specific test case to a business KPI, something that matters in the sort of big picture sense. I think the cool thing about eValus as well, which we can kind of get into here as well. And a second is we can also do sort of more granular intermediate evaluations as well on like, you know, chain's computations that happen, you know, in the progress of an agent. And maybe we can see this in some of the demo here in a second, but like the other piece of this also is incorporating humans. And we talk about a volunteer human in AI, teaming. Like, what's your perspective on how that fits into the evaluation piece and where the human feedback and intuition comes in? Totally. Yeah, I think what we've seen sort of in the field as we're rolling out production, grade use cases is that the most effective evaluations are sort of a blend of, you know, perhaps more traditional kind of like I was saying, software engineering style, unit testing where you can kind of think a priori about unit tests, but ultimately with a lot of generative AI systems, users don't actually know what good looks like for something like say a summary that an AI system generates. Until they've actually seen it, they've interacted it, they've put it in the context of the operational workflow that they're interacting with. So a feedback loop where you can actually get users to, you know, flag when something is going particularly wrong or right, and then having mechanisms to operationalize feedback that we're collecting from users and turn that into evaluations or ultimately ways to improve the underlying workflow is kind of paramount. Cool. So yeah, I think what we have sort of here is this application for inventory management, right? And so the idea is this this notional application is sort of the interface that's rolled out across all of the different distribution centers at ONIX. So we have, you know, people on sort of the factory floor to like folks kind of managing operations, looking at this interface, they're seeing all of these customer orders come in. And what we're seeing here kind of displayed on the screen is my agent board where I'm representing the process of essentially order fulfillment from, you know, a new order that comes in from an inbound customer email or voice mail or whatever, all the way to like, you know, allocating sufficient inventory for that customer order pairing that with a truck that can actually take that order to the customer and then, you know, getting that sort of out for shipment getting orders ready for delivery. And, you know, what we're seeing here is agents that are helping, you know, along with that process at every step of the way. And again, it's built on that idea of the human plus AI teaming approach, right, where agents are not just automating everything in its entirety, but they're choosing to delegate things to users when it's appropriate. They're smartly routing things maybe and putting a pause on certain automations when a customer needs to take action. And so we have these rich interactions between the human and the AI agent that drives the progression of these customer orders through each of these steps of the process. That's really cool. I mean, the way I think about this is like, the AI is doing the ordinary, so me, the human can do the extraordinary, the difficult things. And so it's really the together that actually is, you know, kind of one plus one equals three in this scenarios, because I can really think strategically about the most important things, because I know AI has all the the grunt work for me. Absolutely. Yeah. And I think the thing that, you know, I kind of think about when I see a workflow like this is obviously there's such promise for automation here, you know, despite that promise, AI agents are fallible or they get things wrong. They're not doing necessarily trusted 100% at all times, especially when we're kind of taking a, you know, generative AI system that looks promising, maybe makes a flashy demo or a good prototype, but isn't actually yet in production, right? So you can imagine like an interface like this when we're first rolling it out to our first set of users, right? We might be a little bit, you know, like worried about, you know, are we getting things right 100% of the time, like what's our accuracy metric? How do we know this is actually going to sufficiently handle these different cases, right? Because again, this is like not a trivial process that's going on, right? It's like customer order fulfillment, inventory allocation. There's a ton of business context, right? That's the unique to Onyx Inc. Right? There's not necessarily the training data in the LMS. There's necessarily contain all this Onyx specific information, right? So there's all this Bible knowledge that the LLM has to contend with. And like, how do we know that it's actually doing this correctly? Yeah. And I think the way I think about that is really like if you hired a really smart kid out of college that has zero context about the business process, your business, the institutional knowledge, it takes you time to train it, capture that institutional, institutional knowledge that is probably sitting in someone's head not even in a system. And so this is really like you are training your new employee. The employee just happens to be an AI agent. And it's going to take time and feedback. And well, you give them a little bit more rope and they did something wrong. Let's give them correct them, give them feedback. How do I have the right rails in place? And I think that's what that's where we're honing on is like, these are the rails and evaluations to teach agents and like incorporate that institutional knowledge over time so that you can get value out of them and get it to production, right? Yeah, absolutely. Yeah. And I think feedback is probably the thing to like ironing on here like zoning too, right? Where I actually have an example right here. If we draw our attention to this inventory allocation agent, the sub column here, I can see, you know, we have all of these different customer orders where we're flagging it for user action required. There's 101 so I better get on that. But you can see like we're doing the agent is coming up with these proposals, right? So in this example I'm looking at this is like, oh, we kind of identified an alternative distribution center with available inventory to fill this order. That's a case where, you know, this seems like a success to me. If we actually look at these two orders right next to each other, it's kind of funny. So these are both for 10 CC syringes. You'll notice like the top agent here is saying, okay, 10 CC syringes can be replaced with 20 CC syringes to fulfill at risk orders as per this approved principle, OF012. The principle here is this piece of domain knowledge that we've codified in the ontology that it's retrieving, essentially telling it like, yeah, it's okay to take a 20 CC syringe as a substitution for a 10 CC syringe. And so it's coming up with that identification correctly. If we look right below it, we have a very similar customer order being processed where the agent actually doesn't find that principle. And it's saying additional information is needed regarding any known substitute materials for the 10 CC syringe. So obviously, it's not getting that principle. It's not finding that 20 CC syringes are an appropriate substitution here, which is like, it's funny because they're right next to each other, but yeah, it's obviously getting it wrong in this case. Yeah. Yeah. So the cool thing about it is like, you know, obviously, this is all built in the ontology using these composable different components of AIP, so AIP logic. So if we click into this, we can see the actual agent action log where we can see the different computations that the agent kind of gone through or went through to get this final answer. So if we drill down into this incorrect agent output, actually, if we open this different conditional branch and we drill down into this one LLM call where essentially, we're prompting it to follow the certain process for this inventory allocation step. So we're saying first use this inventory reallocation model to identify any alternative sources of inventory. But if we can't find a replacement distribution center, like in that prior case, where it found it was able to find, I think it was DC 14 as the substitute, it should proceed to solution two where it's checking this documentation that we're kind of semantic searching and retrieving and augmenting the iteration of the LLM. But if solution two doesn't come up with a relevant substitution, we should use the principal object tool to find principles where the status is like approved, et cetera, essentially look at the domain knowledge, the tribal knowledge that we've captured over time and ontology and then use that to find a recommended solution. So in the case where the agent is able to find a substitution, that is where I got it from. These principal objects were kind of delegated to this ontology context. However, if we like keep scrolling here and look at what the LLM actually does, it does kind of do the steps right, it uses the inventory allocation model, it even looks at the documentation, but it kind of just gives up on step three, like it just doesn't do it. And that can happen, right? Like these are non-deterministic systems. It's reasonable that one time it might get it right, one time it might get it wrong. Let's just the name of the game with LLM's. Yeah, so it's very interesting to see this case. Well, the part that's cool is like this level of transparency and audibility for AI plus ontology or AIP and AIP logic and like the ontology. This is all transparent not only to you like debug, but like also to business users to understand like how did I get to the answer? Like when we talk about like the provenance and lineage of data and information to drive trust, if I'm an end user and I can see the suggestion whether I like it or agree with it or not, I have all of that right here. You know, and so the same way I might go interrogate a new employee about why did you do that? What was the data you were looking at? Send me the spreadsheet you had, like you can go see all that transparently right here. Exactly. Yeah, and again, it speaks back to sort of the core value of the ontology where you have your data, your logic, your actions, and you can introspect it all here, right? So these are not just black boxes that are kind of operating and not giving you any opportunity to review them or take human action. And actually, I think the thing that we're that is kind of powerful that we're building up to here is you'll notice next to this final agent comment, we can give it a thumbs up thumbs down. Oh, so rating. And that's kind of maybe the entry point into how we're going to build up eVals for this agent. So first, we're prompting the user to very explicitly give some feedback. So that's one kind of feedback collection paradigm. If I click, in this case, obviously it's getting it wrong as we mentioned, so I'll click thumbs down. So that's kind of this course structuring of the feedback right, we're saying either good or bad. Then we're kind of there's more modalities to that, right? Like one modality. We'll get into other ones, but I think the idea here is just good or bad is a great signal to understand like what happened. Yeah, and it's also yeah, users are very accustomed to seeing stuff like this in different applications. They know the thumbs up thumbs down button generally means like let's let the the dev team know it's going on. So I think going a layer further, if we look at this style, they're just opened. We can see we have the opportunity to provide feedback labels. So this is again a different modality to Chad's point about how are we allowing users to express their like case in the outputs. So if I click on this, we see this drop down menu where I have a few different options. These are ontology defined labels that let me as an operational user kind of express maybe a little bit with a little bit more detail what does negative mean, right? So in this case, I might click did not consult principles, but you could imagine also maybe I didn't even maybe that label didn't even exist, right? Maybe there wasn't anything sufficient. So in that case, we need this escape hatch where users can give unstructured feedback again, a different modality for a user to express their preferences and given evaluation of this output. And they might they might have said should have consulted principles and found OF, but one two, right? And so we're giving this this user a few different opportunities to really capture what should have went right, which what went wrong in particular and what is their sort of taste in this output? Like what do they care about? And in this case, obviously, it's this notion of it should have completed this one step that we defined or it got this output wrong in some way. So I'll go ahead and submit that. And so that's one place where we can sort of provide feedback on these agent outputs. And so we kind of haven't really shown it too much, but maybe it's it's worth getting into how does like where does this feedback go? Because we're kind of talking about, you know, I submitted that email, but like, where is that feedback actually? Or I gave that thumbs up. Where does that feedback actually go? In some, you know, software applications, they will have these thumbs up or thumbs down mechanisms and often users get frustrated with them because it's just like feeding like shouting is void, right? They don't know where their feedback is going. And furthermore, we don't really know if there's a process behind how this feedback has been collected, used, operationalized. But we have that. So let's dive into what we have. I'll click here into feedback workbench. What we're seeing is sort of this more developer facing governance application where we're seeing human feedback coming in for a certain AI agent, right? And so I clicked into the one for the inventory allocation agent that was kind of coming up with that plan and it got that one thing wrong. More we give that thumbs up thumbs down value. And you can see sort of four columns here that kind of take feedback from being unlabeled to being labeled, to being something like a test case or something like a training example for the LLM. And this is kind of representing the state machine of like the feedback loop process, right? So really not just giving it sort of this repository to collect all this feedback, it's actually focusing on the process of the feedback loop, right? And closing it. How do we get to closing the loop as soon as possible? Cool. So you can see that feedback that kind of came in from our submission on the left here where we're saying, oh, it should have looked at the principles. If I click into the specific feedback, I get this sort of holistic view of the feedback, not just sort of the thing I typed in in the form, but also the output that I was commenting on as well as that agent execution log. So I can really dig in and do this error analysis and figure out, yeah, what went wrong? Well, parts of my agent might be working well. What parts aren't working so well. And ultimately, like my goal here is figure out what's going wrong. The user might have typed something in, but maybe it's not the cleanest readable text. They're in their operational workflow as they're giving this feedback, but maybe they're time constrained, right? Maybe they're just jotting something down. So my goal is to figure out, yeah, what's going wrong? Like what were they talking about? So this level of detail, though, is pretty impressive because I think about highly regulated environments, and I need auditors and other people looking at what's going on. Like the fact that anybody or auditor, whoever has security and permissions can come in here and actually understand how did this agent come to these conclusions? I mean, I think this is one of the reasons why we operate in the highly regulated environments is because all of this is transparent and tracked and traced for auditors and everybody else to make sure that the right things are happening, there's not the biases in place and whatnot. Yeah, exactly. Yeah, this is why we win, frankly, it's a core part of our platform. It's built into every way and expressing these use cases, and it's why to your point, we're able to thrive in these regulated environments or places where there's a lot of scrutiny over the operations of these AI systems. So yeah, obviously, there's a sort of, maybe you can imagine there's a manual part of this workflow. I might look at this, maybe I'll come up with a new idea for a label based on a feedback and create that, apply it to the feedback. Also, we're building an AIP, right? So we've actually implemented agents on top of this entire feedback process to help us along the way. So you can see that in this AIP label proposal, that's kind of at the top here. In this case, it's telling me this piece of feedback should be linked to this existing label that we kind of saw in that drop down menu, but I skipped adding just to show up the school part. But essentially, yeah, it's saying like we have this label, you should just label it with this thing. It's automatically doing this categorization process for us. So I might look at this, say that looks good to me, I'll accept it. And then the next step that happens is it brings it into that labeled feedback column, that second step, where we see all of these different sort of understood pieces of feedback, things that we've taken a look at, we've understood what's going on, maybe with the help of that AI agent as well. So now that I've kind of gone through that process, I can think about, what do I want to do with this feedback? Like, I've understood maybe what the user was talking about. I've categorized it, but like, how does that make my AI system better at the end of the day? In this case, I want to talk about eVals in particular. So what I can do now is I can look for these maybe different edge cases that users are finding, or you know, things that are just good tests. And I can click on one of these pieces of feedback and add as a test case, just with a click of a button. What that does is it, again, mutate the ontology, it flags it as a test case in in the back end. And now if I open AIP eVals through this little pop up, we can see those pieces of user feedback are being dynamically pulled into this evaluation suite. So just to orient ourselves on the left, we have those test cases that we flag, there are six of them right now. Those are each kind of that input customer order, along with the users on structured feedback. And then on the right, we can see some other details about this evaluation suite. So it's hooked up to this inventory allocation function. We can see some of the details about that function. And then if you see here in the bottom right, we have one evaluator setup, which is essentially, if I click in, I can show you a little bit more detail as well. This is a built-in evaluator that kind of eVals comes with out of the box. That is actually an LLM as a judge. So what this is doing is it's taking the users unstructured feedback, and then it's checking it against the AI output. And then it's seeing if, you know, the key details or the key points that are expressed by the user feedback are met by the AI output. So under the hood, this is actually just a logic function. Of course, that logic function has its own eVals suite. So again, it kind of goes all the way down the stack. So cool thing about eVals is like, it's just a few clicks of a button to kind of get this evaluation suite to run. I've already mapped some of my inputs. I've picked the model that I want to use as well as the prompt I'm using. And so if I just run this evaluation suite, what eVals will do is it will take this original function, all of my test cases, execute, you know, all of these different test cases, compute these metrics I've defined for accuracy using this element as a judge. And then it gives me a variety of ways to slice and dice the results of my evaluation suite, whether that's, you know, giving me aggregate views of charts and different aggregate metrics like this accuracy evaluator I configured, or sort of a test case view where I'm seeing specifically what failed, what went wrong, digging to the agent trace, do that error analysis. Yeah. So you can see sort of these results being reported back to me here at the bottom. I also have like a few different views where I can go into this, you know, more granular view as I was talking about, see exactly what was being repeated in my. So yeah. But it's not just about accuracy. It's about, you know, time, which would equate to, you know, potential cost as well. Performance for interactive users. Like, it's the whole thing, not just like even accuracy. It's all those combined together is what makes these kind of really powerful. Yeah. Yeah, that's the goal, right? It's like everything that you care about when your building is system like this. So it's all all, all axes of, you know, accuracy, but like it's also a compute cost trade off, right? You have to think about all these things when you're building, you know, these actual production applications, right? Yeah. Cool. So yeah, I think eVals obviously comes with these like really powerful ways of, you know, interacting with your evaluation results. But you can imagine, you know, some of our deployments have a ton of users, ton of users might be flagging stuff. We might have a ton of test cases filling up over time, not just six, right? We might have dozens or hundreds or in some cases, we have like thousands of test cases for certain agents. So when you think about how do I actually take the, not just like, you know, feedback as a test case, but how do I then kind of try to keep closing the loop and get back to like a fix? How do I go from feedback to fix as soon as possible? It's not necessarily going to scale to check every single failing test case. And debug the agents, you know, execution log and then come up with like a prompt improvement or a change in my ontology context or something. So the cool thing about eVals is it also lets you write all of your evaluation suite results to a dataset in Foundry that again, you can build on top of. So I've done that here. You can see this run history dataset over on the side, writing all these results for me. I've built actually that entire dataset into the ontology and then I've built an agentic automation on top of it to do this debugging for me. And I promise it's not, because I'm lazy, it's because of, it's because of scale. Well, it's very cool. Yeah. So yeah, under the hood, there's a pipeline kind of building into this view, but this is a different sort of view for this failing eVals case where we have our failing eVals here on the left, showing the you know, all of those results. But on top of that, I'm running another agent, you know, based in AIP logic where it's doing a failure analysis for me. It's checking that trace, it's looking at my metrics, and then it's, you know, providing maybe a suggestion for, you know, what went wrong, how do we fix it? Here's what you should do. So you can see that here sort of in this middle column. And then ultimately the output of this is sort of a prompt proposal, right? An idea for what can we do to make the agent not fail on this case next time? And so we are talking about that one case that we saw where it wasn't consulting the principles. We see obviously, we're seeing that a failure analysis come up, right? So our failing eVals are actually measuring this piece of user feedback, which is good. But on top of that, it's saying, okay, let's come up with a few ideas that improve on this original prompt, right? And it's expressing what a fix to the prompt might look like in a few different ways. So one is like, oh, maybe we should like prioritize the principles first in how we're, you know, doing this inventory resolution. So you can kind of see that here. Here's like a hacky diff here, I threw together where you can kind of see the changes the prompt. But we also have some other ideas like, oh, just like let's emphasize, you know, the principle citation or like, you know, let's, let's make it a more structured checklist than LM has to follow where it's going through all of these different solutions. So they're whole lines. Yeah, this is pretty cool. It's like, okay, so we gave it feedback. We had the evaluations. These were failing evaluations that were created. Now I'm using another agent, so I actually analyze all the failures to come up with potential root causes and then also potential fixes for this. All so you have agents checking agents. Okay, this is very cool. Yeah, yeah, it's, yeah, agents all the way down. But again, like, I guess one of the meta points here maybe is, you know, we're building these agents into these processes where it's combined with my like human expertise to actually do this, you know, whether it's, you know, in our delivery workbench or we're kind of trying to satisfy these customer orders or as in this debugging video, right, we're presenting it in this interpretable interface where I can actually use my own taste. I can actually go back to eValism look at the failures to gut check that with what the agents are putting. I can take a look at these problems and figure out if they're actually improvements or not. And I can do that sort of gut check. But ultimately, like, it's all of these different, you know, composable parts of the platform that come together in this very helpful way, ultimately, for this feedback loop. It's very neat. Cool. Yeah. So obviously, like, these prompt proposals, like as a gut check sound promising, right, like, we know it's the principal issue that's causing this. A gut check isn't sufficient, right, because we have this production use case, we want to be pretty sure that we're getting it right and maybe empirically sure that this change will actually work. So how do I make this change with confidence and how do I know, like, which prompt is the best? What prompt plays best with, you know, which model, right? You might imagine, you know, different LLMs have different quirks. So some prompts work better with others. I know, like, Claude likes things when it's formatted in XML, but GPT-4O doesn't like it as much or whatever. How do we make this change with confidence? Ultimately, it comes back to AAPE Vals. And what AAPE Vals has, it's kind of a new wish feature. So kind of showing it off maybe for the first time for some people here. Yeah, but this idea of experiments where we have all of these different prompt plus model combinations as kind of hyper parameters. And we want to be sure, you know, which is the most optimal combination, right? So AAPE Vals will optimize these different prompt and model combinations for me. So I'll toggle in my run configuration of eVals this experiment mode. And what it's showing me is like these different experiment parameters that I can configure. So for instance, I might want to know for a model, you know, what's the best between GPT-4.1, Claude Sonnet, 4.1 Nano, perhaps, I could add whatever other ones I want, right? The models are commodity here. So it's like, I can pretty much do whatever I want here. I want to figure out what's actually the best for this agent. What's the best for this use case? So I could add pretty much any of these that I have available here. But at the same time, I want to also tune my prompt. And the agent that was kind of running that failure analysis workflow has come up with a few different prompt options for me. So I have this initial prompt that was kind of used in prod where we identified the failure case selected here as one of the options. But I've also put in those other two kind of proposals to kind of vet them here. And if I just kind of click out of this configuration and click run experiment, what eVals will do is again, do all of this sort of optimization tuning, try all of these different combinations and kind of run a grid search over these options. And then it'll give me a nice way to interpret these results and figure out what actually was the best model in prompt combat. So it's pretty cool. Yeah, a lot of working pieces for sure. Well, yeah, it's not over that because like even in specific agents, you can have different blocks using different LLM. So like the exponential curve of possibilities between just even the prompts and the models you showed there. Now I have an agent that used each block might use different models for different specific tasks. You know, a cheaper model for just basic entity extraction and high end reasoning. I might use O3. And the fact that you get all of those permutations, it really does like you kind of mentioned this, but reminds me of like hyper parameter tuning and optimization is like this is like but in the gen A.I. age. Yeah, exactly. Right. It's yeah, it's interesting because we were talking a little bit earlier about this like more unit testing flow or we're like looking at these particular failing test cases and figuring out what went wrong and why. But now we're talking about like yeah, hyper parameter tuning, like something that you more associate with classical machine learning. Right. So again, eThos is trying to be this marriage of the two, right purpose built for the gen A.I. age essentially. So yeah, obviously this is going to take a little bit. I do have some some pre-computer results based on the same experiment that we can dig into. So I'll filter down here, maybe filter down my table a little bit more because to get the things we care most about here. But yeah, essentially, now I've got this view where I have some results. These are sort of the results across each of these different experiment kind of, you know, model plus prompt combinations. If I want to figure out say like what's the best model for this use case, I could I could group by model drilled down here. Oh, interestingly, this one is no, which is probably the default one. I think default was using cloud or something like that. But we can see it looks like GPT 4.1 was performing best here. But even within that, we want to see, you know, was there a particular prompt that's performing best? And we can see actually, as we saw in that first run of the eVal suite, the initial prompt was performing the worst because it failed everything, which kind of made sense. But actually, we have this, this combination where GPT 4.1 plus this structured checklist prompt option actually increases performance from zero to 50%, which is obviously quite a leap. And again, 50% is not necessarily great. But again, like all of those failing test cases will feed back into that agent debugging flow and give me some more prompt proposals that I could then check out with experiments. And maybe I would narrow into only checking out GPT 4.1 with all of these different prompt options. So yeah, I'm trying to, I'm trying to get really close to like closing the loop, actually improving this workflow via all of these different automations. Again, running based on that user feedback. Yeah. And I think this is like the complexity of the difference in complexity of going from prototype and like an anecdotal use case on a small subset of things to I want hundreds of agents running in parallel to automate an entire business process like we do an insurance and other spaces. Like that's when you need these type of tools to get stuff to production without it, it's just a toy. Yeah, exactly. Yeah, it's, it's, it takes a lot, right? It's not a trivial task, especially like we all know kind of gut intuition, LLMs are supposed to be this really powerful tool in our toolkit that can do these great things like automate entire processes or or do really complex, you know, reasoning processes, etc. But it takes a lot of love and like iterating with users and structuring your process around development to actually get the use case to production, right? It's not as simple as like, you know, writing a writing a programmatic sort of well-defined function spec or something, right? You have to really iterate on it. Yeah, no, it's very cool. Well, thank you for taking the time. Like I think the level of transparency that people have into the system is just so cool. Not only from a development standpoint, but building trust with users, getting adoption similar with auditors and everyone else, being able to see what's going on, that level of transparency, traceability across everything that we do, I think is just core. So thanks for showing that off today. Yeah, absolutely. Yeah, only way you get from prototype Z-DOS. All right. Well, we'll see you later, man. Thanks. All right. Thanks, Chad.