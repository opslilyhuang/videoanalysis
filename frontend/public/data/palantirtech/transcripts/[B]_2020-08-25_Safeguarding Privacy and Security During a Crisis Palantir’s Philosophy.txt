================================================================================
METADATA
================================================================================
Title: Safeguarding Privacy and Security During a Crisis: Palantir’s Philosophy
URL: https://www.youtube.com/watch?v=dlNhgzb2Umo
Published: 2020-08-25
View Count: 3,860
Duration: 2033 seconds
Score: 46.0/100
Rank: B (Basic - 基础背景资料)
Transcript: Available
Category: 
Source: whisper

================================================================================
TRANSCRIPT
================================================================================

Udakast, informing your decisions with intelligence, analysis and insight, brought to you by the team at udaloup.com. Hello, I'm Bob Gourley, the Chief Technology Officer of Udak-LLC, and you're watching the Udakast today with Courtney Bowman, the Director of Privacy and Civil Liberties Engineering for Palantir. Hello, Courtney. Hi, Bob. It's a pleasure to join you. Yes, good talking with you again, Courtney. A little over a year ago in March 2019, you allowed me to interview you for my CTO vision publication. You did a nice piece on artificial intelligence, including ethics, security, and compliance, and your views on that and the approaches that we could all put to use. And I would love to continue the discussion from that, but, you know, over the last year, the world has changed with this pandemic and even more need for an understanding of ethics and compliance and data. And I noticed you've been writing about that. I'd like to talk about some of your recent writing. And I also thought our community would really find your background interesting, your very unique individual. I'd like to talk a little bit about that. I think that sounds exciting. I'm happy to delve into whatever details, biographical or vice-physic work that might be of interest to you. Let's start with the beginning. You pursued a degree in physics and philosophy at Stanford. Is that correct? Actually dual degrees in physics and philosophies and undergraduate at Stanford. Yeah, that's fascinating. Now, what made you want to pursue that? Really, what it came down, it was a longer story, but what it came down to is I couldn't choose between the two. And I decided to do both. One was sort of a both for passions. One had more practical ramifications, namely physics, and it was clear to me and probably to my parents that there's probably a lot more utility that I would get out of a physics degree. And less utility out of a philosophy degree, but I was pretty adamant that I wanted to do both. And in some sense, that maybe wasn't clear to me then, or it wasn't entirely clear to me then that those two pieces of my head would come together later in my life into some meaningful ways of comporting myself towards the world. Yes, really fascinating. In physics, I think we all know is the study of reality of philosophy just as a layman, the study of how we think and why we're here and what is life. But I wanted to ask you, what is philosophy? Is that how you define it? And on some level, I think I probably reduce my understanding of philosophy to its literal meaning in Greek philosophy means love of wisdom. And that's really what what philosophy means to me at a most fundamental level. And the thing that I've taken from studying philosophy over many years and being very passionate about the subject is that is acknowledgement that love of wisdom is really a recognition of humility. It's as much understanding what we know as understanding what we don't know. And constantly that point of humility and calling attention to the limits of understanding the limits of what can be known in the world has helped situate my thinking on a lot of topics in my personal life, but also in my professional life. Right. And speaking of professional life. So now you're at Palantir where, as I mentioned, you're the director of privacy and civil liberties engineering and I thought that title was interesting because that says a lot. Number one, it says you guys take privacy and civil liberties very seriously. And I've known that. And I also put the title engineering in there. And I know you have a big impact on policy and process and other leadership, but you said engineering, which to me implies getting things done and building real solutions. Is that the right implication there? I think that's that's quite right. The so so what I joined Palantir close to 10 years ago, the colleagues that I was working with and the directors and executive team at Palantir really had a vision for integrating privacy as a core function as a reflection of commitments that the company had made in the space of work that it contributes to, which is sensitive data, powerful technology for powerful technology for powerful institutions. And the thing that we were trying to build upon was this, this, this thesis that in order to create technology platforms that enable institutions to do powerful tasks and to enable the use of new data sources. And we recognized risk that attaches to that work and we didn't want the outcomes that were enabling to come at the expense of values that we think are essential to societies that we live in and that and personally value. So, it's so enabling things like privacy and civil liberties were, were challenges that were identified within the information information sciences faces, but we were looking ways of delivering on that commitment and we we did a market assessment to understand the existing landscape of how other institutions deal with privacy. what we found was that most companies at the time really thought about privacy as a sort of compliance oriented, almost a legal formalistic concept where you have a set rubric of requirements that you have to meet and you go through and check the boxes and then you consider your privacy obligations to be fully exhausted. And the insight that we have was that that's really an inadequate approach to thinking about privacy. To really get at the crux of privacy, you have to acknowledge that privacy is much more than legal formalistic qualities. It's a confluence of social sector considerations. It brings in aspects of philosophy, of social sciences, but also core engineering practices. And so we really built up this notion of privacy as an engineering concept that melds together these different concepts and focuses on engineering solutions that anticipate changing environments. And that anticipation component is particularly critical when you think about the fact that the current status of privacy law, privacy regulation, policies, and procedures really only reflect the technology of today and more realistically of 10 years ago. If you want to get ahead of the issues that are going to be on the horizon today and tomorrow and five years from now, you have to use a bit of an imaginative exercise to anticipate what that landscape is going to look like to see around corners and then build against that future vision. So a lot of what we focus on intentionally was to create this engineering practice and exercise of thinking ahead and designing solutions, respect of this broad set of considerations and built into a comprehensive practice. That makes a lot of sense. And everything you're saying resonates with me as a security practitioner. It's a very similar idea as where a lot of folks will say, I'm compliant and therefore I'm secure. I'm following the rules and therefore I'm secure when you're not. And the same kind of thing with privacy, I know the way you explained it just makes it very clear. And there's just no way that any compliance regime can keep up with the continuous innovation underway. It's in startups around the world or large companies around the world are innovating so fast. So I like the way that you're being proactive and taking a longer-term view. Another way to underscore it is the fact that decisions you make today could end up 10, 15, 20 years from now violating somebody's privacy. Maybe the easiest example might be if someone gets access to my DNA and my genome, they're violating the privacy of all of my descendants now and well into the future. So these are complex things that I'm glad you guys are thinking through. Yeah, I think that's very much right. They're complex subjects and as you started to allude to, there are things that go well beyond the scope of a immediate public policy. They kind of implicate a future vision of the world, but there's also, if you scratch the surface, you start to realize there's some underlying social theory and philosophical concepts that need to be reflected in how you approach this question. So that's really getting at the intent behind this concept. And over the years, what we did is build us out as a comprehensive practice within Palantir where today we have about 20 privacy and civil liberties engineers focus on these issues, running the gamut of the policy considerations and the spaces of work that we operate in, but also building privacy enhancing technologies with a view towards those policy and non-technical components. So we're both examining this landscape of risk and opportunity and then building solutions to address that within the product landscape that we as a company focus on. Great. So let's talk a little bit about the crisis. And in any crisis, there's a tendency for good people to want to do good things. And let's just do the right thing and solve the problem. But to frequently, people will do that in a way that might trample on privacy issues. And never good to do that. Whatever the crisis is, whether it's a criminal event or a natural disaster, or now we're all finding out this big pandemic, there was an early rush of people wanted solutions. And people like me would think, whatever it takes, let's just beat this. Let's get all the data together. When really you guys are counseling that you do that in a way that does not violate privacy, there's no reason to violate our ethics or privacy or data security rules when we analyze the data to solve these kind of problems. And that's one thing I wanted to underscore that it's clear to me that we do not have to violate our privacy or push our ethical boundaries as we analyze data. Do you agree? I think that's right. You alluded to some writing that I've done in my role at Palantir is something that we focused on in the early days of the pandemic in mid March. This is before we started to work with any public health agencies or any public or private sector organizations on pandemic response efforts. We saw kind of a rush towards technology to help address the challenges of the current crisis. And we know from experience having built the business in the wake of 9-11 that in the wake of a crisis, the tendency is to throw all available resources at a challenge. And it oftentimes kind of gives short shrift to the normative considerations that usually define the landscape of opportunity and types of solutions that we throw at those problems. We view that as a risky approach and we set out a set of guiding principles and thoughts on how institutions not just Palantir or our customers, but any institutions that are looking to use information and information science and information technology. And we're going to address the pandemic to address the COVID crisis, how they should be thinking about the pandemic and the best use of information to solve the challenges at hand. So we set out as a set of ideas and it turned out to be guiding principles that directed our work in the space. And we're thinking about the end lifecycle of the technology. So the extent to which you may need to collect new information sources and apply those to analysis to drive supply chain understanding or to look at the distribution of health resources across different health service providers. Those information data sources need to be time bound and focused on a limited scope of application so that you're collecting sense of information at the end of the crisis, you have a mechanism to delete it, ensure that it's not retained and used for purposes beyond the immediate crisis. We also call attention to the fact that oftentimes in a moment of crisis or exigent need, there's a tendency to go towards shiny objects, the belief that things like, and this is something that we talked about the last time we, we, you interviewed me, this sort of tendency to look at that novel forms of technology like artificial intelligence and machine learning as a, as a panacea to the world's problems. That may not be the case, maybe the case that there are techniques that can be applied in machine learning landscape that are effective for specific and discrete applications, but the belief that you can just throw data into a black box and have it tell you how to resolve this, an ordinantly complex challenge of solving a pandemic. We view that as Folly and we thought it was more important to start with the immediate challenges. use those challenges to define the landscape of information that's available that needs to be captured. And also the technological components that need to be brought to bear in the analysis of that data. And then use that to gradually guide your approach to new technology or new data, data source acquisition. So really what that gets to is, is a reinforcement of longstanding practices in the privacy landscape. There is a concept of, of fair information practice principles that are well enshrined with an existing privacy regimes and policy and regulations. And in particular, a couple of concepts or a few concepts stand out to us and really help direct my thinking in a lot of the work that Palantir has done, which are concepts of necessity defining what's necessary in a minimum capacity to reform a given function. And then the further concept of proportionality, what's proportionate to the need at hand and what's legitimately justified for a specific outcome. And limiting yourself to only that information for the purposes that are defined at hand. And then as you're building out these processes and using the data in a necessary and proportionate way, ensuring a third concept of transparency to the stakeholders that are involved, making sure that the public is brought along in the journey and understands. The application of data science and why it's intended to perform certain outcomes that have been mandated by, by the public role of institutions that serve us. So as these sort of concepts that you tried to bring to bear and to paint a picture of how institutions in the public and private sectors should be using technology to address the current crisis. And so that Courtney underscores to me the fact that you need to have a vision for what you're going to do, especially in a crisis. And to make that vision real, you need a data strategy. And that to me everything you just said underscores those points. So would you agree you need a good vision and data strategy for what you're going to do. I think you start off with the idea that you just take the data and the data will will provide a pathway. That's another vision of folly. It's it's it's the way to end up with a tangle of of raw data that doesn't really tell you what to do. It doesn't really give you insight into how to solve the challenge and may come with risk. It may and it will certainly create risk that you maybe didn't anticipate and we'll have to deal with later on because there's there's a tendency to rightful tendency to be concerned about the over collection of data, particularly in a society in which we have so many gadgets that are creating data exhaust and trails of information that may not be necessary. But starting with a strategic approach, thinking through what it is that you're trying to do what's necessary to direct yourself towards those aims and then gradually building out towards approximate outcomes where each step justifies the next step in a way that you can demonstrably point to that chain and say, well, I've, I've achieved this objective, the objective comports with the the principles that I laid out initially. Here's why for the furtherance of public health needs it's necessary to go an additional step. All of that trajectory and laying out and curating those details gets you to a point where you can demonstrably and faithfully represent the value of applying information science and information technology to these types of crises situations. So, I think that's fascinating and after laying out this then I mean there's also I guess the way I want to ask it, there's foundational technological things that can be provided that make it easy to do a lot of these things and maybe the simplest example I can think of is, you know, just always having your data encrypted and managing the keys smartly and having a good data fabric like that that has built in these security foundations. So, you know, you know, every engineering thing is like that that you just deliver as a foundational privacy capability. Yeah, I, you'll have to forgive me for the shameless plug but I end up a few colleagues a number of years ago, author of a book called the Architecture Privacy which was really meant to serve as a manual for technologists and entrepreneurs who are building information systems to think through the fundamental components of a privacy protective system. And our argument in that text was that there are a few handful of critical considerations that go into responsible data architectures. One of those things is an access control regime that allows you to selectively reveal and expose information to consistent with this principle of proportionality and use limitation that I talked about a few moments ago. That's kind of a fundamental component. But even before that, there's a sort of threshold question of what are the problems that you're trying to solve and are those problems well suited to information technology. There's certain classes of problems that may not actually lend themselves to technological interventions. And maybe so deeply socially grounded that the real solution comes in the form of a community engagement. But for the class of problems where it does make sense to apply information science and information technology, you want to start with a solid basis for security and the principles that you talked about ensuring encryption in rest and transit. And then the operational controls over how information is used through access limitations. And then building on that, thinking about justifications for further use of the data and other principles of ensuring accountability and oversight. So ensuring that you have a good audit regime in place to make sure that the user interactions with the system and the sensitive data are faithfully represented in logs that can then be interrogated by oversight authorities. So for forensic purposes or for immediate identification of malfeasance from misuse of the data. Those are critical principles. And then the kind of final tranche of the considerations are thinking about the full lifecycle of data management. So it's not just one thing to collect data responsibly, but and to use the data responsibly, but you have to think about the end of the lifecycle. And then the data is when the data no longer serves the purposes for which it was generated. You need to think about how you sunset that data, either archive it or delete it in some strong sense. So that the data isn't sitting out there in some persistent form that then can be exploited for ill gain. So all those principles, I view as being kind of central ideas to to to the appropriate and responsible use of data systems. Those are just sort of scratching the surface of things that need to be in place. You get into more complex considerations when you start to really refine the ideas of what you're doing with the data. Then you can get into more involved questions about different modes of anonymization practices, like differential privacy, which provide you with ways of partially obscuring and providing measurable guarantees of whether you sufficiently de identified data. All those things are additional techniques, but, but the key point is that there's some fundamental things that you can use to secure the use of information and systems and set your enterprise up for solid foundation to move forward from. So that's fascinating and again, you make me think through so many other follow on questions. I want to ask you, but first let me bring it back to this pandemic because just two days ago I was listening in the car to a talk show commentator who was talking about contact tracing. He was railing on about how this is a violation of our civil liberties and he does not want contact tracing. He was afraid that that database is going to be put right next to the Pfizer database and connected to NSA and it's just going to pull all this data together for big government to spy on us. Now that may be mad that someone so smart as that guy was making these delicious arguments, but I know some people believe that and contact tracing could be done in a bad way that reveals too much information about people or it could be done in a smart way where information is protected and you mentioned differential privacy. There could be ways where names are never used, but there's hashes that can get you back to the right data or other smart data strategies. I really believe to get through this pandemic, we need good contact tracing, but we also need to protect privacy and people's rights to keep in their own life private to the greatest extent possible. So I mentioned that because everything you're talking about is so on point and topical and I wanted to say one other thing too as you're talking and I'm thinking, yeah, it's just perfect that you have this philosophical background to deal with these kind of issues. And it reminded me of the old question we used to ask ourselves in the national security domain, do we want to be like Athens or Sparta, do we want the freedom and liberties and free thought of Athens where there's so much creativity and storytelling and peace and harmony and love or do you want to strict regime of Sparta where there is zero freedom, but you are solidly defended. And in a national security context, the right answer is always you don't have to be Athens or Sparta, you want to have freedom and security and now I hear you talk about data and I think it's a similar thing. We want data that can be used and is really functional, it's got to get the job done, but you don't violate security or privacy or civil liberties in order to do that. Yeah, I mean, this is I think you touched Bob on one of the foundational premises of the company when we were founding the company, the idea was that there's sort of this false dichotomy that's that's been established since time in the memorial and you use example of Sparta and Athens, it really is this false choice between security on the one hand or privacy on the other. And granted their instances where you do need to make explicit tradeoffs and there the game is really methodically explaining why you've made specific tradeoffs for specific purposes and as much as possible time bounding the tradeoffs that you've made. But by and large what we've discovered as a company and built the enterprise around is this notion that oftentimes you don't have to trade security for privacy or data utility for privacy in the commercial context oftentimes there's ways of sensibly arriving at or optimizing its both outcomes. And the way that you do that is by smartly engineering systems that enable mission objectives but do so in a way that's that's responsible and methodical. So when you talk about things like contact tracing contact tracing, I should say, we're a company we're not building contact tracing technologies, but there's a prospect that our information systems platforms that are designed to deal with comprehensive sets of data for our customers could be used for some version of contact tracing. Contact tracing application then that raises the question of well, how you do this responsibly and the starting point there is well what's the what's the reasonable use case of contact tracing, where does it make sense. Those threshold questions raise further questions about whether you have a firm testing regime in place and whether you've significantly reduce the volume of cases. So that you you have a sensible load of cases that you can actually contact trace versus dealing with full community spread, where contact tracing doesn't make as much sense. It's just broad public health messaging. But assuming contact tracing makes sense in a given environment. I think there's a lot of sensible proposals that have been put out there for digital versions of contact tracing. It may very well be the case that in certain jurisdictions, the digital exposure notification systems. So the example is that the Apple Google protocol that we've seen a lot of writing about may not make sense and it may make more sense to focus on a manually. A manual version of contact tracing, which is the traditional version of contact tracing. In either case, there are some real concerns because contact tracing involves some very sensitive personal information. And saying you want to make sure that the extent to which that data is being collected by digital devices or by manual auditors. That information is being very selectively collected, controlled and used for specific purposes. And you don't run the risk of the data being repurposed or being inadvertently or inadvertently shared across institutional boundaries. And all of those things are standard practices for public health agencies that are focused on these challenges. And I think I take it as a matter of good faith that most of these institutions really are focused on achieving the right outcomes for for public health initiatives. But the way that they do that and the way that they earn the public trust is by explaining how the systems that they use really are intended the systems and the programs that they're building around these types of regimes are put in place to protect the interests of the constituents to deliver the public health outcomes. And also to ensure that the values and rights and liberties of individuals affected by these systems are being respected at every step along the way. And Courtney, that's also very consistent with the way you wrapped up your medium article. You talked about the fact that history will have its eyes upon us. Us being all of humanity, of course, but also our large corporations, our governments from state, local, federal, other international governments history will have its eye on us. And we have to respond to this pandemic. We have to crush it. But we also have to protect privacy and civil liberties. And I think by saying it the way you did that history will have its eyes on us. It's a good reminder that we got to do the right thing and return to normalcy as soon as we can. And when we do make sure we have not violated privacy or our ethics. I think the other way of framing that is, will we recognize ourselves when we exit the pandemic? Or will we be in a world in which the landscape has so dramatically shifted and the normative expectations around the use of information and information technology has so dramatically shifted towards the the concerns of a surveillance model or panopticon such that it doesn't feel like the society that we were in when we entered the crisis. That's the thing that we constantly have to be vigilant of and guard against. And you know, I, that was really one of the motivations for writing that piece and disseminating it that I wanted to share that as a set of guiding thoughts for institutions focused on these challenges. And not knowing at the time that Palantir was going to become heavily involved in public health response efforts. But when we did get involved, it really was a guiding precept for us as a company. We continually ask our power selves the question of is the work that we're doing really lending itself to outcomes that we're going to feel comfortable with at the end of the pandemic, but five years, 10 years out. When we write the story of Palantir in 2019, 2020, 2021, will it be a story that we as engineers and practitioners in this space feel comfortable having played a small part in contributing to. Great, thanks Courtney and Courtney also want to ask another question about how you keep current now I know you stay current because you're directly engaged not just with your team there at Palantir, but your customers and so you know the issues, but what kind of reading do you do. Yeah, I am so I a lot of my reading is is kind of runs the gamut because of the work that that I'm focused on is fairly heavily involved in. Regulatory circles I have to keep up on developments in data protection landscapes data protection legislation that's being proposed that's that's near near term going to be implemented. That constitutes a big share of my reading, but I also have a few kind of passion projects that I think fits squarely within the space of things that Palantir thinks about as a company is concerned about as a company. So I do a lot of reading on on the prospects of artificial intelligence, but more critically with an eye towards underlying philosophical concepts and I'm really focused on this idea of understanding the limits of technology and I have some specific thoughts on this that that I continue to explore and a threat of philosophy that I study very closely as a student. But it continues to drive a lot of my interest so a lot of the time that I'm that I'm doing leisurely reading it's it's it's reading about philosophy. It's a part of intelligence the ethics of apply data science and then kind of channeling that into to current themes and trends in the legislative and policy landscape. With an eye towards taking these kind of more abstract philosophical concepts and translating them into practical insights that can be reflected in in my day to day job. Wonderful. Great. Courtney, thanks for this. I really appreciate the time. My pleasure, Bob. It's it's always fun chatting with you. Thanks so much for for the interview. Yeah, let's do this again sometime. Great. Thanks, Courtney.