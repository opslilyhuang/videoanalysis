================================================================================
METADATA
================================================================================
Title: Chad & Xander | Lightweight Transforms in Pipeline Builder
URL: https://www.youtube.com/watch?v=DHSYo9s7eRo
Published: 2025-10-28
View Count: 2,472
Duration: 772 seconds
Score: 76.0/100
Rank: A (Active - 高参考价值)
Transcript: Available
Category: 
Source: youtube

================================================================================
TRANSCRIPT
================================================================================

Hi, I'm Chad Wolk, I'm an architect at Palantir. Today I've got Zander who's a tech lead on our pipeline builder team, so thanks for joining me. Thanks very much, I haven't meet Chad. There's a lot of exciting new stuff in pipeline builder, but maybe real quick, just like, what is pipeline builder for the uninitiated? Yes, so pipeline builder is a sort of no-code tool that we use for building pipelines across the platform. This allows users with any level of skill to start bringing in data from across the platform and integrating that into ontology. That's cool. I mean, especially, I've been building pipelines for 15 plus years now. Now I shouldn't get any old, but the idea is actually, like, one of my favorite things is pipeline builder, even though I've built pipelines by hand, the fact that I can actually quickly build out these pipelines in a very robust, scalable way. But then also, if I need to exit to a UDF or some sort of other component, like I have that flexibility of the tool, which is cool. But I think what we're going to talk about today is really some of the new features around lightweight transforms. So maybe we can jump into that and actually just kind of look. So like, we've talked about lightweight transforms before, kind of, you know, be able to run different computing engines, but in the context of pipeline builder, what does that mean here? Right. So exactly. So in pipeline builder, what that means is that we've moved away from a world where we couldn't only execute batch pipelines on Spark. So these are these sort of big distributed workloads, ideal for enormous scale data. We've implemented a whole new backend for pipeline builder, which uses a technology called Data Fusion, which is a sort of open source Apache project, which executes on a single node. And it does so incredibly resource efficient way, and really, really quickly as well. So we've been working on this for sort of the last 18 to 18 to 24 months, nearly. And we're super excited to show you today. Yeah, let's dive in. Yeah, super. So as you can see, we're in pipeline builder, and we're in a traditional Spark pipeline. We're just doing a bit of data integration here. So starting at the left hand side, and we're going to join together a few data sets, and normalize some columns, and do a bit of string cleaning, and then write that to the output. And so just to give you an idea of what some of the scale looks like here, we've got sort of 77 million rows here. We've got 20 million rows, and then 200,000 rows here. And so we're down, we've got two inner joins, we should get a lot of matches. So we're processing in total quite a lot of data here. And just to show you what that looks like on our output data set, we can go ahead and look at the build that I've just run, it's taking about eight minute, nine minutes there to run in total. And to give you an idea of the kind of resources that we're using, we're actually using a medium profile. And so that's using 16 executors. We get good parallelism on this pipeline, so we use all 16 executors. Each of those is at about six gigabytes of RAM, and we've got another six gigabytes of memory on the driver as well. So we're looking north of a hundred gig of memory across the whole Spark module. And then we've got two CPUs for every executor and another CPU for the driver as well. So I want to show you that because as we convert it over, I want to show you that we can not only do quicker with lightweight, but also using fewer resources as well, which is very cool. Awesome. So how do we convert it? Yeah, so let's go ahead and convert it. And this is where we think adoption should be super, super high, because it's literally one click. So we go ahead and we click convert to lightweight pipeline. We get a green tick here. The reason for that is that we can't convert all pipelines yet. We have very high coverage across the suite of stuff that is supported, but not everything is yet. So if anything's not supported here, you'll get an error. We'll show you that in a bit as to what that looks like. You can go and fix things up and then convert afterwards. But let's just click convert. And that's it. We're done. So in the left hand side here, we now see that we're in lightweight. We've converted it and we can now go ahead and deploy. So if we head over to our deploy panel, I'm just going to pick a profile that sort of matches, well, at least we've got eight cores here, and I'm going to pick 60 gig of memory. And so forth. It's worth the number of CPUs and about half as much memory. Right. Right. So let's just apply that. We'll save and then we'll get that deployed. And we'll start a build there as well. OK. And so straight away, that's going to kick off. One thing that we'll notice as well as if we go over to the build report is that because we're using lightweight containers here, the time that it took to initialize that application is about seven seconds, right? So this is incredibly quick compared to spinning up a Spark module, which means you have to start a JVM. We have to download jars. And there's a lot of initialization time even for the Spark context. We're already running. And we're less than 20 seconds into the build. That's very cool. From a developer experience, I think this helps a lot too, because now I can iterate on it, even if I'm with a subset of data, or I'm just trying to run something to get feedback, test out something like so the development cycles also are going to accelerate, not just the build runtime for production and cost reduction. This has actually helped me as a developer too. That's pretty cool. Yeah, absolutely. And you'll also notice when it comes to development experience as well, is that whenever we're using previews, previews are a great feature of Pipeline Builder. But we're actually running preview against lightweight as well. Oh, well. Yeah, that's really snappy. So you're actually seeing the previewed transforms up to that stage of the pipeline. Yeah, absolutely. Absolutely. And that's running against lightweight. So that's super quick. Very cool. Yeah. That is very quick. Which I didn't know that. As you're iterating, this makes things super nice to work with. Yeah. So that's going to run for a while. I guess also, we're going to see how long that runs here in a minute. In the meantime, also, you said it's not everything's supported. Is there a way we can see what does that look like, or situations where we might need to adapt to the convert? Yeah, absolutely. Yeah, absolutely. So let me send you over to a slightly different pipeline here, but very similar operations going on. And here, if I go over to Settings and then Convert, we'll see here that there's something that's not supported here. So we see that Repartition isn't supported here. And I can go ahead and click this toggle on. That's going to show me those areas in line with my pipeline. So I can X out of here. I can scroll down and we see lightweight conversion compatibility checks. And we see Repartition isn't supported. Repartition is a spark operation. It's not necessary in the lightweight world. So actually, in this case, we can just go ahead. I was taken straight to the era. I can go ahead and delete that now, save that. I go back to my convert pipeline. I now get my green tick. I can convert it. Exactly the same process as we saw before. And then we'll be able to go ahead and deploy that. Deploy that on lightweight. Yeah. So here, you mentioned we were using data fusion for the back end of this because it's much more extensible for us. It has similar profiles to maybe like DuckDB as far as memory issues and be able to expand and allows us to integrate deeper into the product. But at the same time, lightweight transforms for code repos. People can use DuckDB, Polars, whatever computing that they want. So people really do have the optionality. I think the cool part here is, though, is I can literally author once, run in multiple different environments or test out even what is the performance profile. Because I think, I've talked to customers that have seen 10 minutes to two minutes. I think we'll see what this one ends up being. But seeing that I'm saving money and time at the same time, I think that is the big difference here. Because, and I think that you can confirm this with me, is because I'm going from this distributed system to single node higher resources on that single node, allows me to actually have more efficient operations instead of having these distributed operations. That actually is the key efficiency gain here. Yeah, absolutely. And as you sort of touched upon there, I think it hugely depends on the kind of things that we're doing and the data skills that we're operating on. DuckDB wrong, there are still cases where Spark is going to win. If we have to go and operate on, you know, terabyte and terabyte of data, and if we're doing these really large shuffle joins, then Spark still wins. But it's in these other cases where, I mean, you can see in the example that we have over here, we're operating on a fair scale of data here, and we're doing so pretty competently in the sort of lightweight world. So it's really about knowing what your pipelines are doing, trying out lightweight. A lot of people have been very pleasantly surprised. We even internally have started processing our own internal logs pipelines using lightweight. You know, these are sort of aggregating those down, is involves reading terabytes of data, but because you're aggregating it down, you never have too much data on a single node, and we're able to do that okay. So I think a lot of people will be pleasantly surprised by what we can do with lightweight. Yeah, I know that's exciting. I think the ease of conversion, be able to test it and go back and forth too. I think, you know, let me go test it out, how's it work? Well, let me go back because it didn't do how I expected it. I think that's a huge plus year for that usability and the developer experience honestly. For me, even if they were right at the same time, but be able to preview things very quickly across my pipeline would be a huge win. Yeah, absolutely. And to that end, if I'm continuing to develop my pipeline and if I'm looking for something that we don't yet support in lightweight, then what we'll see here at the bottom is we see unsupported operations. And so you can see that there's a bunch of sort of geocapabilities that we haven't supported in lightweight yet. But what we're able to do then is just flick straight back to Spark, use the full capabilities that we have in the sort of Spark version of the world and continue to develop without that really encumbering us too much. Right, yeah. Which I think that's the important piece here too, is that one part of a pipeline can be in lightweight, another can be in Spark. So you can actually orchestrate that through Foundry to basically have these pipelines that kind of stitch together that are of different computer engines, depending on the need that I have to help reduce cost, increase performance, all those kind of things. So you have that optionality as far as what that looks like. Yeah, absolutely. All right, so the build's still running here. You're still running. Let's see how we're getting on six minutes. It's estimating nine minutes. So that was what we had on the previous run. So that's well, that's the time to beat. Oh, I see. The estimated is the previous one on Spark, because they're, okay, I see, I'll convert it. Okay, there we go. It's just finished there. So that was six minutes 30 we had, compared to the sort of nine minutes before, and we were running on sort of half the resources there. I have to see, I have to memory, but a fourth the CPU, right, right, a pretty big cost reduction there, but done faster with one button click. Done faster with one button click. What's not to love? Exactly. And I think that this is where we said the mileage will vary depending across the different type of workloads and whatnot. So we see some customers with much higher gains on performance, just depends on the type of pipeline that the idea here is why would you not start with lightweight first? And only convert to Spark when you need to. Yeah, when you need to, absolutely. Well, thanks for taking the time to show me this. This is probably one of the more exciting things for me, just have spent so much time in this space to be able to see the, really, I can author and now start to run on different back lines, on the back ends with just one click. Yeah, we're super excited to get this into the people's hands and seek what kind of results they get. So yeah. Thanks. Awesome. Cheers. to you,