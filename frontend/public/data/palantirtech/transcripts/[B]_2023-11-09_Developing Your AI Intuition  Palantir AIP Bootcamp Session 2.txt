================================================================================
METADATA
================================================================================
Title: Developing Your AI Intuition | Palantir AIP Bootcamp Session
URL: https://www.youtube.com/watch?v=2p0WcDp9LBc
Published: 2023-11-09
View Count: 19,215
Duration: 1119 seconds
Score: 55.0/100
Rank: B (Basic - 基础背景资料)
Transcript: Available
Category: 
Source: whisper

================================================================================
TRANSCRIPT
================================================================================

Hi, my name is Alkit, and I'm going to walk through our developing your AI intuition session, though we run at our AIP boot camps. The goal of this session is to get our customers hands-on keyboard with our product and help them understand the strengths and limitations of working with large language models, and how AIP addresses those limitations. This is not a platform overview, and it's also not an ontology overview. It's a deep dive on how to think about building AI workflows today, although ontology is a key component of that. To start, let's dig into what we mean when we say AI. Today, that typically refers to a machine learning model, and over the last year or so, the most significant change is the advent of the large language model or LLM. These models have many useful and impressive capabilities, and we're going to explore that now. We're going to be using AIP logic to do this. AIP logic is a tool for builders, and it has everything you need to take an LLM from development to production, things like a prompt engineering environment, testing and evaluation for what you build, monitoring, automation, and more. We're going to use it as a playground for experimentation. Let's see what we can do with LLMs. I'm going to add a use LLM block to start interacting with an LLM. Let's start with a simple task. I'm going to ask it to rate the following concert reviews on a scale of one to five. Then I'm going to give it a concert review. Something like, I hate that I only get to experience this concert one time. Unbelievable experience. Great. I'm going to go ahead and run that, and this will open AIP logic debugger. This is going to step through the exact interactions that I have with the LLM, including things like the intermediate chain of thought, and I can see that it's produced an output of five, which is the correct answer that I'm looking for here. Even though this review has a bunch of negative terms like, hate, it clearly is a very positive review. It's something that is gushing about how this was an unbelievably amazing experience. It's able to correctly interpret that and understand the language and get it to that one to five scale and output of five. In fact, just be a couple edits here, something like, I hated the experience completely unbelievable. And that in on it, let's see what we get. In this case, it correctly recognizes that even with just a couple edits, that's a one, and it's a very negative review of this concert overall. So, it's able to get some good results just by providing some instructions. I didn't have to give it thousands of example labeled concert reviews or anything like that. It was able to use that general purpose reasoning capability to get me the answers that I wanted here. All right, so what happens if I put in something random? So instead of concert review, what if I put in banana over here, and then go ahead and run that? In this case, I get some text back. It says, I'm sorry, but banana is not a concert review. Please provide a concert review for meter rate on a scale of one to five. This is fine in the interactive environment that we're in now where I can see the response, but if I want to use this as a building block in a larger system, this kind of random text response isn't going to get me something that I can build on top of. This is where we get to AIP logics first guardrail. It's strong typing system. So by default, LLMs are text in and text out. And that's exactly what we're getting back here. We have the output set to a straight north text. In this case, I can instead ask for some other type back. So maybe I'll ask for an integer back. So I'm expecting a number between one and five. So now when I run this, let's see what we get. Cool. So in this case, I've gotten a zero. Now this is a little bit outside of the parameters of what I expected. I was asking for a number on a scale of one to five, but this is also a case where my prompting is a little bit unclear and this highlights the importance of being explicit about the instructions and how you want it to behave. In this case, it's done a very reasonable thing of picking a review that or a star rating that maybe is something that represents an invalid input or something like that. If we wanted to be more explicit about that, something like for invalid concert reviews, return negative one. And then run that, we can provide a bit more rails for the LLM to get us exactly what we expect. And then later on, maybe we can do something like filter it out or use it for something in a later prompt. Now that we know that we're going to have a integer to work here, let's try training this together and building up some more complicated logic. So I'm going to add a second use LLM block that depends on the output of that first. I'm going to call this star rating. Then I'm going to go ahead and ask our second instruction to do something like based on the star rating and name of a concert, draft a title for a review of it. And I'm going to give it a name. So in this case, the Taylor Swift Eras tour. And I'm going to give it a star rating. So and in this case, I'm going to use that star rating variable that we output previously, which we've now verified to be an integer. Let's call this review title and then switch that to be our output. And I'm also going to switch this over to a real review again. So I'm going to change this back to a positive view. Great. Okay, let's go ahead and run this and see what we get. So those first steps are going to be the same where I now get that five star output. But that's going to get chained into the second block, where now it's taking that variable inserting it in here and then getting us a review title that's drafted from that output. A timeless journey. Taylor Swift's Eras tour delivers a five star experience. And that looks like a five star review. So this is looking great to me. So we've seen an example of some of our basic types, things like integers, dates, floating point numbers, timestamps, etc. But we also have the ability to validate these are objects that are referenced in your ontology. Those noun in suburb your enterprise or even actions that are applied in a sandbox for you to test and the AIP will validate each of those as well. Let's take a look at another limitation. So I'm going to go ahead and clear out these two blocks and add a new one. For this one, I'm going to set up some basic instructions, answer the user's query. And I'm going to ask a question like how long is the Taylor Swift Eras concert? And go ahead and run that. Now this result is interesting. There isn't a specific concert called Taylor Swift Eras. However, Taylor Swift's concerts typically last around two to two and a half hours. If you're referring to a concert that includes songs from different eras of her career, you can expect a similar duration. So this is interesting for two reasons. One, it looks like it's attempted to answer without knowing anything about the eras tour. And two, it's interesting because this entry isn't correct. The Taylor Swift Eras tour is famously long. It's over three hours because she goes through a series of different songs. And this is something the LN doesn't know about. And this is because the LLM that we're using here, GPT-4, is trained on a specific set of data. It's trained on data up to, I believe, 2021. And so anything that happens after that, it just won't know about. It's also true that even if some information happened before 2021 and is in its training data, it might mis-re-call or recall incorrectly that information. I mean, you might have heard about this in the news as hallucinations where you have examples of LLM's coming up with news articles that didn't exist or citing cases that didn't exist. So how do we work around this limitation? So I'm going to bring up the Wikipedia page for the Taylor Swift Eras tour. And I'm just going to grab this first introduction and summary section over here, copy that, and then go back to my logic and provide some extra instructions. So answer the user's query based on this context. And I'm going to paste in all of that Wikipedia article. Great. So using this, I'm going to run this and see what we get. All right, this is interesting. This gets us the correct answer where the Taylor Swift Eras concert spans over three hours. And this because somewhere in this context, it has information about the Taylor Swift Eras tour. So this is perfect. But copying and pasting this information in is fine as a demonstrative example of where an LLM might need context. It's not going to work for a production use case. So what do we actually do for a production use case? That takes us to our next topic of tools. Now, before I get into that, a common question that I get around this point is, okay, great. If it seems to get the right answer, if I give it the right context, can I just put all of my information in here and all of my data in? And the answer is no for a couple of reasons. One is these models have a limitation on the amount of information that fits in their context window. So in this case, we're using GPC4 and we can switch between a couple of different models. But it's somewhere between 8,000 tokens and 100,000 tokens. So as a back of envelope here, 8,000 tokens is about 10 pages worth of content. So what is a token? A token is how a large language model sees its input. It can be a word or a fraction of a word and it's how it learns representations of language. So I'm going to switch over to the page for the tokenizer for the model that we're using GPC4. And you can see that several words map to a single token. So in this case, many is one token words, including the space before it, is another token. But some are broken into parts, things like Indiv and Isabel, the stem in the suffix of the word. This is interesting because language models can then learn representations of the root of the word without needing to independently learn the meanings for all possible combinations. It can kind of learn that when it has this suffix, that might mean that it's being used as an adjective or something like that. Another interesting thing here is how numbers are tokenized. They're grouped together into these groups of three in some cases, but they're not individual digits, which we'll see later when we run into its limitations around a map. Now that we know a little bit about tokenization, let's go back to our logic. And our objective here is then to figure out the right things to fit into that context window, so it can complete the task that we're looking to complete. We can either manually curate what we put in by passing it in as a variable, or we can give the LLM tools so it can get that context on its own. Let's look at an example of using some tools. So I'm going to go ahead and clear this out. Let's look at an example from the demo ontology that we upset up here. So I'm going to go ahead and add a use LLM block and ask to question like how many flights arrive in San Francisco? And I have some flight data loaded up that are ontology over here. So I'm going to go ahead and run that. Okay, so we get a generic answer here. So something about how San Francisco handles a thousand flights per day. But it doesn't give us a precise answer from our ontology. It doesn't know anything about the flight information that we have loaded in. So let's take a look at how we can add a tool to do this better. AAP logic has several tools built in by default, things like a calculator tool to perform math operations, something that LLMs are famously bad at. The ability to query objects in your ontology or apply actions in a sandbox or get the current date. Another thing an LLM doesn't really have any idea of that. It also has this final ontology function tool to be able to create any new tool that you might want to define either in code or using some of the no code interfaces that we have in the platform. Let's go ahead and add a query objects tool. And I'm going to select a flight object for it to query. Great. So now when I run this, let's see what we get. Okay, great. Now I'm getting the right answer. I'm getting those 23,932 flights that I have in this example ontology that I've set up. And we can see exactly how it got there using AIP logic debugger. So we can see it uses that object type tool to query our flight objects. I'm doing an aggregation of counting. And we can see exactly how it does that filter. It filters where arrival city is San Francisco. AIP will then return the right answer which LLM can continue to use in this case 23,932. There might be a series of these tool calls in concert to get to that final operation that you want or to complete the objective that you've described in your instructions to the LLM. Quick aside, an analogy I like is the LLM as an eager intern. It clearly has capability and can do useful work but doesn't have any context on your enterprise and needs the right guidance through prompting and the right tools to be able to complete your task. The more focused a task is, the more likely the intern is to succeed at that task and that's why choosing the right tools and context to give your LLM is so critical. We can also click on to any of these to see the exact details of what the LLM sees as well. So in this case, I can see the results for flights in San Francisco. This is really helpful in debugging when an LLM isn't working in the way that you expect. It might not have access to the right information or be exposed to the right tools that you need for it to complete its task. Now let's take a look at how we can combine together some of those tools. So in this case, we have that result. I'm going to ask another follow-up. What is that number? Cute. And let's see what we get. All right. This is interesting. We got something that looks close, but if I bring up the actual calculation, this starts off right about 137, but then it actually ends up wrong. This is something that LLMs are not very good at. So we want to work around this limitation. We can give the LLM a calculator tool to be able to perform this operation instead. So when I run this with that calculator tool, it's able to use first the object type tool to go in and find the right set of flights, then describe what kind of calculation it wants and then have the calculator return the right results. So this is a good way to chain together different tools that have the LLM reason about which ones it needs to complete a task successfully. The logics that we've seen so far are mostly reading information and able to reason within the tools that we have available. We can also get an LLM to stage potential actions that we might want to apply. So for example, I'm going to switch over to a more fleshed out logic that has a series of different blocks that takes an incoming email and then uses that to filter for distribution center information. And then query several different tools. In this case, things like customer orders, distribution centers, finished goods, and then combine that together into a reallocation proposal. And we can see it actually creating and taking these actions. Now this is applying in a sandbox where it's querying for different tools, creating a proposal which can write back to different source systems, but isn't actually executing that. So you can have a human in the loop for review there. So I can present this inside of a custom application built in our no code application builder or using our APIs and your own applications in the right context so that the operator that is responsible for this decision can see this and then decide whether this is the right action or whether it needs some follow on edits before applied. This is just the beginning. There's a lot of places to go from here, whether it's collecting feedback to incorporate into your prompts and tools or using embeddings to make sure that you're retrieving the best context or hooking this up to automation so your operators can focus on the most important part of the decision making the part that isn't wrote, the part that requires a human in the loop. At our boot camps, this is much more of an improv session where, based on the questions and the types of workflows that our customers want to do, we can adapt the content and take it to where we can build a workflow that same day.