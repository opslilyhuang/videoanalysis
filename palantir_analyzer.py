"""
Palantir YouTube Video Analysis & Transcript Extraction System
Target: Strategic competitive analysis for NotebookLM
"""

import os
import sys
from dotenv import load_dotenv
load_dotenv()
import re
import json
import time
import logging
import requests
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Literal
from dataclasses import dataclass

import yt_dlp
from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound

# é…ç½®æ—¥å¿—
import os
# æ£€æµ‹è¿è¡Œç¯å¢ƒ
LOG_PATH = '/opt/vedioanalysis/transcript_extraction.log' if os.path.exists('/opt/vedioanalysis') else 'transcript_extraction.log'

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_PATH),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class TranscriptResult:
    """å­—å¹•æå–ç»“æœ"""
    text: str
    source: Literal["bibigpt_api", "youtube_api", "yt_dlp", "whisper_api", "whisper_local"]
    language: str
    duration_ms: int
    success: bool = True
    error: Optional[str] = None

class TranscriptExtractionStats:
    """å­—å¹•æå–ç»Ÿè®¡"""
    def __init__(self):
        self.stats = {
            "bibigpt_api": {"success": 0, "failure": 0},
            "youtube_api": {"success": 0, "failure": 0},
            "yt_dlp": {"success": 0, "failure": 0},
            "whisper_api": {"success": 0, "failure": 0},
            "whisper_local": {"success": 0, "failure": 0},
        }

    def record(self, source: str, success: bool):
        if source in self.stats:
            if success:
                self.stats[source]["success"] += 1
            else:
                self.stats[source]["failure"] += 1

    def get_report(self) -> dict:
        total_success = sum(s["success"] for s in self.stats.values())
        total_failure = sum(s["failure"] for s in self.stats.values())
        total = total_success + total_failure

        return {
            **self.stats,
            "total": total,
            "success_rate": f"{total_success/total*100:.1f}%" if total > 0 else "N/A"
        }

# å…¨å±€ç»Ÿè®¡å®ä¾‹
extraction_stats = TranscriptExtractionStats()


class VideoScorer:
    """
    è§†é¢‘ç»¼åˆä»·å€¼è¯„åˆ†ç³»ç»Ÿ
    Score = V_score Ã— 0.4 + T_score Ã— 0.3 + K_score Ã— 0.3
    """

    # æ ¸å¿ƒä¸šåŠ¡å…³é”®è¯
    CORE_KEYWORDS = ['AIPCon', 'Foundrycon', 'Paragon', 'Pipeline', 'AIP', 'Foundry', 'Gotham', 'Apollo']
    # æ¬¡è¦å…³é”®è¯
    SECONDARY_KEYWORDS = ['Demo', 'Tutorial', 'Workshop', 'Case Study', 'Bootcamp', 'How to', 'Guide']

    @staticmethod
    def check_keywords(title: str, description: str) -> Tuple[bool, List[str]]:
        """
        æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯ï¼Œè¿”å› (æ˜¯å¦åŒ¹é…, åŒ¹é…çš„å…³é”®è¯åˆ—è¡¨)
        """
        text = (title + " " + description).lower()
        matched = []

        for keyword in VideoScorer.CORE_KEYWORDS + VideoScorer.SECONDARY_KEYWORDS:
            if keyword.lower() in text:
                matched.append(keyword)

        return len(matched) > 0, matched

    @staticmethod
    def get_view_score(view_count: int) -> float:
        """
        æ’­æ”¾é‡å¾—åˆ† - æƒé‡ 40%
        ä½¿ç”¨åŒºé—´æ˜ å°„é¿å…é•¿å°¾æ•ˆåº”
        """
        if view_count >= 100000:
            return 100.0
        elif view_count >= 50000:
            return 80.0
        elif view_count >= 20000:
            return 60.0
        else:
            return 40.0

    @staticmethod
    def get_time_score(publish_date: str) -> float:
        """
        æ—¶æ•ˆæ€§å¾—åˆ† - æƒé‡ 30%
        è¶Šæ–°åˆ†æ•°è¶Šé«˜
        """
        try:
            # ISO 8601 format: 2024-03-15T10:00:00Z
            date_obj = datetime.fromisoformat(publish_date.replace('Z', '+00:00'))
            year = date_obj.year
            month = date_obj.month

            if year >= 2025:
                return 100.0
            elif year == 2024:
                return 80.0 if month >= 7 else 60.0
            else:
                return 30.0
        except:
            return 40.0  # é»˜è®¤åˆ†æ•°

    @staticmethod
    def get_keyword_score(title: str, description: str) -> float:
        """
        æ ¸å¿ƒä¸šåŠ¡ç›¸å…³åº¦ - æƒé‡ 30%
        """
        text = (title + " " + description).lower()

        # æ£€æŸ¥æ ¸å¿ƒå…³é”®è¯
        for keyword in VideoScorer.CORE_KEYWORDS:
            if keyword.lower() in text:
                return 100.0

        # æ£€æŸ¥æ¬¡è¦å…³é”®è¯
        for keyword in VideoScorer.SECONDARY_KEYWORDS:
            if keyword.lower() in text:
                return 70.0

        return 40.0

    @classmethod
    def calculate_score(cls, video_data: Dict) -> float:
        """
        è®¡ç®—ç»¼åˆå¾—åˆ† (0-100)
        """
        v_score = cls.get_view_score(video_data.get('view_count', 0))
        t_score = cls.get_time_score(video_data.get('upload_date', ''))
        k_score = cls.get_keyword_score(
            video_data.get('title', ''),
            video_data.get('description', '')
        )

        total_score = (v_score * 0.4) + (t_score * 0.3) + (k_score * 0.3)

        return round(total_score, 2)


class RankAssigner:
    """ç­‰çº§åˆ¤å®šç³»ç»Ÿ"""

    @staticmethod
    def get_rank(score: float) -> str:
        """
        Sçº§ (Strategic): Score >= 85
        Açº§ (Active): 70 <= Score < 85
        Bçº§ (Basic): Score < 70
        """
        if score >= 85:
            return 'S'
        elif score >= 70:
            return 'A'
        else:
            return 'B'


class PalantirVideoAnalyzer:
    """
    Palantir YouTube é¢‘é“åˆ†æå™¨
    ä½¿ç”¨ yt-dlp è·å–å…ƒæ•°æ®ï¼Œyoutube-transcript-api è·å–å­—å¹•
    """

    PALANTIR_CHANNEL_ID = "UCfwPå„è‡ªçš„tqhWG5jX9SP"  # å°†åœ¨è¿è¡Œæ—¶æ›¿æ¢

    def __init__(self, output_dir: str = "output", data_dir: Optional[str] = None, channel_id: str = "palantirtech"):
        """
        output_dir: ä¸»è¾“å‡ºç›®å½•ï¼ˆtranscriptsã€video_index.csv ç­‰ï¼‰
        data_dir: å¯é€‰ï¼Œå‰ç«¯æ•°æ®ç›®å½•ã€‚è®¾ä¸º "frontend/public/data" æ—¶ï¼Œä¼šåŒæ­¥è¾“å‡º master_index.csvã€config.jsonã€status.json åˆ° data_dir/channel_id/
        channel_id: é¢‘é“ IDï¼Œç”¨äº data_dir ä¸‹çš„å­ç›®å½•å
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.data_dir = Path(data_dir) / channel_id if data_dir else None
        self.channel_id = channel_id
        if self.data_dir:
            self.data_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºå­ç›®å½•
        self.transcripts_dir = self.output_dir / "transcripts"
        self.transcripts_dir.mkdir(exist_ok=True)

        # yt-dlp é…ç½®
        self.ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'extract_flat': False,
        }

    def fetch_channel_videos(self, channel_url: str, limit: Optional[int] = None) -> List[Dict]:
        """
        è·å–é¢‘é“è§†é¢‘åˆ—è¡¨
        """
        print(f"ğŸ“¡ æ­£åœ¨è·å–é¢‘é“è§†é¢‘åˆ—è¡¨: {channel_url}")

        # ä½¿ç”¨ /videos URL æ ¼å¼ä»¥è·å–å®Œæ•´çš„è§†é¢‘åˆ—è¡¨
        if '/@' in channel_url or '/channel/' in channel_url or '/c/' in channel_url:
            # å¦‚æœæ˜¯é¢‘é“ä¸»é¡µï¼Œæ·»åŠ  /videos
            if not channel_url.endswith('/videos'):
                channel_url = channel_url.rstrip('/') + '/videos'

        ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'extract_flat': 'in_playlist',  # å¿«é€Ÿè·å–åˆ—è¡¨
            'playlistend': limit,  # é™åˆ¶æ•°é‡
        }

        videos = []

        try:
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                result = ydl.extract_info(channel_url, download=False)

                if 'entries' in result:
                    for entry in result['entries']:
                        if entry is None:
                            continue

                        video_id = entry.get('id')
                        if not video_id or not video_id.startswith(('video:', '')):
                            # è·³è¿‡éè§†é¢‘æ¡ç›®
                            if video_id and video_id.startswith('UC'):
                                continue

                        # ç¡®ä¿ video_id æ˜¯çº¯å‡€çš„ ID
                        if video_id and video_id.startswith('video:'):
                            video_id = video_id.replace('video:', '')

                        if not video_id or len(video_id) != 11:
                            continue

                        videos.append({
                            'video_id': video_id,
                            'url': f"https://www.youtube.com/watch?v={video_id}",
                            'title': entry.get('title', 'Unknown'),
                            'duration': entry.get('duration'),
                        })

                        if limit and len(videos) >= limit:
                            break

        except Exception as e:
            print(f"âŒ è·å–é¢‘é“è§†é¢‘å¤±è´¥: {e}")
            return []

        print(f"âœ… æ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘")
        return videos

    def fetch_video_details(self, video_url: str) -> Optional[Dict]:
        """
        è·å–å•ä¸ªè§†é¢‘çš„è¯¦ç»†ä¿¡æ¯
        """
        ydl_opts = self.ydl_opts.copy()

        try:
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(video_url, download=False)

                return {
                    'video_id': info.get('id'),
                    'url': info.get('webpage_url'),
                    'title': info.get('title'),
                    'description': info.get('description', ''),
                    'view_count': info.get('view_count', 0),
                    'upload_date': info.get('upload_date', ''),  # YYYYMMDD
                    'duration': info.get('duration'),
                    'channel': info.get('channel'),
                    'thumbnail': info.get('thumbnail'),
                }
        except Exception as e:
            print(f"âŒ è·å–è§†é¢‘è¯¦æƒ…å¤±è´¥ {video_url}: {e}")
            return None

    def get_transcript(self, video_id: str, timeout_ms: int = 10000) -> Optional[str]:
        """
        è·å–è§†é¢‘å­—å¹•ï¼ˆæ™ºèƒ½ fallback ç­–ç•¥ï¼‰

        Args:
            video_id: YouTube è§†é¢‘ ID
            timeout_ms: å‰å‡ ç§æ–¹æ³•çš„è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰

        Returns:
            å­—å¹•æ–‡æœ¬ï¼Œå¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥åˆ™è¿”å› None

        Fallback ç­–ç•¥ï¼ˆæ ¹æ®è¿è¡Œç¯å¢ƒè‡ªåŠ¨è°ƒæ•´ï¼‰ï¼š

        ã€æœ¬åœ°ç¯å¢ƒ - å¯è®¿é—® YouTubeã€‘
            1. YouTube Transcript API (10ç§’è¶…æ—¶ï¼Œå…è´¹)
            2. yt-dlp ä¸‹è½½å­—å¹• (10ç§’è¶…æ—¶ï¼Œå…è´¹)
            3. BibiGPT API (10ç§’è¶…æ—¶ï¼Œéœ€è¦ API token)
            4. Whisper è½¬å½• (3-5åˆ†é’Ÿï¼Œæœ¬åœ° faster-whisperï¼Œå…è´¹)

        ã€äº‘ç¯å¢ƒ - æ— æ³•è®¿é—® YouTubeã€‘
            1. BibiGPT API (10ç§’è¶…æ—¶ï¼Œéœ€è¦ API tokenï¼Œæ— éœ€è®¿é—® YouTube)
            2. Whisper è½¬å½• (3-5åˆ†é’Ÿï¼Œæœ¬åœ° faster-whisperï¼Œå…è´¹)

        æ³¨æ„ï¼šYouTube API å’Œ yt-dlp åœ¨äº‘ç¯å¢ƒä¼šè¢«è·³è¿‡ï¼ˆå› ä¸ºç½‘ç»œä¸å¯è¾¾ï¼‰
        """
        start_time = time.time()
        logger.info(f"å¼€å§‹æå–å­—å¹•: {video_id}")

        # æ£€æµ‹è¿è¡Œç¯å¢ƒ
        can_access_youtube = self._can_access_youtube()

        if can_access_youtube:
            # æœ¬åœ°ç¯å¢ƒï¼šä¼˜å…ˆä½¿ç”¨å…è´¹çš„ YouTube API/yt-dlpï¼Œä¸ä½¿ç”¨ BibiGPT
            logger.info(f"[æœ¬åœ°ç¯å¢ƒ] ä½¿ç”¨ç­–ç•¥: YouTube API â†’ yt-dlp â†’ OpenAI Whisper â†’ æœ¬åœ° Whisper")

            # æ–¹æ³•1: YouTube Transcript APIï¼ˆå…è´¹ï¼‰
            result = self._try_youtube_transcript_api(video_id, timeout_ms)
            if result and result.success:
                logger.info(f"âœ“ YouTube API æˆåŠŸ: {video_id} (è€—æ—¶: {result.duration_ms}ms)")
                extraction_stats.record("youtube_api", True)
                return result.text

            # æ–¹æ³•2: yt-dlp ä¸‹è½½å­—å¹•ï¼ˆå…è´¹ï¼‰
            result = self._try_yt_dlp_subtitles(video_id, timeout_ms)
            if result and result.success:
                logger.info(f"âœ“ yt-dlp æˆåŠŸ: {video_id} (è€—æ—¶: {result.duration_ms}ms)")
                extraction_stats.record("yt_dlp", True)
                return result.text

            # æ–¹æ³•3: OpenAI Whisper APIï¼ˆéœ€è¦ OPENAI_API_KEYï¼Œä¸æ¶ˆè€— BibiGPTï¼‰
            logger.warning(f"å‰ä¸¤ç§æ–¹æ³•å¤±è´¥ï¼Œå°è¯• OpenAI Whisper: {video_id}")
            result = self._try_whisper_transcription(video_id)
            if result and result.success:
                logger.info(f"âœ“ Whisper æˆåŠŸ: {video_id} (è€—æ—¶: {result.duration_ms}ms)")
                extraction_stats.record(result.source, True)
                return result.text

        else:
            # äº‘ç¯å¢ƒï¼šè·³è¿‡ YouTube API/yt-dlpï¼ˆç½‘ç»œä¸å¯è¾¾ï¼‰ï¼Œç›´æ¥ä½¿ç”¨ BibiGPT
            logger.info(f"[äº‘ç¯å¢ƒ] ä½¿ç”¨ç­–ç•¥: BibiGPT â†’ Whisperï¼ˆè·³è¿‡ YouTube API/yt-dlpï¼‰")

            # æ–¹æ³•1: BibiGPT APIï¼ˆéœ€è¦ tokenï¼Œæ— éœ€è®¿é—® YouTubeï¼Œä½¿ç”¨æ›´é•¿è¶…æ—¶ï¼‰
            # å¯¹äºé•¿è§†é¢‘ï¼ˆ5åˆ†é’Ÿä»¥ä¸Šï¼‰ï¼ŒBibiGPTå¤„ç†å¯èƒ½éœ€è¦1-2åˆ†é’Ÿ
            result = self._try_bibigpt_api(video_id, 90000)  # 90ç§’è¶…æ—¶ï¼ˆåŸæ¥æ˜¯30ç§’ï¼‰
            if result and result.success:
                logger.info(f"âœ“ BibiGPT API æˆåŠŸ: {video_id} (è€—æ—¶: {result.duration_ms}ms)")
                extraction_stats.record("bibigpt_api", True)
                return result.text

        # æœ€åå…œåº•ï¼šWhisper è½¬å½•ï¼ˆæœ¬åœ°ï¼Œå…è´¹ï¼Œä½†è€—æ—¶è¾ƒé•¿ï¼‰
        logger.warning(f"å‰è¿°æ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ° Whisper: {video_id}")
        result = self._try_whisper_transcription(video_id)
        if result and result.success:
            logger.info(f"âœ“ Whisper æˆåŠŸ: {video_id} (è€—æ—¶: {result.duration_ms}ms)")
            extraction_stats.record(result.source, True)
            return result.text

        # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
        total_time = int((time.time() - start_time) * 1000)
        logger.error(f"âœ— æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥: {video_id} (æ€»è€—æ—¶: {total_time}ms)")
        extraction_stats.record("whisper_local", False)
        return None

    def _try_bibigpt_api(self, video_id: str, timeout_ms: int) -> Optional[TranscriptResult]:
        """
        æ–¹æ³•0: ä½¿ç”¨ BibiGPT API è·å–å­—å¹•ï¼ˆä¿®æ­£ç‰ˆï¼‰
        ä½¿ç”¨ /api/v1/summarizeWithConfig ç«¯ç‚¹ + includeDetail å‚æ•°
        """
        start_time = time.time()
        api_token = os.getenv("BIBIGPT_API_TOKEN")

        if not api_token:
            logger.info("BibiGPT API token æœªé…ç½®ï¼Œè·³è¿‡")
            return None

        try:
            url = "https://api.bibigpt.co/api/v1/summarizeWithConfig"
            youtube_url = f"https://www.youtube.com/watch?v={video_id}"

            # å°è¯•å¤šç§è¯­è¨€ï¼ˆä¼˜å…ˆä¸­æ–‡ï¼Œå…¶æ¬¡è‹±æ–‡ï¼‰
            languages = ["zh-CN", "en-US", "auto"]

            for lang in languages:
                try:
                    payload = {
                        "url": youtube_url,
                        "promptConfig": {
                            "outputLanguage": lang,
                            "isRefresh": True  # å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
                        },
                        "includeDetail": True  # å…³é”®ï¼šåŒ…å«è¯¦ç»†å­—å¹•æ•°æ®
                    }

                    response = requests.post(
                        url,
                        json=payload,
                        headers={
                            "Authorization": f"Bearer {api_token}",
                            "Content-Type": "application/json"
                        },
                        timeout=timeout_ms / 1000
                    )

                    if response.status_code == 200:
                        data = response.json()

                        # æ£€æŸ¥è¿”å›çš„æ•°æ®ç»“æ„
                        if "detail" in data:
                            detail = data["detail"]

                            # ä» subtitlesArray æå–å­—å¹•ï¼ˆBibiGPT è¿”å›çš„å­—å¹•æ ¼å¼ï¼‰
                            if "subtitlesArray" in detail:
                                subtitles = detail["subtitlesArray"]

                                if isinstance(subtitles, list) and len(subtitles) > 0:
                                    # åˆå¹¶æ‰€æœ‰å­—å¹•ç‰‡æ®µ
                                    subtitle_text = "\n".join([
                                        seg.get("text", "")
                                        for seg in subtitles
                                        if seg.get("text", "").strip()
                                    ])

                                    if subtitle_text.strip() and len(subtitle_text) > 10:  # é™ä½é•¿åº¦é™åˆ¶
                                        duration_ms = int((time.time() - start_time) * 1000)
                                        logger.info(f"BibiGPT è¿”å› {len(subtitles)} ä¸ªå­—å¹•ç‰‡æ®µï¼Œæ–‡æœ¬é•¿åº¦: {len(subtitle_text)}")
                                        return TranscriptResult(
                                            text=subtitle_text.strip(),
                                            source="bibigpt_api",
                                            language=lang,
                                            duration_ms=duration_ms
                                        )
                                    else:
                                        logger.warning(f"BibiGPT è¿”å›å­—å¹•æ–‡æœ¬è¿‡çŸ­: {len(subtitle_text)} å­—ç¬¦")
                            else:
                                logger.warning(f"BibiGPT å“åº”ä¸­ç¼ºå°‘ subtitlesArray å­—æ®µã€‚detail keys: {list(detail.keys())[:10]}")
                        else:
                            logger.warning(f"BibiGPT å“åº”ä¸­ç¼ºå°‘ detail å­—æ®µã€‚data keys: {list(data.keys())}")

                except requests.exceptions.Timeout:
                    logger.warning(f"BibiGPT API è¶…æ—¶ (lang={lang}): {video_id}")
                    continue
                except Exception as e:
                    logger.warning(f"BibiGPT API å¤±è´¥ (lang={lang}): {video_id} - {str(e)}")
                    continue

            extraction_stats.record("bibigpt_api", False)
            return None

        except Exception as e:
            logger.warning(f"BibiGPT API å¼‚å¸¸: {video_id} - {str(e)}")
            extraction_stats.record("bibigpt_api", False)
            return None

    def _clean_bibigpt_subtitle(self, subtitle_data: str) -> str:
        """æ¸…ç† BibiGPT è¿”å›çš„å­—å¹•æ•°æ®"""
        if isinstance(subtitle_data, str):
            # å¦‚æœæ˜¯çº¯æ–‡æœ¬ï¼Œç›´æ¥è¿”å›
            return subtitle_data.strip()
        elif isinstance(subtitle_data, dict):
            # å¦‚æœæ˜¯å­—å…¸ï¼Œæå–æ–‡æœ¬å†…å®¹
            if "text" in subtitle_data:
                return subtitle_data["text"].strip()
            elif "segments" in subtitle_data:
                # åˆå¹¶åˆ†æ®µå­—å¹•
                return "\n".join([seg.get("text", "") for seg in subtitle_data["segments"]])
        elif isinstance(subtitle_data, list):
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œåˆå¹¶æ–‡æœ¬
            return "\n".join([item.get("text", str(item)) for item in subtitle_data])

        return str(subtitle_data).strip()

    def _try_youtube_transcript_api(self, video_id: str, timeout_ms: int) -> Optional[TranscriptResult]:
        """æ–¹æ³•1: ä½¿ç”¨ YouTube Transcript API"""
        start_time = time.time()
        try:
            import signal

            def timeout_handler(signum, frame):
                raise TimeoutError("YouTube API è¶…æ—¶")

            # è®¾ç½®è¶…æ—¶
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.setitimer(signal.ITIMER_REAL, timeout_ms / 1000)

            try:
                api = YouTubeTranscriptApi()
                transcript_list = api.list(video_id)

                # è¯­è¨€ä¼˜å…ˆçº§
                languages = ['zh-Hans', 'zh', 'en', 'en-US', 'en-GB']

                for lang in languages:
                    try:
                        transcript = transcript_list.find_transcript([lang])
                        text = self._format_transcript(transcript.fetch())
                        duration_ms = int((time.time() - start_time) * 1000)
                        return TranscriptResult(
                            text=text,
                            source="youtube_api",
                            language=lang,
                            duration_ms=duration_ms
                        )
                    except NoTranscriptFound:
                        try:
                            transcript = transcript_list.find_generated_transcript([lang])
                            text = self._format_transcript(transcript.fetch())
                            duration_ms = int((time.time() - start_time) * 1000)
                            return TranscriptResult(
                                text=text,
                                source="youtube_api",
                                language=lang,
                                duration_ms=duration_ms
                            )
                        except:
                            continue

                # å°è¯•ä»»ä½•å¯ç”¨å­—å¹•
                for transcript_obj in transcript_list:
                    text = self._format_transcript(transcript_obj.fetch())
                    duration_ms = int((time.time() - start_time) * 1000)
                    return TranscriptResult(
                        text=text,
                        source="youtube_api",
                        language="unknown",
                        duration_ms=duration_ms
                    )

            except TimeoutError:
                logger.warning(f"YouTube API è¶…æ—¶: {video_id}")
                extraction_stats.record("youtube_api", False)
                return None
            finally:
                signal.alarm(0)  # å–æ¶ˆè¶…æ—¶

        except TranscriptsDisabled:
            logger.info(f"å­—å¹•è¢«ç¦ç”¨: {video_id}")
            extraction_stats.record("youtube_api", False)
            return None
        except Exception as e:
            logger.warning(f"YouTube API å¤±è´¥: {video_id} - {str(e)}")
            extraction_stats.record("youtube_api", False)
            return None

    def _try_yt_dlp_subtitles(self, video_id: str, timeout_ms: int) -> Optional[TranscriptResult]:
        """æ–¹æ³•2: ä½¿ç”¨ yt-dlp ä¸‹è½½å­—å¹•"""
        start_time = time.time()
        temp_dir = self.output_dir / '.temp_subs'
        temp_dir.mkdir(exist_ok=True)

        try:
            ydl_opts = {
                'quiet': True,
                'no_warnings': True,
                'writesubtitles': True,
                'writeautomaticsub': True,
                'subtitleslangs': ['zh-Hans', 'zh', 'en', 'en-US', 'en-GB'],
                'skip_download': True,
                'subtitlesformat': 'vtt',
                'outtmpl': f'{temp_dir}/%(id)s.%(ext)s',
            }

            # ä½¿ç”¨ subprocess æ·»åŠ è¶…æ—¶æ§åˆ¶
            import subprocess
            cmd = [
                sys.executable, '-m', 'yt_dlp',
                '--write-sub', '--write-auto-sub',
                '--sub-langs', 'zh-Hans,zh,en,en-US,en-GB',
                '--skip-download',
                '--sub-format', 'vtt',
                '-o', f'{temp_dir}/%(id)s.%(ext)s',
                f'https://www.youtube.com/watch?v={video_id}'
            ]

            try:
                result = subprocess.run(
                    cmd,
                    timeout=timeout_ms / 1000,
                    capture_output=True,
                    text=True
                )
            except subprocess.TimeoutExpired:
                logger.warning(f"yt-dlp è¶…æ—¶: {video_id}")
                extraction_stats.record("yt_dlp", False)
                return None

            # æŸ¥æ‰¾ä¸‹è½½çš„å­—å¹•æ–‡ä»¶
            for lang in ['zh-Hans', 'zh', 'en', 'en-US', 'en-GB']:
                for ext in ['.vtt', '.en.vtt', '.zh.vtt']:
                    sub_file = temp_dir / f'{video_id}.{ext}'
                    if sub_file.exists():
                        text = self._parse_vtt(sub_file)
                        if text:
                            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                            sub_file.unlink(missing_ok=True)
                            duration_ms = int((time.time() - start_time) * 1000)
                            return TranscriptResult(
                                text=text,
                                source="yt_dlp",
                                language=lang,
                                duration_ms=duration_ms
                            )

            # å°è¯•ä»»ä½• .vtt æ–‡ä»¶
            for vtt_file in temp_dir.glob(f'{video_id}*.vtt'):
                text = self._parse_vtt(vtt_file)
                if text:
                    vtt_file.unlink(missing_ok=True)
                    duration_ms = int((time.time() - start_time) * 1000)
                    return TranscriptResult(
                        text=text,
                        source="yt_dlp",
                        language="unknown",
                        duration_ms=duration_ms
                    )

            extraction_stats.record("yt_dlp", False)
            return None

        except Exception as e:
            logger.warning(f"yt-dlp å¤±è´¥: {video_id} - {str(e)}")
            extraction_stats.record("yt_dlp", False)
            return None

    def _try_whisper_transcription(self, video_id: str) -> Optional[TranscriptResult]:
        """æ–¹æ³•3: ä½¿ç”¨ Whisper è½¬å½•"""
        start_time = time.time()
        transcript = self._transcribe_with_whisper(video_id)

        if transcript:
            duration_ms = int((time.time() - start_time) * 1000)
            # åˆ¤æ–­ä½¿ç”¨çš„æ˜¯ API è¿˜æ˜¯æœ¬åœ°
            api_key = os.getenv("OPENAI_API_KEY")
            source = "whisper_api" if api_key else "whisper_local"

            return TranscriptResult(
                text=transcript,
                source=source,
                language="en",  # Whisper é»˜è®¤è‹±æ–‡
                duration_ms=duration_ms
            )

        extraction_stats.record("whisper_local", False)
        return None

    def get_extraction_stats(self) -> dict:
        """è·å–å­—å¹•æå–ç»Ÿè®¡æŠ¥å‘Š"""
        return extraction_stats.get_report()

    def _transcribe_with_whisper(self, video_id: str) -> Optional[str]:
        """
        è½¬å½•æ— å­—å¹•è§†é¢‘ã€‚ä¼˜å…ˆä½¿ç”¨ OpenAI Whisper APIï¼ˆéœ€ OPENAI_API_KEYï¼‰ï¼Œ
        å¦åˆ™ä½¿ç”¨æœ¬åœ° faster-whisperï¼ˆéœ€ ffmpegï¼Œpip install faster-whisperï¼‰ã€‚
        """
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            return self._transcribe_whisper_api(video_id, api_key)
        return self._transcribe_whisper_local(video_id)

    def _transcribe_whisper_api(self, video_id: str, api_key: str) -> Optional[str]:
        """ä½¿ç”¨ OpenAI Whisper API è½¬å½•"""
        temp_audio_dir = self.output_dir / ".temp_audio"
        temp_audio_dir.mkdir(exist_ok=True)
        base_path = temp_audio_dir / video_id
        try:
            import shutil
            has_ffmpeg = bool(shutil.which("ffmpeg"))
            ydl_opts = {
                "quiet": True,
                "no_warnings": True,
                "format": "bestaudio[ext=m4a]/bestaudio[ext=webm]/bestaudio[ext=mp3]/bestaudio",
            }
            if has_ffmpeg:
                ydl_opts["postprocessors"] = [{"key": "FFmpegExtractAudio", "preferredcodec": "m4a"}]
                ydl_opts["outtmpl"] = str(base_path)
            else:
                ydl_opts["outtmpl"] = str(base_path) + ".%(ext)s"

            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                ydl.download([f"https://www.youtube.com/watch?v={video_id}"])

            audio_path = None
            for ext in (".m4a", ".webm", ".mp3", ".mp4", ".mpeg", ".mpga", ".wav"):
                p = base_path.with_suffix(ext)
                if p.exists():
                    audio_path = p
                    break
            if not audio_path:
                for f in temp_audio_dir.glob(f"{video_id}.*"):
                    if f.suffix.lower() in (".m4a", ".webm", ".mp3", ".mp4", ".mpeg", ".mpga", ".wav"):
                        audio_path = f
                        break
            if not audio_path or not audio_path.exists():
                return None

            from openai import OpenAI
            client = OpenAI(api_key=api_key)
            with open(audio_path, "rb") as f:
                resp = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=f,
                    language="en",
                )
            text = (resp.text or "").strip()
            audio_path.unlink(missing_ok=True)
            return text if text else None
        except Exception as e:
            err_msg = str(e).lower()
            if "invalid_api_key" in err_msg or "authentication" in err_msg:
                print(f"   âš ï¸ OPENAI_API_KEY æ— æ•ˆï¼Œè¯·æ£€æŸ¥ .env é…ç½®")
            else:
                print(f"   âš ï¸ Whisper API è½¬å½•å¤±è´¥: {e}")
            return None

    _whisper_model_cache = None
    _can_access_youtube_cache = None

    def _can_access_youtube(self, timeout_ms: int = 5000) -> bool:
        """
        æ£€æµ‹å½“å‰ç¯å¢ƒæ˜¯å¦èƒ½è®¿é—® YouTube
        æœ¬åœ°ç¯å¢ƒï¼šå¯ä»¥è®¿é—® â†’ ä½¿ç”¨ YouTube API/yt-dlp/OpenAI Whisper
        äº‘ç¯å¢ƒï¼šæ— æ³•è®¿é—® â†’ ä½¿ç”¨ BibiGPT/Whisper
        """
        if PalantirVideoAnalyzer._can_access_youtube_cache is not None:
            return PalantirVideoAnalyzer._can_access_youtube_cache

        try:
            import socket
            import urllib.request
            import ssl

            # æ–¹æ³•1: å°è¯• HTTP è¯·æ±‚ï¼ˆå¿½ç•¥ SSL è¯ä¹¦éªŒè¯ï¼‰
            try:
                ssl_context = ssl._create_unverified_context()
                response = urllib.request.urlopen('https://www.youtube.com', timeout=timeout_ms/1000, context=ssl_context)
                if response.status == 200:
                    PalantirVideoAnalyzer._can_access_youtube_cache = True
                    logger.info("âœ“ æ£€æµ‹åˆ°å¯ä»¥è®¿é—® YouTubeï¼ˆæœ¬åœ°ç¯å¢ƒï¼‰ï¼Œä¼˜å…ˆä½¿ç”¨ YouTube API/yt-dlp")
                    return True
            except:
                pass

            # æ–¹æ³•2: socket è¿æ¥ï¼ˆå¤‡ç”¨ï¼‰
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(timeout_ms / 1000)
            result = sock.connect_ex(("www.youtube.com", 443))
            sock.close()

            # æŸäº›ç¯å¢ƒä¸‹ socket è¿”å›é0ä½†å®é™…å¯è®¿é—®
            # å¦‚æœ socket å¤±è´¥ï¼Œå‡è®¾æ— æ³•è®¿é—®
            can_access = (result == 0)
            PalantirVideoAnalyzer._can_access_youtube_cache = can_access

            if can_access:
                logger.info("âœ“ æ£€æµ‹åˆ°å¯ä»¥è®¿é—® YouTubeï¼ˆæœ¬åœ°ç¯å¢ƒï¼‰ï¼Œä¼˜å…ˆä½¿ç”¨ YouTube API/yt-dlp")
            else:
                logger.info("âœ— æ£€æµ‹åˆ°æ— æ³•è®¿é—® YouTubeï¼ˆäº‘ç¯å¢ƒï¼‰ï¼Œä½¿ç”¨ BibiGPT â†’ Whisper")

            return can_access
        except Exception as e:
            logger.warning(f"ç½‘ç»œæ£€æµ‹å¤±è´¥: {e}ï¼Œå‡è®¾æ— æ³•è®¿é—® YouTube")
            PalantirVideoAnalyzer._can_access_youtube_cache = False
            return False

    def _transcribe_whisper_local(self, video_id: str) -> Optional[str]:
        """ä½¿ç”¨æœ¬åœ° faster-whisper è½¬å½•ï¼ˆéœ€ ffmpeg ç”¨äºéŸ³é¢‘æå–ï¼‰"""
        import shutil
        try:
            from faster_whisper import WhisperModel
        except ImportError:
            print("   âš ï¸ è¯·å®‰è£…: pip install faster-whisper")
            return None

        has_ffmpeg = bool(shutil.which("ffmpeg"))
        temp_audio_dir = self.output_dir / ".temp_audio"
        temp_audio_dir.mkdir(exist_ok=True)
        base_path = temp_audio_dir / video_id
        try:
            ydl_opts = {
                "quiet": True,
                "no_warnings": True,
                "format": "bestaudio[ext=m4a]/bestaudio[ext=webm]/bestaudio[ext=mp3]/bestaudio",
            }
            if has_ffmpeg:
                ydl_opts["postprocessors"] = [{"key": "FFmpegExtractAudio", "preferredcodec": "m4a"}]
                ydl_opts["outtmpl"] = str(base_path)
            else:
                ydl_opts["outtmpl"] = str(base_path) + ".%(ext)s"
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                ydl.download([f"https://www.youtube.com/watch?v={video_id}"])

            audio_path = None
            for ext in (".m4a", ".webm", ".mp3", ".mp4", ".opus"):
                p = base_path.with_suffix(ext)
                if p.exists():
                    audio_path = p
                    break
            if not audio_path:
                for f in temp_audio_dir.glob(f"{video_id}.*"):
                    if f.suffix.lower() in (".m4a", ".mp3", ".webm", ".opus", ".mp4"):
                        audio_path = f
                        break

            if not audio_path or not audio_path.exists():
                return None

            if PalantirVideoAnalyzer._whisper_model_cache is None:
                # ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆfaster-whisper é»˜è®¤ï¼‰
                PalantirVideoAnalyzer._whisper_model_cache = WhisperModel("base", device="cpu", compute_type="int8")
            model = PalantirVideoAnalyzer._whisper_model_cache

            segments, info = model.transcribe(str(audio_path), language="en", beam_size=5)
            text = "".join([seg.text for seg in segments]).strip()
            audio_path.unlink(missing_ok=True)
            return text if text else None
        except Exception as e:
            if "ffmpeg" in str(e).lower() or "ffprobe" in str(e).lower():
                print(f"   âš ï¸ éœ€è¦ ffmpeg")
            else:
                print(f"   âš ï¸ æœ¬åœ° Whisper è½¬å½•å¤±è´¥: {e}")
            return None

    def _parse_vtt(self, vtt_file: Path) -> Optional[str]:
        """
        è§£æ VTT å­—å¹•æ–‡ä»¶ä¸ºçº¯æ–‡æœ¬
        """
        try:
            import webvtt
            captions = webvtt.read(str(vtt_file))
            full_text = []
            for caption in captions:
                text = caption.text.strip()
                # ç§»é™¤ VTT æ ‡ç­¾
                text = re.sub(r'<[^>]+>', '', text)
                if text:
                    full_text.append(text)

            return ' '.join(full_text)
        except ImportError:
            # å¦‚æœæ²¡æœ‰ webvtt åº“ï¼Œä½¿ç”¨ç®€å•è§£æ
            with open(vtt_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            full_text = []
            for line in lines:
                line = line.strip()
                # è·³è¿‡ VTT å¤´éƒ¨ã€æ—¶é—´æˆ³å’Œç©ºè¡Œ
                if (line.startswith('WEBVTT') or
                    '-->' in line or
                    line.startswith('NOTE') or
                    not line):
                    continue

                # ç§»é™¤ HTML æ ‡ç­¾
                line = re.sub(r'<[^>]+>', '', line)
                if line:
                    full_text.append(line)

            return ' '.join(full_text)
        except Exception:
            return None

    def _format_transcript(self, transcript_data) -> str:
        """
        æ ¼å¼åŒ–å­—å¹•ä¸ºè¿ç»­æ–‡æœ¬
        æ”¯æŒ dict å’Œ FetchedTranscriptSnippet å¯¹è±¡
        """
        full_text = []
        for entry in transcript_data:
            # å…¼å®¹ä¸¤ç§æ ¼å¼ï¼šdict å’Œå¯¹è±¡
            if isinstance(entry, dict):
                text = entry.get('text', '').strip()
            else:
                # FetchedTranscriptSnippet å¯¹è±¡
                text = str(entry.text).strip()

            if text:
                full_text.append(text)

        return ' '.join(full_text)

    def sanitize_filename(self, filename: str) -> str:
        """
        æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦
        """
        # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
        filename = re.sub(r'[<>:"/\\|?*]', '', filename)
        # é™åˆ¶é•¿åº¦
        if len(filename) > 200:
            filename = filename[:200]

        return filename.strip()

    def save_transcript(self, video_data: Dict, transcript: Optional[str], rank: str, score: float, has_transcript: bool = True, category: str = "", source: str = ""):
        """
        ä¿å­˜å­—å¹•ä¸ºç‹¬ç«‹æ–‡æœ¬æ–‡ä»¶
        æ–‡ä»¶åæ ¼å¼: [ç­‰çº§]_[æ—¥æœŸ]_[æ ‡é¢˜].txt
        å¦‚æœæ²¡æœ‰å­—å¹•ï¼Œä»ç„¶ä¿å­˜å…ƒæ•°æ®å¹¶æ ‡è®° NO TRANSCRIPT AVAILABLE
        é‡è¦ï¼šè‹¥ç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ä¸”å«å®é™…å­—å¹•ï¼ˆå¦‚ Whisper è½¬æ¢ç»“æœï¼‰ï¼Œåˆ™ä¸è¦†ç›–ï¼Œé¿å…åç»­æµç¨‹ç ´åå·²æœ‰æ•°æ®
        """
        # è§£ææ—¥æœŸ
        upload_date = video_data.get('upload_date', '')
        try:
            if upload_date:
                date_obj = datetime.strptime(upload_date, '%Y%m%d')
                formatted_date = date_obj.strftime('%Y-%m-%d')
            else:
                formatted_date = 'Unknown'
        except:
            formatted_date = 'Unknown'

        # æ„å»ºæ–‡ä»¶å
        title = self.sanitize_filename(video_data.get('title', 'Unknown'))
        filename = f"[{rank}]_{formatted_date}_{title}.txt"
        filepath = self.transcripts_dir / filename

        # è‹¥ç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ä¸”å«å®é™…å­—å¹•ï¼ˆå¦‚ Whisper ç»“æœï¼‰ï¼Œä¸è¦†ç›–ï¼Œé¿å…åç»­æµç¨‹ç ´å
        if not has_transcript and filepath.exists():
            try:
                existing = filepath.read_text(encoding='utf-8')
                if 'NO TRANSCRIPT AVAILABLE' not in existing:
                    print(f"  è·³è¿‡è¦†ç›–ï¼ˆä¿ç•™å·²æœ‰å­—å¹•ï¼‰: {filename}")
                    return
            except Exception:
                pass

        # æ„å»ºå…ƒæ•°æ®å¤´
        transcript_status = "Available" if has_transcript else "NOT AVAILABLE"
        metadata_header = f"""{'=' * 80}
METADATA
{'=' * 80}
Title: {video_data.get('title')}
URL: {video_data.get('url')}
Published: {formatted_date}
View Count: {(video_data.get('view_count') or 0):,}
Duration: {video_data.get('duration') or 0} seconds
Score: {score}/100
Rank: {rank} ({self._get_rank_description(rank)})
Transcript: {transcript_status}
Category: {category or ''}
Source: {source or 'youtube'}

{'=' * 80}
TRANSCRIPT
{'=' * 80}

"""

        # å†™å…¥æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(metadata_header)
            if transcript:
                f.write(transcript)
            else:
                f.write("NO TRANSCRIPT AVAILABLE\n\n")
                f.write("This video does not have captions or subtitles available on YouTube.\n")
                f.write("To obtain the transcript, you would need to:\n")
                f.write("1. Use a speech-to-text service (e.g., OpenAI Whisper) on the downloaded audio\n")
                f.write("2. Manually transcribe the content\n")
                f.write(f"\nVideo URL: {video_data.get('url')}\n")

        status_icon = "ğŸ’¾" if has_transcript else "ğŸ“‹"
        print(f"{status_icon} å·²ä¿å­˜: {filename} ({'æœ‰å­—å¹•' if has_transcript else 'æ— å­—å¹•'})")

    def _get_rank_description(self, rank: str) -> str:
        """è·å–ç­‰çº§æè¿°"""
        descriptions = {
            'S': 'Strategic - æˆ˜ç•¥çº§ç ”ç©¶å¯¹è±¡',
            'A': 'Active - é«˜å‚è€ƒä»·å€¼',
            'B': 'Basic - åŸºç¡€èƒŒæ™¯èµ„æ–™'
        }
        return descriptions.get(rank, 'Unknown')

    def filter_channel(self, channel_url: str, limit: Optional[int] = None) -> List[Dict]:
        """
        é˜¶æ®µ 1ï¼šå…ˆç­›é€‰å‡ºç¬¦åˆä¸‰ç±»æ¡ä»¶çš„è§†é¢‘ï¼Œä¸è·å–å­—å¹•
        è¾“å‡º: filtered_candidates.json, filtered_candidates.csv
        è¿”å›: ç¬¦åˆæ¡ä»¶çš„è§†é¢‘åˆ—è¡¨
        """
        print("\n" + "=" * 80)
        print("ğŸ“‹ é˜¶æ®µ 1ï¼šç­›é€‰ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
        print("=" * 80 + "\n")
        print("ç­›é€‰æ¡ä»¶ï¼ˆç¬¦åˆä»»ä¸€å³ä¿ç•™ï¼‰:")
        print("   1ï¸âƒ£  æ’­æ”¾é‡ >= 20,000")
        print("   2ï¸âƒ£  å‘å¸ƒæ—¥æœŸ >= 2024å¹´1æœˆ")
        print("   3ï¸âƒ£  åŒ…å«å…³é”®è¯ (AIPCon, Paragon, Demo, Tutorialç­‰)")
        print()

        videos = self.fetch_channel_videos(channel_url, limit)
        if not videos:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•è§†é¢‘")
            return []

        candidates: List[Dict] = []
        cat_20k = set()   # video_id é›†åˆï¼Œç”¨äºå»é‡ç»Ÿè®¡
        cat_2024 = set()
        cat_keywords_set = set()

        self._write_status(0, len(videos), "filtering", phase="filter")

        for idx, video in enumerate(videos, 1):
            video_id = video['video_id']
            print(f"[{idx}/{len(videos)}] æ£€æŸ¥: {video['title'][:50]}...")
            self._write_status(idx, len(videos), "filtering", phase="filter")

            details = self.fetch_video_details(video['url'])
            if not details:
                continue

            view_count = details.get('view_count', 0)
            upload_date = details.get('upload_date', '')
            title = details.get('title', '')
            description = details.get('description', '')

            meets_20k = view_count >= 20000
            meets_2024 = False
            if upload_date and len(upload_date) >= 6:
                try:
                    meets_2024 = int(upload_date[:6]) >= 202401
                except ValueError:
                    pass
            has_kw, matched = VideoScorer.check_keywords(title, description)
            meets_keywords = has_kw

            if meets_20k:
                cat_20k.add(video_id)
            if meets_2024:
                cat_2024.add(video_id)
            if meets_keywords:
                cat_keywords_set.add(video_id)

            if meets_20k or meets_2024 or meets_keywords:
                score = VideoScorer.calculate_score(details)
                rank = RankAssigner.get_rank(score)
                candidates.append({
                    **details,
                    'score': score,
                    'rank': rank,
                    'matched_criteria': {
                        '20k_views': meets_20k,
                        'since_2024': meets_2024,
                        'keywords': meets_keywords,
                        'keywords_list': matched,
                    }
                })
                print(f"   âœ“ ç¬¦åˆæ¡ä»¶ [{'20K' if meets_20k else ''}{'2024' if meets_2024 else ''}{'å…³é”®è¯' if meets_keywords else ''}]")

        # ä¿å­˜ç­›é€‰ç»“æœ
        filter_file = self.output_dir / "filtered_candidates.json"
        with open(filter_file, 'w', encoding='utf-8') as f:
            json.dump(candidates, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ å·²ä¿å­˜ç­›é€‰ç»“æœ: {filter_file}")

        # CSV
        import csv
        csv_file = self.output_dir / "filtered_candidates.csv"
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            w = csv.writer(f)
            w.writerow(['Rank', 'Score', 'Title', 'Date', 'Views', '20K+', '2024+', 'Keywords', 'URL'])
            for c in candidates:
                w.writerow([
                    c['rank'],
                    c['score'],
                    c.get('title', '')[:80],
                    c.get('upload_date', ''),
                    c.get('view_count', 0),
                    'âœ“' if c['matched_criteria']['20k_views'] else '',
                    'âœ“' if c['matched_criteria']['since_2024'] else '',
                    ','.join(c['matched_criteria']['keywords_list'][:3]) if c['matched_criteria']['keywords_list'] else '',
                    c.get('url', ''),
                ])
        print(f"ğŸ’¾ å·²ä¿å­˜ CSV: {csv_file}")

        # ç»Ÿè®¡æŠ¥å‘Š
        print("\n" + "=" * 80)
        print("ğŸ“Š ç­›é€‰å®Œæˆ - ä¸‰ç±»æ•°æ®ç»Ÿè®¡")
        print("=" * 80)
        print(f"\né¢‘é“æ€»è§†é¢‘æ•°: {len(videos)}")
        print(f"\nä¸‰ç±»åˆ†åˆ«ç¬¦åˆçš„æ•°é‡ï¼ˆå¯é‡å¤ï¼‰:")
        print(f"  ğŸ“º æ’­æ”¾é‡ 20K+: {len(cat_20k)}")
        print(f"  ğŸ“… 2024å¹´1æœˆ+: {len(cat_2024)}")
        print(f"  ğŸ”‘ å…³é”®è¯åŒ¹é…: {len(cat_keywords_set)}")
        print(f"\nå»é‡åéœ€å¤„ç†çš„è§†é¢‘æ€»æ•°: {len(candidates)}")
        print(f"\nä¸‹ä¸€æ­¥: è¿è¡Œ process_filtered_candidates() è·å–å­—å¹•")
        print("=" * 80 + "\n")

        self._write_status(len(videos), len(videos), "idle", phase="filter")

        # ä¿å­˜ä¸‰ç±»æ•°é‡åˆ° filter_summary.json ä¾›å‰ç«¯å±•ç¤º
        summary = {
            "channel_total": len(videos),
            "cat_20k_views": len(cat_20k),
            "cat_2024_01": len(cat_2024),
            "cat_keywords": len(cat_keywords_set),
            "filtered_total": len(candidates),
            "updated_at": datetime.now().isoformat() + "Z",
        }
        self._write_filter_summary(summary)

        return candidates

    def process_filtered_candidates(self, limit: Optional[int] = None):
        """
        é˜¶æ®µ 2ï¼šä»…å¯¹å·²ç­›é€‰å‡ºçš„è§†é¢‘è·å–å­—å¹•å¹¶ä¿å­˜
        ä» filtered_candidates.json è¯»å–ï¼Œä¸é‡æ–°è¯·æ±‚é¢‘é“
        """
        filter_file = self.output_dir / "filtered_candidates.json"
        if not filter_file.exists():
            print("âŒ æœªæ‰¾åˆ° filtered_candidates.jsonï¼Œè¯·å…ˆè¿è¡Œ filter_channel()")
            return

        with open(filter_file, 'r', encoding='utf-8') as f:
            candidates = json.load(f)

        to_process = candidates[:limit] if limit else candidates
        total = len(to_process)
        print(f"\nğŸ“ é˜¶æ®µ 2ï¼šå¤„ç† {total} ä¸ªå·²ç­›é€‰è§†é¢‘ï¼ˆè·å–å­—å¹•ï¼‰\n")

        stats = {'S': 0, 'A': 0, 'B': 0, 'with_transcript': 0, 'without_transcript': 0, 'failed': 0}
        failed_list: List[Dict] = []

        self._write_status(0, total, "processing", failed_count=0)

        for idx, details in enumerate(to_process, 1):
            video_id = details.get('video_id')
            title = details.get('title', 'Unknown')
            print(f"[{idx}/{total}] {title[:50]}...")

            self._write_status(idx, total, "processing", failed_count=len(failed_list))

            try:
                transcript = self.get_transcript(video_id)
                has_transcript = transcript is not None
                rank = details.get('rank', 'B')
                score = details.get('score', 0)

                self.save_transcript(details, transcript, rank, score, has_transcript)

                stats[rank] += 1
                if has_transcript:
                    stats['with_transcript'] += 1
                else:
                    stats['without_transcript'] += 1
            except Exception as e:
                stats['failed'] += 1
                failed_list.append({
                    **details,
                    'error': str(e),
                    'rank': details.get('rank', 'B'),
                    'score': details.get('score', 0),
                })
                print(f"   âŒ å¤±è´¥: {e}")

        self._write_status(total, total, "idle", failed_count=stats['failed'])
        self._write_failed_videos(failed_list)
        self._write_frontend_output(stats)

        print("\n" + "=" * 80)
        print("ğŸ“Š å­—å¹•å¤„ç†å®Œæˆ")
        print("=" * 80)
        print(f"  å·²å¤„ç†: {total} | æœ‰å­—å¹•: {stats['with_transcript']} | æ— å­—å¹•: {stats['without_transcript']} | å¤±è´¥: {stats['failed']}")
        if failed_list:
            print(f"\n  å¤±è´¥é¡¹å·²ä¿å­˜ï¼Œé‡æ–°å¤„ç†è¯·è¿è¡Œ: python palantir_analyzer.py --retry-failed")
        print("=" * 80 + "\n")

    def _write_filter_summary(self, summary: Dict):
        """å†™å…¥ filter_summary.json ä¾›å‰ç«¯å±•ç¤ºä¸‰ç±»æ•°é‡"""
        paths = [self.output_dir / "filter_summary.json"]
        if self.data_dir:
            paths.append(self.data_dir / "filter_summary.json")
        for path in paths:
            try:
                with open(path, "w", encoding="utf-8") as f:
                    json.dump(summary, f, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"âš ï¸  å†™å…¥ filter_summary.json å¤±è´¥: {e}")

    def _update_filter_summary_cat_others(self):
        """ä» master_index ç»Ÿè®¡ cat_others å¹¶æ›´æ–° filter_summary"""
        import csv
        path = self.data_dir / "master_index.csv" if self.data_dir else None
        if not path or not path.exists():
            return
        others_count = 0
        try:
            with open(path, encoding="utf-8") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if (row.get("Category") or row.get("category") or "").strip() == "å…¶ä»–":
                        others_count += 1
        except Exception:
            return
        summary_path = self.data_dir / "filter_summary.json"
        if not summary_path.exists():
            return
        try:
            with open(summary_path, encoding="utf-8") as f:
                summary = json.load(f)
        except Exception:
            return
        summary["cat_others"] = others_count
        summary["updated_at"] = datetime.now().isoformat() + "Z"
        self._write_filter_summary(summary)

    def _write_failed_videos(self, failed_list: List[Dict]):
        """ä¿å­˜å¤±è´¥è§†é¢‘åˆ—è¡¨"""
        failed_file = self.output_dir / "failed_videos.json"
        try:
            with open(failed_file, "w", encoding="utf-8") as f:
                json.dump(failed_list, f, ensure_ascii=False, indent=2)
            if failed_list:
                print(f"ğŸ’¾ å¤±è´¥é¡¹å·²ä¿å­˜: {failed_file}")
        except Exception as e:
            print(f"âš ï¸  å†™å…¥ failed_videos.json å¤±è´¥: {e}")
        if self.data_dir:
            dst = self.data_dir / "failed_videos.json"
            try:
                with open(dst, "w", encoding="utf-8") as f:
                    json.dump(failed_list, f, ensure_ascii=False, indent=2)
            except Exception:
                pass

    def _write_status(self, current: int, total: int, status: str, phase: str = "process", failed_count: int = 0):
        """å†™å…¥ status.json ä¾›å‰ç«¯è¿›åº¦æ¡è¯»å–ï¼Œå®Œæˆæ—¶è¿½åŠ åˆ° status_history"""
        if not self.data_dir:
            return
        path = self.data_dir / "status.json"
        data = {
            "current": current,
            "total": total,
            "status": status,
            "phase": phase,
            "failed_count": failed_count,
            "channel": self.channel_id,
            "updatedAt": datetime.now().isoformat() + "Z",
        }
        try:
            with open(path, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2)
            if status == "idle" and total > 0:
                hist_path = self.data_dir / "status_history.json"
                hist = []
                if hist_path.exists():
                    with open(hist_path, encoding="utf-8") as f:
                        hist = json.load(f)
                phase_label = "ç­›é€‰" if phase == "filter" else ("Whisperè½¬æ¢" if phase == "whisper" else "å¤„ç†")
                hist.append({
                    **data,
                    "completedAt": datetime.now().isoformat() + "Z",
                    "phase_label": phase_label,
                })
                if len(hist) > 100:
                    hist = hist[-100:]
                with open(hist_path, "w", encoding="utf-8") as f:
                    json.dump(hist, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  å†™å…¥ status.json å¤±è´¥: {e}")

    def _write_config_json(self):
        """å†™å…¥ config.json ä¾›å‰ç«¯é…ç½®é¢æ¿è¯»å–"""
        if not self.data_dir:
            return
        path = self.data_dir / "config.json"
        data = {
            "keywords": {k: 5 for k in VideoScorer.CORE_KEYWORDS}
            | {k: 2 for k in VideoScorer.SECONDARY_KEYWORDS},
            "thresholds": {"S": 85, "A": 70, "B": 0},
            "weights": {"view": 0.4, "time": 0.3, "keyword": 0.3},
        }
        try:
            with open(path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  å†™å…¥ config.json å¤±è´¥: {e}")

    def _write_frontend_output(self, stats: Dict):
        """ç”Ÿæˆå‰ç«¯æ‰€éœ€çš„ master_index.csvã€config.jsonã€transcript_index.jsonï¼Œå¹¶åŒæ­¥ transcripts"""
        if not self.data_dir:
            return
        self._write_config_json()
        self._generate_master_index_csv()
        self._generate_transcript_index()
        # åŒæ­¥ transcripts åˆ° data_dir ä¾›å‰ç«¯è®¿é—®
        dst = self.data_dir / "transcripts"
        dst.mkdir(exist_ok=True)
        import shutil
        for f in self.transcripts_dir.glob("*.txt"):
            try:
                shutil.copy2(f, dst / f.name)
            except Exception as e:
                print(f"âš ï¸  å¤åˆ¶ {f.name} å¤±è´¥: {e}")

    def _generate_master_index_csv(self):
        """ç”Ÿæˆ master_index.csvï¼ˆå« Transcript åˆ—ï¼‰ä¾›å‰ç«¯è¯»å–"""
        import csv

        path = self.data_dir / "master_index.csv"
        rows = []

        for txt_file in self.transcripts_dir.glob("*.txt"):
            try:
                with open(txt_file, "r", encoding="utf-8") as f:
                    lines = f.readlines()[:25]
                rank, score, title, date, views, url, transcript, category, transcript_source = "Unknown", 0, "Unknown", "Unknown", 0, "", "æ— ", "", "youtube"
                for line in lines:
                    line = line.strip()
                    if line.startswith("Rank:"):
                        rank = line.split(":")[1].strip()[0]
                    elif line.startswith("Score:"):
                        score = line.split(":")[1].strip().replace("/100", "").strip()
                    elif line.startswith("Title:"):
                        title = line.split(":", 1)[1].strip()
                    elif line.startswith("Published:"):
                        date = line.split(":", 1)[1].strip()
                    elif line.startswith("View Count:"):
                        views = line.split(":")[1].strip().replace(",", "")
                    elif line.startswith("URL:"):
                        url = line.split(":", 1)[1].strip()
                    elif "Transcript:" in line:
                        transcript = "æœ‰" if "Available" in line else "æ— "
                    elif line.startswith("Category:"):
                        category = line.split(":", 1)[1].strip()
                    elif line.startswith("Source:"):
                        transcript_source = line.split(":", 1)[1].strip().lower()
                rows.append([rank, score, title, date, views, transcript, category, url, transcript_source])
            except Exception as e:
                print(f"âš ï¸  è§£æ {txt_file} å¤±è´¥: {e}")

        try:
            with open(path, "w", newline="", encoding="utf-8") as f:
                w = csv.writer(f)
                w.writerow(["Rank", "Score", "Title", "Date", "Views", "Transcript", "Category", "URL", "TranscriptSource"])
                w.writerows(rows)
            print(f"ğŸ“Š å·²ç”Ÿæˆå‰ç«¯ç´¢å¼•: {path}")
        except Exception as e:
            print(f"âš ï¸  å†™å…¥ master_index.csv å¤±è´¥: {e}")

    def _generate_transcript_index(self):
        """ç”Ÿæˆ transcript_index.json: video_id -> filenameï¼Œä¾› API æŒ‰ video_id æŸ¥æ‰¾ã€‚
        åŒä¸€è§†é¢‘è‹¥æœ‰å¤šä¸ª transcript æ–‡ä»¶ï¼ˆå¦‚å…ˆæ— å­—å¹•å ä½ã€å Whisper è½¬æ¢ï¼‰ï¼Œä¼˜å…ˆé€‰ç”¨æœ‰å®é™…å†…å®¹çš„æ–‡ä»¶ã€‚"""
        import re
        path = self.data_dir / "transcript_index.json"
        index = {}  # video_id -> (filename, has_content)
        for txt_file in sorted(self.transcripts_dir.glob("*.txt")):
            try:
                with open(txt_file, "r", encoding="utf-8") as f:
                    raw = f.read()
                for line in raw.split("\n"):
                    if line.strip().startswith("URL:"):
                        url = line.split(":", 1)[1].strip()
                        m = re.search(r"[?&]v=([a-zA-Z0-9_-]{11})", url)
                        if m:
                            vid = m.group(1)
                            has_content = "NO TRANSCRIPT AVAILABLE" not in raw
                            if vid not in index or (has_content and not index[vid][1]):
                                index[vid] = (txt_file.name, has_content)
                        break
            except Exception:
                pass
        out = {vid: fn for vid, (fn, _) in index.items()}
        try:
            with open(path, "w", encoding="utf-8") as f:
                json.dump(out, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  å†™å…¥ transcript_index.json å¤±è´¥: {e}")

    def _sync_transcripts_to_data_dir(self):
        """åŒæ­¥ transcripts åˆ° data_dir å¹¶æ›´æ–° master_indexã€transcript_indexï¼ˆä¾› Whisper å¢é‡åŒæ­¥ï¼‰"""
        if not self.data_dir:
            return
        import shutil
        dst = self.data_dir / "transcripts"
        dst.mkdir(exist_ok=True)
        for f in self.transcripts_dir.glob("*.txt"):
            try:
                shutil.copy2(f, dst / f.name)
            except Exception:
                pass
        self._generate_master_index_csv()
        self._generate_transcript_index()

    def analyze_channel(self, channel_url: str, limit: Optional[int] = None):
        """
        å®Œæ•´åˆ†ææµç¨‹
        ç¬¦åˆä»¥ä¸‹ä»»ä¸€æ¡ä»¶çš„è§†é¢‘å°†è¢«ä¿å­˜ï¼š
        1. æ’­æ”¾é‡ >= 20,000
        2. å‘å¸ƒæ—¥æœŸ >= 2024å¹´1æœˆ
        3. åŒ…å«å…³é”®è¯
        """
        print("\n" + "=" * 80)
        print("ğŸš€ Palantir YouTube é¢‘é“åˆ†æç³»ç»Ÿå¯åŠ¨")
        print("=" * 80 + "\n")
        print("ğŸ“‹ ç­›é€‰æ¡ä»¶ï¼ˆç¬¦åˆä»»ä¸€å³ä¿å­˜ï¼‰:")
        print("   1ï¸âƒ£  æ’­æ”¾é‡ >= 20,000")
        print("   2ï¸âƒ£  å‘å¸ƒæ—¥æœŸ >= 2024å¹´1æœˆ")
        print("   3ï¸âƒ£  åŒ…å«å…³é”®è¯ (AIPCon, Paragon, Demo, Tutorialç­‰)")
        print()

        # 1. è·å–è§†é¢‘åˆ—è¡¨
        videos = self.fetch_channel_videos(channel_url, limit)
        if not videos:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•è§†é¢‘")
            return

        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'S': 0,
            'A': 0,
            'B': 0,
            'with_transcript': 0,
            'without_transcript': 0,
            'total_processed': 0,
            'saved': 0,
            'skipped': 0,
            # åˆ†ç±»ç»Ÿè®¡
            'cat_20k_views': 0,      # æ’­æ”¾é‡20K+
            'cat_2024_01': 0,         # 2024å¹´1æœˆ+
            'cat_keywords': 0,          # å…³é”®è¯åŒ¹é…
        }

        # 2. éå†å¤„ç†æ¯ä¸ªè§†é¢‘
        for idx, video in enumerate(videos, 1):
            video_id = video['video_id']
            print(f"\n[{idx}/{len(videos)}] å¤„ç†: {video['title']}")

            # è·å–è¯¦ç»†ä¿¡æ¯
            details = self.fetch_video_details(video['url'])
            if not details:
                continue

            stats['total_processed'] += 1

            # è®¡ç®—è¯„åˆ†
            score = VideoScorer.calculate_score(details)

            # åˆ¤å®šç­‰çº§
            rank = RankAssigner.get_rank(score)

            print(f"   ğŸ“Š è¯„åˆ†: {score}/100 | ç­‰çº§: {rank}")

            # æ£€æŸ¥æ˜¯å¦ç¬¦åˆä¸‰ç±»æ¡ä»¶
            view_count = details.get('view_count', 0)
            upload_date = details.get('upload_date', '')
            title = details.get('title', '')
            description = details.get('description', '')

            # æ¡ä»¶1: æ’­æ”¾é‡ >= 20,000
            meets_view_threshold = view_count >= 20000

            # æ¡ä»¶2: å‘å¸ƒæ—¥æœŸ >= 2024å¹´1æœˆ
            meets_date_threshold = False
            if upload_date and len(upload_date) >= 6:
                try:
                    year_month = int(upload_date[:6])  # YYYYMM
                    meets_date_threshold = year_month >= 202401
                except:
                    pass

            # æ¡ä»¶3: å…³é”®è¯åŒ¹é…
            has_keywords, matched_keywords = VideoScorer.check_keywords(title, description)
            meets_keyword_threshold = has_keywords

            # åˆ¤æ–­æ˜¯å¦ç¬¦åˆä»»ä¸€æ¡ä»¶
            should_save = meets_view_threshold or meets_date_threshold or meets_keyword_threshold

            # æ›´æ–°åˆ†ç±»ç»Ÿè®¡
            if meets_view_threshold:
                stats['cat_20k_views'] += 1
            if meets_date_threshold:
                stats['cat_2024_01'] += 1
            if meets_keyword_threshold:
                stats['cat_keywords'] += 1

            if not should_save:
                print(f"   â­ï¸  è·³è¿‡ (ä¸ç¬¦åˆä»»ä½•ç­›é€‰æ¡ä»¶)")
                stats['skipped'] += 1
                continue

            print(f"   âœ“ ç¬¦åˆæ¡ä»¶: "
                  f"{'[20K+æ’­æ”¾]' if meets_view_threshold else ''}"
                  f"{'[2024+]' if meets_date_threshold else ''}"
                  f"{'[å…³é”®è¯]' if meets_keyword_threshold else ''}")

            # è·å–å­—å¹•
            transcript = self.get_transcript(video_id)
            has_transcript = transcript is not None

            # ä¿å­˜ç»“æœï¼ˆå³ä½¿æ²¡æœ‰å­—å¹•ä¹Ÿä¿å­˜å…ƒæ•°æ®ï¼‰
            self.save_transcript(details, transcript, rank, score, has_transcript)

            stats[rank] += 1
            stats['saved'] += 1
            if has_transcript:
                stats['with_transcript'] += 1
            else:
                stats['without_transcript'] += 1

        # 3. è¾“å‡ºç»Ÿè®¡æŠ¥å‘Š
        print("\n" + "=" * 80)
        print("ğŸ“Š åˆ†æå®Œæˆç»Ÿè®¡")
        print("=" * 80)
        print(f"\nå¤„ç†è¿›åº¦:")
        print(f"  æ€»è§†é¢‘æ•°: {stats['total_processed']}/{len(videos)}")
        print(f"  å·²ä¿å­˜: {stats['saved']}")
        print(f"  å·²è·³è¿‡: {stats['skipped']}")

        print(f"\nç­‰çº§åˆ†å¸ƒ:")
        print(f"  S çº§ (æˆ˜ç•¥çº§): {stats['S']}")
        print(f"  A çº§ (é«˜ä»·å€¼): {stats['A']}")
        print(f"  B çº§ (åŸºç¡€): {stats['B']}")

        print(f"\nå­—å¹•çŠ¶æ€:")
        print(f"  æœ‰å­—å¹•: {stats['with_transcript']}")
        print(f"  æ— å­—å¹•: {stats['without_transcript']}")

        print(f"\nåˆ†ç±»ç»Ÿè®¡ (å¯é‡å¤è®¡æ•°):")
        print(f"  ğŸ“º æ’­æ”¾é‡ 20K+: {stats['cat_20k_views']}")
        print(f"  ğŸ“… 2024å¹´1æœˆ+: {stats['cat_2024_01']}")
        print(f"  ğŸ”‘ å…³é”®è¯åŒ¹é…: {stats['cat_keywords']}")

        print(f"\nğŸ“ è¾“å‡ºç›®å½•: {self.output_dir.absolute()}")
        print("=" * 80 + "\n")

        # 4. ç”Ÿæˆ CSV ç´¢å¼•æ–‡ä»¶å’Œåˆ†ç±»æŠ¥å‘Š
        self.generate_summary_csv()
        self.generate_category_report(stats)

    def generate_summary_csv(self):
        """
        ç”Ÿæˆ CSV ç´¢å¼•æ–‡ä»¶
        """
        import csv

        csv_file = self.output_dir / "video_index.csv"

        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['Rank', 'Score', 'Title', 'Date', 'Views', 'URL'])

            # éå†æ‰€æœ‰ txt æ–‡ä»¶
            for txt_file in self.transcripts_dir.glob('*.txt'):
                try:
                    with open(txt_file, 'r', encoding='utf-8') as tf:
                        lines = tf.readlines()

                        # è§£æå…ƒæ•°æ®
                        rank = 'Unknown'
                        score = 0
                        title = 'Unknown'
                        date = 'Unknown'
                        views = 0
                        url = ''

                        for line in lines[:20]:  # åªè¯»å–å‰20è¡Œï¼ˆå…ƒæ•°æ®éƒ¨åˆ†ï¼‰
                            line = line.strip()
                            if line.startswith('Rank:'):
                                rank = line.split(':')[1].strip()[0]  # æå–ç­‰çº§å­—æ¯
                            elif line.startswith('Score:'):
                                score = line.split(':')[1].strip().replace('/100', '')
                            elif line.startswith('Title:'):
                                title = line.split(':', 1)[1].strip()
                            elif line.startswith('Published:'):
                                date = line.split(':', 1)[1].strip()
                            elif line.startswith('View Count:'):
                                views = line.split(':')[1].strip().replace(',', '')
                            elif line.startswith('URL:'):
                                url = line.split(':', 1)[1].strip()

                        writer.writerow([rank, score, title, date, views, url])

                except Exception as e:
                    print(f"âš ï¸  å¤„ç†æ–‡ä»¶ {txt_file} å¤±è´¥: {e}")

        print(f"ğŸ“Š å·²ç”Ÿæˆç´¢å¼•æ–‡ä»¶: {csv_file}")

    def generate_category_report(self, stats: Dict):
        """
        ç”Ÿæˆåˆ†ç±»æŠ¥å‘Šæ–‡ä»¶
        """
        report_file = self.output_dir / "category_report.md"

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# Palantir YouTube é¢‘é“åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("---\n\n")

            f.write("## ğŸ“Š æ€»ä½“ç»Ÿè®¡\n\n")
            f.write(f"- **æ€»å¤„ç†è§†é¢‘æ•°**: {stats['total_processed']}\n")
            f.write(f"- **å·²ä¿å­˜è§†é¢‘æ•°**: {stats['saved']}\n")
            f.write(f"- **å·²è·³è¿‡è§†é¢‘æ•°**: {stats['skipped']}\n\n")

            f.write("## ğŸ¯ ç­‰çº§åˆ†å¸ƒ\n\n")
            f.write(f"| ç­‰çº§ | æè¿° | æ•°é‡ |\n")
            f.write(f"|------|------|------|\n")
            f.write(f"| **S** | Strategic - æˆ˜ç•¥çº§ç ”ç©¶å¯¹è±¡ | {stats['S']} |\n")
            f.write(f"| **A** | Active - é«˜å‚è€ƒä»·å€¼ | {stats['A']} |\n")
            f.write(f"| **B** | Basic - åŸºç¡€èƒŒæ™¯èµ„æ–™ | {stats['B']} |\n\n")

            f.write("## ğŸ“ å­—å¹•çŠ¶æ€\n\n")
            if stats['saved'] > 0:
                f.write(f"- **æœ‰å­—å¹•**: {stats['with_transcript']} ({stats['with_transcript']/stats['saved']*100:.1f}%)\n")
                f.write(f"- **æ— å­—å¹•**: {stats['without_transcript']} ({stats['without_transcript']/stats['saved']*100:.1f}%)\n\n")
            else:
                f.write(f"- **æœ‰å­—å¹•**: {stats['with_transcript']}\n")
                f.write(f"- **æ— å­—å¹•**: {stats['without_transcript']}\n\n")
            f.write("> âš ï¸  æ³¨æ„ï¼šæ— å­—å¹•çš„è§†é¢‘å·²ä¿å­˜å…ƒæ•°æ®ï¼Œå¯é€šè¿‡è¯­éŸ³è½¬æ–‡å­—æœåŠ¡è·å–å®Œæ•´å†…å®¹\n\n")

            f.write("## ğŸ“‚ åˆ†ç±»ç»Ÿè®¡ï¼ˆå¯é‡å¤è®¡æ•°ï¼‰\n\n")
            f.write("è§†é¢‘å¯èƒ½åŒæ—¶æ»¡è¶³å¤šä¸ªæ¡ä»¶ï¼Œå› æ­¤æ€»æ•°å¯èƒ½è¶…è¿‡å®é™…ä¿å­˜æ•°é‡ã€‚\n\n")
            f.write(f"| åˆ†ç±»æ¡ä»¶ | æ•°é‡ |\n")
            f.write(f"|----------|------|\n")
            f.write(f"| ğŸ“º æ’­æ”¾é‡ â‰¥ 20,000 | {stats['cat_20k_views']} |\n")
            f.write(f"| ğŸ“… å‘å¸ƒæ—¥æœŸ â‰¥ 2024å¹´1æœˆ | {stats['cat_2024_01']} |\n")
            f.write(f"| ğŸ”‘ åŒ…å«å…³é”®è¯ | {stats['cat_keywords']} |\n\n")

            f.write("## ğŸ”‘ å…³é”®è¯åˆ—è¡¨\n\n")
            f.write("**æ ¸å¿ƒå…³é”®è¯**: AIPCon, Foundrycon, Paragon, Pipeline, AIP, Foundry, Gotham, Apollo\n\n")
            f.write("**æ¬¡è¦å…³é”®è¯**: Demo, Tutorial, Workshop, Case Study, Bootcamp, How to, Guide\n\n")

            f.write("---\n\n")
            f.write("## ğŸ“‚ è¾“å‡ºæ–‡ä»¶\n\n")
            f.write(f"- **è§†é¢‘è„šæœ¬**: `transcripts/` ç›®å½•\n")
            f.write(f"- **ç´¢å¼•æ–‡ä»¶**: `video_index.csv` (å¯ç”¨ Excel æ‰“å¼€)\n")
            f.write(f"- **åˆ†ç±»æŠ¥å‘Š**: `category_report.md` (æœ¬æ–‡ä»¶)\n\n")

        print(f"ğŸ“„ å·²ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š: {report_file}")

    def retry_failed(self):
        """
        ä»…é‡æ–°å¤„ç†å¤±è´¥çš„è§†é¢‘
        ä» failed_videos.json è¯»å–ï¼Œé€ä¸ªé‡è¯•
        """
        failed_file = self.output_dir / "failed_videos.json"
        if not failed_file.exists():
            print("âŒ æœªæ‰¾åˆ° failed_videos.jsonï¼Œæ²¡æœ‰å¤±è´¥é¡¹éœ€è¦é‡è¯•")
            return

        with open(failed_file, "r", encoding="utf-8") as f:
            failed_list = json.load(f)

        if not failed_list:
            print("âœ… æ²¡æœ‰å¤±è´¥é¡¹éœ€è¦é‡è¯•")
            return

        print(f"\nğŸ“ é‡æ–°å¤„ç† {len(failed_list)} ä¸ªå¤±è´¥è§†é¢‘\n")
        still_failed: List[Dict] = []

        for idx, details in enumerate(failed_list, 1):
            video_id = details.get("video_id")
            title = details.get("title", "Unknown")
            print(f"[{idx}/{len(failed_list)}] {title[:50]}...")

            try:
                transcript = self.get_transcript(video_id)
                has_transcript = transcript is not None
                rank = details.get("rank", "B")
                score = details.get("score", 0)
                self.save_transcript(details, transcript, rank, score, has_transcript)
                print(f"   âœ“ æˆåŠŸ")
            except Exception as e:
                still_failed.append({**details, "error": str(e)})
                print(f"   âŒ ä»å¤±è´¥: {e}")

        self._write_failed_videos(still_failed)
        if self.data_dir:
            self._write_config_json()
            self._generate_master_index_csv()
        print(f"\nâœ… é‡è¯•å®Œæˆ: æˆåŠŸ {len(failed_list) - len(still_failed)}, ä»å¤±è´¥ {len(still_failed)}")

    def _read_existing_category(self, video_id: str, video_url: str) -> str:
        """ä»å·²æœ‰ transcript æ–‡ä»¶ä¸­è¯»å– Categoryï¼Œç”¨äº Whisper é‡å†™æ—¶ä¿ç•™"""
        vid = video_id or ""
        if not vid and video_url:
            import re
            m = re.search(r"[?&]v=([a-zA-Z0-9_-]{11})", video_url or "")
            vid = m.group(1) if m else ""
        if not vid:
            return ""
        for txt_file in self.transcripts_dir.glob("*.txt"):
            try:
                with open(txt_file, "r", encoding="utf-8") as f:
                    content = f.read()
                    if vid not in content:
                        continue
                    for line in content.split("\n"):
                        if line.strip().startswith("Category:"):
                            return line.split(":", 1)[1].strip()
            except Exception:
                pass
        return ""

    def process_whisper_missing(self, limit: Optional[int] = None):
        """ä»…å¯¹å½“å‰æ— å­—å¹•çš„è§†é¢‘ä½¿ç”¨ Whisper è½¬å½•ï¼ˆéœ€ ffmpegï¼‰ï¼Œè‡ªåŠ¨ä¿å­˜"""
        import csv
        csv_path = self.data_dir / "master_index.csv" if self.data_dir else None
        if not csv_path or not csv_path.exists():
            csv_path = self.output_dir / "filtered_candidates.csv"
        if not csv_path.exists():
            print("âŒ æœªæ‰¾åˆ° master_index.csv æˆ– filtered_candidates.jsonï¼Œè¯·å…ˆè¿è¡Œå®Œæ•´æµç¨‹")
            return
        with open(csv_path, encoding="utf-8") as f:
            reader = csv.DictReader(f)
            rows = list(reader)
        # ä» master_index æ‰¾ Transcript=æ—  çš„ï¼Œç›´æ¥å¤„ç†å…¨éƒ¨ï¼ˆä¸ä¾èµ– filtered_candidatesï¼‰
        need_whisper = [r for r in rows if r.get("Transcript") == "æ— "]
        if not need_whisper:
            print("âœ… æ²¡æœ‰éœ€è¦ Whisper è½¬å½•çš„è§†é¢‘")
            self._write_status(0, 0, "idle", phase="whisper", failed_count=0)
            return
        vid_pat = re.compile(r"[?&]v=([a-zA-Z0-9_-]{11})")
        to_process = []
        for r in need_whisper:
            url = r.get("URL", r.get("url", ""))
            m = vid_pat.search(url or "")
            vid = m.group(1) if m else ""
            if vid:
                to_process.append({
                    "video_id": vid,
                    "title": r.get("Title", r.get("title", "Unknown")),
                    "url": url,
                    "rank": r.get("Rank", "B"),
                    "score": r.get("Score", 0),
                    "category": r.get("Category", ""),
                })
        to_process = (to_process[:limit] if limit else to_process)
        print(f"\nğŸ“ å¯¹ {len(to_process)} ä¸ªæ— å­—å¹•è§†é¢‘ä½¿ç”¨ Whisper è¯­éŸ³è½¬æ–‡å­—ï¼ˆéœ€ ffmpegï¼‰\n")
        self._write_status(0, len(to_process), "processing", phase="whisper", failed_count=0)
        for idx, details in enumerate(to_process, 1):
            vid = details.get("video_id")
            title = details.get("title", "Unknown")
            print(f"[{idx}/{len(to_process)}] {title[:50]}...")
            self._write_status(idx, len(to_process), "processing", phase="whisper")
            transcript = self._transcribe_with_whisper(vid)
            rank = details.get("rank", "B")
            score = details.get("score", 0)
            # ä¿ç•™å·²æœ‰åˆ†ç±»ï¼Œä¸è¦†ç›–ä¸ºã€Œå…¶ä»–ã€
            existing_cat = self._read_existing_category(details.get("video_id", ""), details.get("url", ""))
            if not existing_cat:
                existing_cat = details.get("category", "")
            self.save_transcript(details, transcript, rank, score, transcript is not None, category=existing_cat or "", source="whisper" if transcript else "")
            if self.data_dir and transcript:
                self._sync_transcripts_to_data_dir()
        self._write_status(len(to_process), len(to_process), "idle", phase="whisper")
        if self.data_dir:
            self._sync_transcripts_to_data_dir()
        print("\nâœ… Whisper è½¬å½•å®Œæˆ")

    def process_others(self, channel_url: str, limit: Optional[int] = None):
        """å¤„ç†ä¸‰ç±»ä¹‹å¤–çš„å‰©ä½™è§†é¢‘ï¼Œåˆ†ç±»ä¸ºã€Œå…¶ä»–ã€"""
        filter_file = self.output_dir / "filtered_candidates.json"
        if not filter_file.exists():
            print("âŒ éœ€è¦å…ˆè¿è¡Œ filter_channel ç”Ÿæˆ filtered_candidates.json")
            return
        with open(filter_file, encoding="utf-8") as f:
            candidates = json.load(f)
        existing_ids = {c.get("video_id") for c in candidates if c.get("video_id")}

        videos = self.fetch_channel_videos(channel_url, limit)
        if not videos:
            print("âŒ æœªè·å–åˆ°è§†é¢‘åˆ—è¡¨")
            return

        others = [v for v in videos if v.get("video_id") and v["video_id"] not in existing_ids]
        if not others:
            print("âœ… æ²¡æœ‰ã€Œå…¶ä»–ã€ç±»è§†é¢‘éœ€è¦å¤„ç†")
            return

        print(f"\nğŸ“ å¤„ç† {len(others)} ä¸ªã€Œå…¶ä»–ã€ç±»è§†é¢‘ï¼ˆä¸‰ç±»ä¹‹å¤–ï¼‰\n")
        self._write_status(0, len(others), "processing", phase="process", failed_count=0)

        for idx, video in enumerate(others, 1):
            details = self.fetch_video_details(video["url"])
            if not details:
                continue
            self._write_status(idx, len(others), "processing", phase="process")

            score = VideoScorer.calculate_score(details)
            rank = RankAssigner.get_rank(score)
            transcript = self.get_transcript(details["video_id"])
            has_transcript = transcript is not None

            self.save_transcript(details, transcript, rank, score, has_transcript, category="å…¶ä»–")

        self._write_status(len(others), len(others), "idle", phase="process")
        if self.data_dir:
            self._generate_master_index_csv()
            self._generate_transcript_index()
            import shutil
            dst = self.data_dir / "transcripts"
            dst.mkdir(exist_ok=True)
            for f in self.transcripts_dir.glob("*.txt"):
                try:
                    shutil.copy2(f, dst / f.name)
                except Exception:
                    pass
            self._update_filter_summary_cat_others()
        print(f"\nâœ… ã€Œå…¶ä»–ã€ç±»å¤„ç†å®Œæˆ: {len(others)} ä¸ªè§†é¢‘")


def main():
    """
    ä¸»å‡½æ•° - ä¸¤é˜¶æ®µæµç¨‹ï¼š
    1. å…ˆç­›é€‰å‡ºç¬¦åˆä¸‰ç±»çš„è§†é¢‘ï¼ˆfilter_channelï¼‰â†’ è¾“å‡ºä¸‰ç±»æ•°é‡
    2. å†æŒ‰éœ€å¤„ç†ç­›é€‰ç»“æœï¼ˆprocess_filtered_candidatesï¼‰

    å‘½ä»¤è¡Œ:
      python palantir_analyzer.py           # å®Œæ•´æµç¨‹ï¼šç­›é€‰ + å¤„ç†
      python palantir_analyzer.py --filter-only   # ä»…ç­›é€‰ï¼Œè¾“å‡ºä¸‰ç±»æ•°é‡åé€€å‡º
      python palantir_analyzer.py --retry-failed  # ä»…é‡è¯•å¤±è´¥é¡¹
    """
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--filter-only", action="store_true", help="ä»…æ‰§è¡Œç­›é€‰ï¼Œè¾“å‡ºä¸‰ç±»æ•°é‡åé€€å‡º")
    parser.add_argument("--retry-failed", action="store_true", help="ä»…é‡è¯•å¤±è´¥çš„è§†é¢‘")
    parser.add_argument("--whisper-missing", action="store_true", help="ä»…å¯¹æ— å­—å¹•è§†é¢‘ç”¨ Whisper è½¬å½•ï¼ˆéœ€ ffmpegï¼‰")
    parser.add_argument("--process-others", action="store_true", help="å¤„ç†ä¸‰ç±»ä¹‹å¤–çš„å‰©ä½™è§†é¢‘ï¼Œåˆ†ç±»ä¸ºã€Œå…¶ä»–ã€")
    parser.add_argument("--whisper-one", type=str, default=None, help="å¯¹å•ä¸ªè§†é¢‘ ID æ‰§è¡Œ Whisper è½¬æ¢")
    parser.add_argument("--dashboard", type=str, default="palantirtech", help="çœ‹æ¿ ID")
    parser.add_argument("--convert-urls", type=str, default=None, help="ä¸´æ—¶è½¬æ¢å¤šä¸ªè§†é¢‘ IDï¼Œé€—å·åˆ†éš”ï¼Œæœ€å¤š 5 ä¸ª")
    parser.add_argument("--auto-whisper", action="store_true", help="å®Œæ•´æµç¨‹åè‡ªåŠ¨å¯¹æ— å­—å¹•è§†é¢‘æ‰§è¡Œ Whisper è½¬æ¢")
    parser.add_argument("--limit", type=int, default=None, help="é™åˆ¶å¤„ç†æ•°é‡ï¼ˆæµ‹è¯•ç”¨ï¼‰")
    args = parser.parse_args()

    channel_id = args.dashboard
    PALANTIR_CHANNEL_URL = "https://www.youtube.com/@palantirtech"
    analyzer = PalantirVideoAnalyzer(
        output_dir="palantir_analysis",
        data_dir="frontend/public/data",  # å‰ç«¯æ•°æ®ç›®å½•
        channel_id=channel_id,
    )

    if args.whisper_one:
        vid = args.whisper_one
        if channel_id in ("temp", "slim"):
            dash_dir = Path("frontend/public/data") / channel_id
            analyzer_whisper = PalantirVideoAnalyzer(output_dir=str(dash_dir), data_dir="frontend/public/data", channel_id=channel_id)
            for old_f in analyzer_whisper.transcripts_dir.glob("*.txt"):
                try:
                    if vid in old_f.read_text(encoding="utf-8"):
                        old_f.unlink()
                        break
                except Exception:
                    pass
            details = analyzer_whisper.fetch_video_details(f"https://www.youtube.com/watch?v={vid}")
            if not details:
                details = {"video_id": vid, "url": f"https://www.youtube.com/watch?v={vid}", "title": "Unknown", "rank": "B", "score": 0}
            else:
                details["rank"] = RankAssigner.get_rank(VideoScorer.calculate_score(details))
                details["score"] = VideoScorer.calculate_score(details)
            analyzer_whisper._write_status(0, 1, "processing", phase="whisper", failed_count=0)
            transcript = analyzer_whisper._transcribe_with_whisper(vid)
            analyzer_whisper.save_transcript(details, transcript, details.get("rank", "B"), details.get("score", 0), transcript is not None, category=details.get("category", "å…¶ä»–"), source="whisper" if transcript else "")
            analyzer_whisper._write_status(1, 1, "idle", phase="whisper")
            if analyzer_whisper.data_dir:
                analyzer_whisper._generate_master_index_csv()
                analyzer_whisper._generate_transcript_index()
                import shutil
                dst = analyzer_whisper.data_dir / "transcripts"
                dst.mkdir(exist_ok=True)
                for f in analyzer_whisper.transcripts_dir.glob("*.txt"):
                    try:
                        shutil.copy2(f, dst / f.name)
                    except Exception:
                        pass
            print(f"âœ… {channel_id} çœ‹æ¿å•è§†é¢‘ Whisper è½¬æ¢å®Œæˆ")
        else:
            filter_file = analyzer.output_dir / "filtered_candidates.json"
            if not filter_file.exists():
                print("âŒ éœ€è¦å…ˆè¿è¡Œå®Œæ•´æµç¨‹")
                return
            analyzer._write_status(0, 1, "processing", phase="whisper", failed_count=0)
            with open(filter_file, encoding="utf-8") as f:
                candidates = json.load(f)
            details = next((c for c in candidates if c.get("video_id") == vid), None)
            if not details:
                details = {"video_id": vid, "url": f"https://www.youtube.com/watch?v={vid}", "title": "Unknown", "rank": "B", "score": 0}
            transcript = analyzer._transcribe_with_whisper(vid)
            analyzer.save_transcript(details, transcript, details.get("rank", "B"), details.get("score", 0), transcript is not None, category="")
            analyzer._write_status(1, 1, "idle", phase="whisper")
            if analyzer.data_dir:
                analyzer._sync_transcripts_to_data_dir()
            print("âœ… å•è§†é¢‘è½¬æ¢å®Œæˆ")
        return

    if args.convert_urls:
        channel_id = args.dashboard if args.dashboard in ("temp", "slim") else "temp"
        temp_data_dir = Path("frontend/public/data") / channel_id
        temp_data_dir.mkdir(parents=True, exist_ok=True)
        # ä½¿ç”¨ data_dir ä½œä¸º output_dirï¼Œä½¿å­—å¹•ç›´æ¥ä¿å­˜åˆ° frontend/public/data/temp/transcripts/
        # ä¾› API çš„ get_transcript ä»åŒä¸€è·¯å¾„è¯»å–
        analyzer_temp = PalantirVideoAnalyzer(output_dir=str(temp_data_dir), data_dir="frontend/public/data", channel_id=channel_id)
        vids = [v.strip() for v in args.convert_urls.split(",") if v.strip()][:5]
        for vid in vids:
            details = analyzer_temp.fetch_video_details(f"https://www.youtube.com/watch?v={vid}")
            if not details:
                details = {"video_id": vid, "url": f"https://www.youtube.com/watch?v={vid}", "title": "Temp", "rank": "B", "score": 0}
            else:
                details["rank"] = RankAssigner.get_rank(VideoScorer.calculate_score(details))
                details["score"] = VideoScorer.calculate_score(details)
            transcript = analyzer_temp.get_transcript(vid)
            analyzer_temp.save_transcript(details, transcript, details.get("rank", "B"), details.get("score", 0), transcript is not None, category="å…¶ä»–")
        if analyzer_temp.data_dir:
            analyzer_temp._generate_master_index_csv()
            analyzer_temp._generate_transcript_index()
        print(f"âœ… ä¸´æ—¶è½¬æ¢å®Œæˆ: {len(vids)} ä¸ªè§†é¢‘ï¼Œå­—å¹•å·²ä¿å­˜è‡³ {temp_data_dir / 'transcripts'}")
        return

    if args.retry_failed:
        analyzer.retry_failed()
        return

    if args.whisper_missing:
        analyzer.process_whisper_missing(limit=args.limit)
        return

    if args.process_others:
        analyzer.process_others(PALANTIR_CHANNEL_URL, limit=args.limit)
        return

    # é˜¶æ®µ 1ï¼šå…ˆç­›é€‰
    analyzer.filter_channel(PALANTIR_CHANNEL_URL, limit=args.limit)

    if args.filter_only:
        print("\nâœ… ç­›é€‰å®Œæˆï¼Œä¸‰ç±»æ•°é‡å·²è¾“å‡ºã€‚æ‰§è¡Œå®Œæ•´æµç¨‹è¯·å»æ‰ --filter-only")
        return

    # é˜¶æ®µ 2ï¼šå¤„ç†ç­›é€‰ç»“æœ
    print("\nâ–¶ å¼€å§‹å¤„ç†ï¼ˆè·å–å­—å¹•ï¼‰...\n")
    analyzer.process_filtered_candidates(limit=args.limit)

    if args.auto_whisper:
        print("\nâ–¶ å¯¹æ— å­—å¹•è§†é¢‘æ‰§è¡Œ Whisper è½¬æ¢...\n")
        analyzer.process_whisper_missing(limit=args.limit)


if __name__ == "__main__":
    main()
